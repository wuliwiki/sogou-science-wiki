<!DOCTYPE html>
<!-- saved from url=(0083)https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm -->
<html class="" data-reactroot=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./751.降维 - 搜狗科学百科_files/analytics.js.download" type="text/javascript"></script>
<script type="text/javascript">window.addEventListener('DOMContentLoaded',function(){var v=archive_analytics.values;v.service='wb';v.server_name='wwwb-app221.us.archive.org';v.server_ms=175;archive_analytics.send_pageview({});});</script>
<script type="text/javascript" src="./751.降维 - 搜狗科学百科_files/bundle-playback.js.download" charset="utf-8"></script>
<script type="text/javascript" src="./751.降维 - 搜狗科学百科_files/wombat.js.download" charset="utf-8"></script>
<script type="text/javascript">
  __wm.init("https://web.archive.org/web");
  __wm.wombat("https://baike.sogou.com/kexue/d10751.htm","20221025115712","https://web.archive.org/","web","/_static/",
	      "1666699032");
</script>
<link rel="stylesheet" type="text/css" href="./751.降维 - 搜狗科学百科_files/banner-styles.css">
<link rel="stylesheet" type="text/css" href="./751.降维 - 搜狗科学百科_files/iconochive.css">
<!-- End Wayback Rewrite JS Include -->
<meta name="save" content="history"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="VWGb6TyYx8"><meta content="降维 - 搜狗科学百科" name="keywords"><meta content="搜狗科学百科是一部有着平等、协作、分享、自由理念的网络科学全书，为每一个互联网用户创造一个涵盖所有领域知识、服务的中文知识性平台。" name="description"><meta http-equiv="x-dns-prefetch-control" content="on"><meta name="server" baike="235" ip="210" env="online"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://cache.soso.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://hhy.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://pic.baike.soso.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://ugc.qpic.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://xui.ptlogin2.qq.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://q1.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://q2.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://q3.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://q4.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://q.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://img01.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://img02.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://img03.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025115712/https://img04.sogoucdn.com/"><link rel="Shortcut Icon" href="https://web.archive.org/web/20221025115712im_/https://www.sogou.com/images/logo/new/favicon.ico?v=4"><link rel="Bookmark" href="https://www.sogou.com/images/logo/new/favicon.ico?v=4"><link href="./751.降维 - 搜狗科学百科_files/base_b849887.css" rel="stylesheet"><link href="./751.降维 - 搜狗科学百科_files/detail_378aed5.css" rel="stylesheet"><link href="./751.降维 - 搜狗科学百科_files/inviteAudit_7894507.css" rel="stylesheet"><link rel="stylesheet" href="./751.降维 - 搜狗科学百科_files/highlight.min.css"><title>降维 - 搜狗科学百科</title><style>.onekey-close {
	position: absolute;
	top: 16px;
	right: 16px;
	width: 24px;
	height: 24px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	text-indent: -999em;
	background-size: 84px;
	background-position: -63px 0;
}

.onekey-login {
	position: absolute;
	top: 16.4%;
	left: 0;
	right: 0;
	width: 100%;
}

/* .onekey-login-img {
    width: 75px;
    height: 75px;
    background: url("https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/images/sprite_wap_baike.png") no-repeat;
    background-size: 100px 91px;
    background-position: 0 0;
    background-repeat: no-repeat;
    margin: 0 auto;
} */

.onekey-login-title {
	text-align: center;
	padding-bottom: 3px;
	font-size: 21px;
	font-weight: bold;
	line-height: 30px;
	color: #000;
}

.onekey-login-txt {
	text-align: center;
	font-family: PingFangSC;
	font-size: 14px;
	line-height: 20px;
	color: #8f8f8f;
}

.onekey-login-qq,
.onekey-login-wx,
.onekey-login-phone {
	display: block;
	width: 245px;
	height: 54px;
	border-radius: 45px;
	text-align: center;

	margin: 0 auto;
	font-size: 17px;
	line-height: 24px;
	color: #000;
	/* padding: 16px 77px; */
	border-radius: 12px;
	border: solid 1px #e0e0e0;
}
.onekey-qq-content,
.onekey-vx-content,
.onekey-phone-content {
	display: inline-block;
	margin-top: 16px;
}
.onekey-qq-content {
	padding: 0 5px;
}

.onekey-login-qq {
	margin-top: 48px;
	margin-bottom: 24px;
}

.onekey-login-qq:before {
	display: inline-block;
	content: "";
	width: 20px;
	height: 20px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 80px;
	background-position: -20px 0;
	vertical-align: top;
	margin: 17px 8px 0 0;
}

.onekey-login-wx {
	margin-bottom: 24px;
}

.onekey-login-wx:before {
	display: inline-block;
	content: "";
	width: 21px;
	height: 21px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 84px;
	background-position: 0 0;
	vertical-align: top;
	margin: 17px 10px 0 0;
}

.onekey-login-phone {
}

.onekey-login-phone:before {
	display: inline-block;
	content: "";
	width: 21px;
	height: 21px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 84px;
	background-position: -42px 0;
	vertical-align: top;
	margin: 17px 10px 0 0;
}

.onekey-fixed {
	z-index: 100;
	position: fixed;
	top: 0;
	bottom: 0;
	left: 0;
	right: 0;
	background: #fff;
	width: 100%;
	height: 100%;
}

.onekey-fixed.forbid {
	z-index: 100;
	position: fixed;
	top: auto;
	bottom: 68px;
	left: 9%;
	right: 0;
	background: rgba(0, 0, 0, 0.7);
	width: 82%;
	height: 43px;
	border-radius: 25px;
	color: #ffffff;
}
.onekey-login-title.forbid {
	text-align: center;
	padding-bottom: 3px;
	font-size: 14px;
	font-weight: normal;
	line-height: 30px;
	color: white;
}
</style><style>#login_mask {
  background: #000;
  opacity: 0.5;
  filter: alpha(opacity=50);
  position: fixed;
  /*fixed好像在哪个IE上有BUG，先用用*/
  left: 0;
  top: 0;
  z-index: 999;
  height: 100%;
}

#login_iframe_container {
  position: fixed;
  width: 550px;
  height: 360px;
  z-index: 1020;
  background-color: #ffffff;
}

@media screen and (max-width: 828px) {
  #login_iframe_container {
    top: 50% !important;
    left: 50% !important;
    transform: translate(-50%, -50%);
  }
}

#login_iframe_container.new-login {
  width: 550px;
  height: 360px;
  background-image: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/background_2a4a8a6.png);
}

#login_iframe_container.new-login.no-bg {
  background: #fff;
}

#login_iframe_container.new-login .login-title {
  width: 100%;
  height: 42px;
  line-height: 42px;
  text-align: center;
  font-size: 30px;
  letter-spacing: 0.19px;
  color: #ffffff;
  margin-top: 62px;
}
#login_iframe_container.new-login .forbid-title {
  width: 100%;
  height: 42px;
  line-height: 42px;
  text-align: center;
  font-size: 24px;
  letter-spacing: 0.19px;
  color: #333333;
  margin-top: 150px;
}

#login_iframe_container.new-login.no-bg .login-title {
  color: #333333;
}

#login_iframe_container.new-login .login-subtitle {
  width: 100%;
  height: 18px;
  line-height: 18px;
  font-size: 13px;
  letter-spacing: 0.08px;
  color: #ffffff;
  text-align: center;
  margin-top: 9px;
  margin-bottom: 43px;
}

#login_iframe_container.new-login.no-bg .login-subtitle {
  color: #999999;
}

#login_iframe_container.new-login .login-subtitle::before {
  content: '';
  display: inline-block;
  width: 10px;
  height: 1px;
  background-color: #ffffff;
  position: relative;
  top: -4px;
  left: -5px;
}

#login_iframe_container.new-login .login-subtitle::after {
  content: '';
  display: inline-block;
  width: 10px;
  height: 1px;
  background-color: #ffffff;
  position: relative;
  top: -4px;
  left: 5px;
}

#login_iframe_container.new-login.no-bg .login-subtitle::before {
  background-color: #999999;
}

#login_iframe_container.new-login.no-bg .login-subtitle::after {
  background-color: #999999;
}

#login_iframe_container.new-login .close-btn {
  position: absolute;
  top: 20px;
  right: 20px;
  width: 12px;
  height: 12px;
  background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -59px -10px;
  background-size: 81px 91px;
  cursor: pointer;
}

#login_iframe_container.new-login .login-btn {
  width: 220px;
  height: 47px;
  border-radius: 24px;
  border: solid 1px #dddddd;
  background-color: #ffffff;
  margin: 0 auto;
  margin-top: 28px;
  position: relative;
  display: block;
}

#login_iframe_container.new-login .login-btn .login-icon {
  position: absolute;
}

#login_iframe_container.new-login .login-btn .login-text {
  width: 61px;
  height: 47px;
  line-height: 47px;
  vertical-align: middle;
  font-size: 15px;
  letter-spacing: 0.1px;
  color: #666666;
  position: absolute;
  right: 62px;
}

#login_iframe_container.new-login .login-btn.qq-btn .login-icon {
  width: 22px;
  height: 27px;
  top: 10px;
  left: 67px;
  background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -10px -54px;
  background-size: 81px 91px;
}

#login_iframe_container.new-login .login-btn.qq-btn .login-text {
  right: 59px;
}

#login_iframe_container.new-login .login-btn.wechat-btn .login-icon {
  width: 29px;
  height: 24px;
  top: 12px;
  left: 62px;
  background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -10px -10px;
  background-size: 81px 91px;
}</style><style>/* -- container -- */
.rodal,
.rodal-mask {
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 100;
}

.rodal {
    position: fixed;
}

/* -- mask -- */
.rodal-mask {
    position: fixed;
    background: rgba(0, 0, 0, .5);
}

/* -- dialog -- */
.rodal-dialog {
    position: absolute;
    z-index: 101;
    background: #fff;
    border-radius: 3px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, .2);
}

.rodal-center {
    top: 50%;
    transform: translateY(-50%);
    left: 0;
    right: 0;
    margin: 0 auto;
}

.rodal-bottom {
    left: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

.rodal-top {
    left: 0;
    right: 0;
    top: 0;
    margin: auto;
}

.rodal-left {
    top: 0;
    left: 0;
    bottom: 0;
    margin: auto;
}

.rodal-right {
    top: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

/* -- close button -- */
.rodal-close {
    position: absolute;
    cursor: pointer;
    top: 16px;
    right: 16px;
    width: 16px;
    height: 16px;
}

.rodal-close:before,
.rodal-close:after {
    position: absolute;
    content: '';
    height: 2px;
    width: 100%;
    top: 50%;
    left: 0;
    margin-top: -1px;
    background: #999;
    border-radius: 100%;
    -webkit-transition: background .2s;
    transition: background .2s;
}

.rodal-close:before {
    -webkit-transform: rotate(45deg);
    transform: rotate(45deg);
}

.rodal-close:after {
    -webkit-transform: rotate(-45deg);
    transform: rotate(-45deg);
}

.rodal-close:hover:before,
.rodal-close:hover:after {
    background: #333;
}

/* -- fade -- */
/* @-webkit-keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

@keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

.rodal-fade-enter {
    -webkit-animation: rodal-fade-enter both ease-in;
    animation: rodal-fade-enter both ease-in;
} */

@-webkit-keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

@keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

.rodal-fade-leave {
    -webkit-animation: rodal-fade-leave both ease-out;
    animation: rodal-fade-leave both ease-out;
}

/* -- zoom -- */
@-webkit-keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-enter {
    -webkit-animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-leave {
    -webkit-animation: rodal-zoom-leave both;
    animation: rodal-zoom-leave both;
}

/* -- slideDown -- */
@-webkit-keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-enter {
    -webkit-animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-leave {
    -webkit-animation: rodal-slideDown-leave both;
    animation: rodal-slideDown-leave both;
}

/* -- slideLeft -- */
@-webkit-keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-enter {
    -webkit-animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-leave {
    -webkit-animation: rodal-slideLeft-leave both;
    animation: rodal-slideLeft-leave both;
}

/* -- slideRight -- */
@-webkit-keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-enter {
    -webkit-animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-leave {
    -webkit-animation: rodal-slideRight-leave both;
    animation: rodal-slideRight-leave both;
}

/* -- slideUp -- */
@-webkit-keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-enter {
    -webkit-animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
    animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
}

@-webkit-keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-leave {
    -webkit-animation: rodal-slideUp-leave both;
    animation: rodal-slideUp-leave both;
}

/* -- flip -- */
@-webkit-keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

@keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

.rodal-flip-enter {
    -webkit-animation: rodal-flip-enter both ease-in;
    animation: rodal-flip-enter both ease-in;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

@-webkit-keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

@keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

.rodal-flip-leave {
    -webkit-animation: rodal-flip-leave both;
    animation: rodal-flip-leave both;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

/* -- rotate -- */
@-webkit-keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-enter {
    -webkit-animation: rodal-rotate-enter both;
    animation: rodal-rotate-enter both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

@-webkit-keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-leave {
    -webkit-animation: rodal-rotate-leave both;
    animation: rodal-rotate-leave both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

/* -- door -- */
@-webkit-keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

@keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

.rodal-door-enter {
    -webkit-animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

@keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

.rodal-door-leave {
    -webkit-animation: rodal-door-leave both;
    animation: rodal-door-leave both;
}</style><style>/* -- container -- */
.rodal,
.rodal-mask {
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 100;
}

.rodal {
    position: fixed;
}

/* -- mask -- */
.rodal-mask {
    position: fixed;
    background: rgba(0, 0, 0, .5);
}

/* -- dialog -- */
.rodal-dialog {
    position: absolute;
    z-index: 101;
    background: #fff;
    border-radius: 3px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, .2);
}

.rodal-center {
    top: 50%;
    transform: translateY(-50%);
    left: 0;
    right: 0;
    margin: 0 auto;
}

.rodal-bottom {
    left: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

.rodal-top {
    left: 0;
    right: 0;
    top: 0;
    margin: auto;
}

.rodal-left {
    top: 0;
    left: 0;
    bottom: 0;
    margin: auto;
}

.rodal-right {
    top: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

/* -- close button -- */
.rodal-close {
    position: absolute;
    cursor: pointer;
    top: 16px;
    right: 16px;
    width: 16px;
    height: 16px;
}

.rodal-close:before,
.rodal-close:after {
    position: absolute;
    content: '';
    height: 2px;
    width: 100%;
    top: 50%;
    left: 0;
    margin-top: -1px;
    background: #999;
    border-radius: 100%;
    -webkit-transition: background .2s;
    transition: background .2s;
}

.rodal-close:before {
    -webkit-transform: rotate(45deg);
    transform: rotate(45deg);
}

.rodal-close:after {
    -webkit-transform: rotate(-45deg);
    transform: rotate(-45deg);
}

.rodal-close:hover:before,
.rodal-close:hover:after {
    background: #333;
}

/* -- fade -- */
/* @-webkit-keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

@keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

.rodal-fade-enter {
    -webkit-animation: rodal-fade-enter both ease-in;
    animation: rodal-fade-enter both ease-in;
} */

@-webkit-keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

@keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

.rodal-fade-leave {
    -webkit-animation: rodal-fade-leave both ease-out;
    animation: rodal-fade-leave both ease-out;
}

/* -- zoom -- */
@-webkit-keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-enter {
    -webkit-animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-leave {
    -webkit-animation: rodal-zoom-leave both;
    animation: rodal-zoom-leave both;
}

/* -- slideDown -- */
@-webkit-keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-enter {
    -webkit-animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-leave {
    -webkit-animation: rodal-slideDown-leave both;
    animation: rodal-slideDown-leave both;
}

/* -- slideLeft -- */
@-webkit-keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-enter {
    -webkit-animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-leave {
    -webkit-animation: rodal-slideLeft-leave both;
    animation: rodal-slideLeft-leave both;
}

/* -- slideRight -- */
@-webkit-keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-enter {
    -webkit-animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-leave {
    -webkit-animation: rodal-slideRight-leave both;
    animation: rodal-slideRight-leave both;
}

/* -- slideUp -- */
@-webkit-keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-enter {
    -webkit-animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
    animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
}

@-webkit-keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-leave {
    -webkit-animation: rodal-slideUp-leave both;
    animation: rodal-slideUp-leave both;
}

/* -- flip -- */
@-webkit-keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

@keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

.rodal-flip-enter {
    -webkit-animation: rodal-flip-enter both ease-in;
    animation: rodal-flip-enter both ease-in;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

@-webkit-keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

@keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

.rodal-flip-leave {
    -webkit-animation: rodal-flip-leave both;
    animation: rodal-flip-leave both;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

/* -- rotate -- */
@-webkit-keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-enter {
    -webkit-animation: rodal-rotate-enter both;
    animation: rodal-rotate-enter both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

@-webkit-keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-leave {
    -webkit-animation: rodal-rotate-leave both;
    animation: rodal-rotate-leave both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

/* -- door -- */
@-webkit-keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

@keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

.rodal-door-enter {
    -webkit-animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

@keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

.rodal-door-leave {
    -webkit-animation: rodal-door-leave both;
    animation: rodal-door-leave both;
}</style></head><body class=""><!-- BEGIN WAYBACK TOOLBAR INSERT -->
<style type="text/css">
body {
  margin-top:0 !important;
  padding-top:0 !important;
  /*min-width:800px !important;*/
}
</style>
<script>__wm.rw(0);</script>
<div id="wm-ipp-base" lang="en" style="display: block; direction: ltr;">
</div><div id="wm-ipp-print">The Wayback Machine - https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm</div>
<script type="text/javascript">//<![CDATA[
__wm.bt(675,27,25,2,"web","https://baike.sogou.com/kexue/d10751.htm","20221025115712",1996,"/_static/",["/_static/css/banner-styles.css?v=S1zqJCYt","/_static/css/iconochive.css?v=qtvMKcIJ"], false);
  __wm.rw(1);
//]]></script>
<!-- END WAYBACK TOOLBAR INSERT --><script>window._gtag=window._gtag||{};window._gtag.shouldGrayed = false;if ('5a72503cdc73452ba2dea5a88e3199a8') window._gtag.traceId = '5a72503cdc73452ba2dea5a88e3199a8';if ({"illegality":true}) window.userInfo = {"illegality":true};</script><div class="topnavbox"><ul class="topnav"><li><a href="https://web.archive.org/web/20221025115712/https://www.sogou.com/web?query=">网页</a></li><li><a href="https://web.archive.org/web/20221025115712/https://weixin.sogou.com/weixin?p=75351201">微信</a></li><li><a href="https://web.archive.org/web/20221025115712/https://zhihu.sogou.com/zhihu?p=75351218">知乎</a></li><li><a href="https://web.archive.org/web/20221025115712/https://pic.sogou.com/pics?query=">图片</a></li><li><a href="https://web.archive.org/web/20221025115712/https://v.sogou.com/v?query=">视频</a></li><li><a href="https://web.archive.org/web/20221025115712/https://mingyi.sogou.com/">医疗</a></li><li class="cur"><strong>科学</strong></li><li><a href="https://web.archive.org/web/20221025115712/https://hanyu.sogou.com/">汉语</a></li><li><a href="https://web.archive.org/web/20221025115712/https://wenwen.sogou.com/">问问</a></li><li><a href="https://web.archive.org/web/20221025115712/https://www.sogou.com/docs/more.htm">更多<span class="topraquo">»</span></a></li></ul></div><div id="header"><div class="header-wrap"><a class="header-logo" href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue"></a><div class="header-search"><div class="querybox" id="suggBox"><form><input id="searchInput" class="query" type="text" placeholder="搜科学领域专业百科词条" name="query" autocomplete="off" value=""><a href="javascript:;" class="query-search"></a></form></div></div><div class="header-rgt"><span class="btn-header-rgt btn-edit" id="editLemma">创建</span><div class="header-user no-login"></div></div></div></div><div class="fixed-placeholder" style="visibility:none"></div><div id="container" class=""><div class="content lemma-level1"><div class="detail-title" id="abstract-title"><h1>降维</h1><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#!" class="detail-edit">编辑</a></div><div class="section_content" data-id="53985499320213506"><div><p>在统计学、机器学习和信息论中，降维是通过获得一组主要变量并减少所考虑的随机变量数量的过程<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_1" class="kx_ref">[1]</a></sup>。主要分为特征选择和特征提取。<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_2" class="kx_ref">[2]</a></sup> </p></div></div><div id="catalog"><h2 class="title2">目录<a href="javascript:" class="detail-edit">编辑</a></h2><div class="catalog_wrap" style=""><ul class="catalog_list col3"><li><span class="order">1</span><a href="javascript:" data-level="1" data-id="14995490575745557">特征选择</a></li><li><span class="order">2</span><a href="javascript:" data-level="1" data-id="14995490592522754">特征投影</a></li><li class="secondary_catalog"><span>2.1 </span><a href="javascript:" data-id="14995490592522754">主成分分析(PCA)</a></li><li class="secondary_catalog"><span>2.2 </span><a href="javascript:" data-id="14995490592522754">非负矩阵分解(NMF)</a></li><li class="secondary_catalog"><span>2.3 </span><a href="javascript:" data-id="14995490592522754">内核主成分分析</a></li></ul><ul class="catalog_list col3"><li class="secondary_catalog"><span>2.4 </span><a href="javascript:" data-id="14995490592522754">基于图的核主成分分析</a></li><li class="secondary_catalog"><span>2.5 </span><a href="javascript:" data-id="14995490592522754">线性判别分析（LDA）</a></li><li class="secondary_catalog"><span>2.6 </span><a href="javascript:" data-id="14995490592522754">广义判别分析(GDA)</a></li><li class="secondary_catalog"><span>2.7 </span><a href="javascript:" data-id="14995490592522754">自动编码器</a></li><li><span class="order">3</span><a href="javascript:" data-level="1" data-id="14995490592522755">降维</a></li></ul><ul class="catalog_list col3"><li><span class="order">4</span><a href="javascript:" data-level="1" data-id="14995490592522756">降维的优势</a></li><li><span class="order">5</span><a href="javascript:" data-level="1" data-id="14995490592522757">应用</a></li><li><span class="order">6</span><a href="javascript:" data-level="1" data-id="14995490592522758">笔记</a></li><li><span class="order">7</span><a href="javascript:" data-level="1" data-id="references">参考文献</a></li></ul></div></div><div id="paragraphs"><div><div id="par_14995490575745557"><h2 class="title">1 特征选择<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>特征选择方法试图找到原始变量的子集(也称为特征或属性)。有三种策略：过滤策略(如信息增益)，包装策略（如由准确性引导的搜索）和嵌入策略（如选择特征以在基于预测误差构建模型时添加或移除特征）。</p><p>在某些情况下，回归或分类等数据分析在缩小的空间中比在原始空间中可以更精确地完成。<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_3" class="kx_ref">[3]</a></sup> </p></div></div><div id="par_14995490592522754"><h2 class="title">2 特征投影<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>特征投影(也称为特征提取)将高维空间中的数据转换为更少维度的空间。数据转换可以是线性的，如主成分分析，但也存在许多非线性降维技术。<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_4" class="kx_ref">[4]</a></sup><sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_5" class="kx_ref">[5]</a></sup>对于多维数据，张量表示可以通过多线性子空间的学习进行降维。<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_6" class="kx_ref">[6]</a></sup> </p><p></p><h3>2.1 <span>主成分分析(PCA)</span></h3><p></p><p>降维的主要线性技术，主成分分析，以使低维表示中的数据方差最大化的方式，执行数据到低维空间的线性映射。在实践中，构建数据的协方差(或相关性)矩阵，并计算该矩阵上的特征向量。与最大特征值(主成分)相对应的特征向量现在可以用于重构原始数据的大部分方差。此外，前几个特征向量通常可以根据系统的大规模物理行为来解释。原始空间(具有点数的维度)已经减少(存在数据丢失，但是希望保留最重要的方差)到由几个特征向量跨越的空间。</p><p></p><h3>2.2 <span>非负矩阵分解(NMF)</span></h3><p></p><p>NMF将非负矩阵分解为两个非负矩阵的乘积，这在只有非负信号存在的领域是一个很有前途的工具。<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_7" class="kx_ref">[7]</a></sup><sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_8" class="kx_ref">[8]</a></sup>比如天文学<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_9" class="kx_ref">[9]</a></sup><sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_10" class="kx_ref">[10]</a></sup>。NMF是众所周知的， 自Lee &amp; Seung的乘法规则更新以来<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_7" class="kx_ref">[7]</a></sup>，NMF在不断发展：包含不确定性 <sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_9" class="kx_ref">[9]</a></sup>、丢失数据和并行计算的考虑 <sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_11" class="kx_ref">[11]</a></sup>，顺序构造<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_11" class="kx_ref">[11]</a></sup> 导致了NMF的稳定性和线性度<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_10" class="kx_ref">[10]</a></sup>，同时也有其他更新。</p><p>在构造期间具有稳定的组件基础，以及线性建模过程，顺序NMF<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_11" class="kx_ref">[11]</a></sup> 作为探测系外行星的方法之一，特别是用于星周盘的直接成像，能够保持天体中星周结构直接成像的通量<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_10" class="kx_ref">[10]</a></sup>。与主成分分析相比，NMF没有去除导致非物理非负通量的矩阵平均值，因此NMF能够保存比主成分分析更多的信息（Ren 等人所证明）<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_10" class="kx_ref">[10]</a></sup>。</p><p></p><h3>2.3 <span>内核主成分分析</span></h3><p></p><p>主成分分析可以通过核技巧以非线性方式使用。由此产生的技术能够构造非线性映射，使数据中的方差最大化。这种技术被称为内核主成分分析。</p><p></p><h3>2.4 <span>基于图的核主成分分析</span></h3><p></p><p>其他突出的非线性技术包括流形学习技术，如Isomap、局部线性嵌入(LLE)、Hessian LLE、拉普拉斯特征映射和基于tangent空间分析的方法<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_12" class="kx_ref">[12]</a></sup><sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_13" class="kx_ref">[13]</a></sup>。这些技术使用保持数据局部属性的成本函数来构建低维数据表示，并且可以被视为基于图形内核的PCA定义。</p><p>最近，有人提出了一些技术，这些技术不是定义一个固定的内核，而是尝试使用半定编程来学习内核。这种技术最突出的例子是最大方差展开(MVU)。MVU的中心思想是精确地保留最近邻之间的所有成对距离(在内积空间中)，同时最大化不是最近邻的点之间的距离。</p><p>邻域保持的另一种方法是通过最小化测量输入和输出空间距离差异的成本函数。这种技术的重要例子包括：经典多维标度，与主成分分析相同；Isomap，使用数据空间中的测地距离；扩散图，使用数据空间中的扩散距离；t分布式随机近邻嵌入(t-SNE)，它最大限度的减少了点对分布之间的差异；曲元分析。</p><p>非线性降维的另一种方法是使用自动编码器，这是一种特殊的前馈神经网络，具有瓶颈隐藏层。<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_14" class="kx_ref">[14]</a></sup>深度编码器的训练通常使用贪婪的分层预训练(例如，使用一堆受限的玻尔兹曼机器)来执行，随后是基于反向传播的微调阶段。</p><p></p><h3>2.5 <span>线性判别分析（LDA）</span></h3><p></p><p>线性判别式分析(LDA)是费希尔线性判别式(Fisher linear discrimina tion)的推广，费希尔线性判别式是一种用于统计、模式识别和机器学习的方法，用于寻找表征或分离两类或更多类对象或事件的特征的线性组合。</p><p></p><h3>2.6 <span>广义判别分析(GDA)</span></h3><p></p><p>GDA利用核函数算子处理非线性判别分析。基础理论与支持向量机(SVM)很接近，因为GDA方法提供了输入向量到高维特征空间的映射。<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_15" class="kx_ref">[15]</a></sup><sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_16" class="kx_ref">[16]</a></sup>与LDA相似，GDA的目标是通过最大化类间散射与类内散射的比率，找到特征空间在较低维空间中的投影。</p><p></p><h3>2.7 <span>自动编码器</span></h3><p></p><p>自动编码器可用于学习非线性降维函数和编码以及从编码到原始表示的反函数。</p></div></div><div id="par_14995490592522755"><h2 class="title">3 降维<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>对于高维数据集(即维数大于10的数据集)，为了避免维数灾难的影响，通常在应用K-最近邻算法(k-NN)之前进行降维。<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_17" class="kx_ref">[17]</a></sup> </p><p>使用主成分分析（PCA），线性判别分析（LDA），典型相关分析（CCA）或非负矩阵分解（NMF）技术作为预处理步骤，可以在一个步骤中通过K-NN对缩小空间中的特征向量进行聚类来组合特征提取和降维。 在机器学习中，这个过程也称为低维嵌入。<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_18" class="kx_ref">[18]</a></sup> </p><p>对于超高维数据集(例如，当对实时视频流、DNA数据或高维时间序列执行相似性搜索时)，使用局部敏感散列、随机投影，<sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_19" class="kx_ref">[19]</a></sup> “草图” <sup><a href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/d10751.htm#quote_20" class="kx_ref">[20]</a></sup> 或者其他高维相似性搜索时，采用VLDB工具箱技术可能是唯一可行的选择。</p></div></div><div id="par_14995490592522756"><h2 class="title">4 降维的优势<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>
 </p><ol>
  <li>减少了所需的时间和存储空间。</li>
  <li>多重共线性的消除改善了机器学习模型参数的解释。</li>
  <li>当数据缩小到非常低的维度(如2D或3D)时，更容易可视化。</li>
  <li>避免维数灾难。</li>
 </ol><p></p></div></div><div id="par_14995490592522757"><h2 class="title">5 应用<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>神经科学中有时使用的降维技术是信息量最大的维度，采用低维代表所收集的数据集，从而尽可能多地保留关于原始数据的信息。</p></div></div><div id="par_14995490592522758"><h2 class="title">6 笔记<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>
 </p><ol>
  <li id="cite-ref-2">Roweis, S. T.; Saul, L. K. (2000). "Nonlinear Dimensionality Reduction by Locally Linear Embedding". <i>Science</i>. <b>290</b> (5500): 2323–2326. Bibcode:2000Sci...290.2323R. CiteSeerX&nbsp;10.1.1.111.3313. doi:10.1126/science.290.5500.2323. PMID&nbsp;11125150.</li>
  <li id="cite-ref-4">Pudil, P.; Novovičová, J. (1998). "Novel Methods for Feature Subset Selection with Respect to Problem Knowledge". In Liu, Huan; Motoda, Hiroshi. <i>Feature Extraction, Construction and Selection</i>. p.&nbsp;101. doi:10.1007/978-1-4615-5725-8_7. ISBN&nbsp;978-1-4613-7622-4. PMID&nbsp;[1] Check <code class="cs1-code">|pmid=</code> value (help).</li>
  <li id="cite-ref-6">Rico-Sulayes, Antonio (2017). "Reducing Vector Space Dimensionality in Automatic Classification for Authorship Attribution". <i>Revista Ingeniería Electrónica, Automática y Comunicaciones</i>. <b>38</b> (3): 26–35.</li>
  <li id="cite-ref-8">Samet, H. (2006) <i>Foundations of Multidimensional and Metric Data Structures</i>. Morgan Kaufmann. ISBN 0-12-369446-9</li>
  <li id="cite-ref-10">C. Ding, X. He, H. Zha, H.D. Simon, Adaptive Dimension Reduction for Clustering High Dimensional Data, Proceedings of International Conference on Data Mining, 2002</li>
  <li id="cite-ref-12">Lu, Haiping; Plataniotis, K.N.; Venetsanopoulos, A.N. (2011). "A Survey of Multilinear Subspace Learning for Tensor Data" (PDF). <i>Pattern Recognition</i>. <b>44</b> (7): 1540–1551. doi:10.1016/j.patcog.2011.01.004.</li>
  <li id="cite-ref-14">Daniel D. Lee &amp; H. Sebastian Seung (1999). "Learning the parts of objects by non-negative matrix factorization". <i>Nature</i>. <b>401</b> (6755): 788–791. Bibcode:1999Natur.401..788L. doi:10.1038/44565. PMID&nbsp;10548103.</li>
  <li id="cite-ref-16">Daniel D. Lee &amp; H. Sebastian Seung (2001). <i>Algorithms for Non-negative Matrix Factorization</i> (PDF). Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference. MIT Press. pp.&nbsp;556–562.</li>
  <li id="cite-ref-18">Blanton, Michael R.; Roweis, Sam (2007). "K-corrections and filter transformations in the ultraviolet, optical, and near infrared". <i>The Astronomical Journal</i>. <b>133</b> (2): 734–754. arXiv:astro-ph/0606170. Bibcode:2007AJ....133..734B. doi:10.1086/510127.</li>
  <li id="cite-ref-20">Ren, Bin; Pueyo, Laurent; Zhu, Guangtun B.; Duchêne, Gaspard (2018). "Non-negative Matrix Factorization: Robust Extraction of Extended Structures". <i>The Astrophysical Journal</i>. <b>852</b> (2): 104. arXiv:1712.10317. Bibcode:2018ApJ...852..104R. doi:10.3847/1538-4357/aaa1f2.</li>
  <li id="cite-ref-22">Zhu, Guangtun B. (2016-12-19). "Nonnegative Matrix Factorization (NMF) with Heteroscedastic Uncertainties and Missing data". arXiv:1612.06037 [astro-ph.IM].</li>
  <li id="cite-ref-24">Zhang, Zhenyue; Zha, Hongyuan (2004). "Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment". <i>SIAM Journal on Scientific Computing</i>. <b>26</b> (1): 313–338. doi:10.1137/s1064827502419154.</li>
  <li id="cite-ref-26">Bengio, Yoshua; Monperrus, Martin; Larochelle, Hugo (2006). "Nonlocal Estimation of Manifold Structure". <i>Neural Computation</i> (in 英语). <b>18</b> (10): 2509–2528. CiteSeerX&nbsp;10.1.1.116.4230. doi:10.1162/neco.2006.18.10.2509. PMID&nbsp;16907635.</li>
  <li id="cite-ref-28">Hongbing Hu, Stephen A. Zahorian, (2010) "Dimensionality Reduction Methods for HMM Phonetic Recognition," ICASSP 2010, Dallas, TX</li>
  <li id="cite-ref-30">Baudat, G.; Anouar, F. (2000). "Generalized Discriminant Analysis Using a Kernel Approach". <i>Neural Computation</i>. <b>12</b> (10): 2385–2404. CiteSeerX&nbsp;10.1.1.412.760. doi:10.1162/089976600300014980.</li>
  <li id="cite-ref-32">Haghighat, Mohammad; Zonouz, Saman; Abdel-Mottaleb, Mohamed (2015). "CloudID: Trustworthy cloud-based and cross-enterprise biometric identification". <i>Expert Systems with Applications</i>. <b>42</b> (21): 7905–7916. doi:10.1016/j.eswa.2015.06.025.</li>
  <li id="cite-ref-34">Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, Uri Shaft (1999) "When is “nearest neighbor” meaningful?". <i>Database Theory—ICDT99</i>, 217–235</li>
  <li id="cite-ref-36">Shaw, B.; Jebara, T. (2009). "Structure preserving embedding" (PDF). <i>Proceedings of the 26th Annual International Conference on Machine Learning – ICML '09</i>. p.&nbsp;1. CiteSeerX&nbsp;10.1.1.161.451. doi:10.1145/1553374.1553494. ISBN&nbsp;9781605585161.</li>
  <li id="cite-ref-38">Bingham, E.; Mannila, H. (2001). "Random projection in dimensionality reduction". <i>Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining – KDD '01</i>. p.&nbsp;245. doi:10.1145/502512.502546. ISBN&nbsp;978-1581133912.</li>
  <li id="cite-ref-40">Shasha, D High (2004) <i>Performance Discovery in Time Series</i> Berlin: Springer. ISBN 0-387-00857-8</li>
 </ol><p></p></div></div></div></div><div id="references"><h2 class="title" id="par_references">参考文献</h2><ul class="references"><li id="quote_1"><span class="references-num">[1]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Roweis, S. T.; Saul, L. K. (2000). "Nonlinear Dimensionality Reduction by Locally Linear Embedding". Science. 290 (5500): 2323–2326. Bibcode:2000Sci...290.2323R. CiteSeerX 10.1.1.111.3313. doi:10.1126/science.290.5500.2323. PMID 11125150..</span></p></li><li id="quote_2"><span class="references-num">[2]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Pudil, P.; Novovičová, J. (1998). "Novel Methods for Feature Subset Selection with Respect to Problem Knowledge". In Liu, Huan; Motoda, Hiroshi. Feature Extraction, Construction and Selection. p. 101. doi:10.1007/978-1-4615-5725-8_7. ISBN 978-1-4613-7622-4. PMID [1] Check |pmid= value (help)..</span></p></li><li id="quote_3"><span class="references-num">[3]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Rico-Sulayes, Antonio (2017). "Reducing Vector Space Dimensionality in Automatic Classification for Authorship Attribution". Revista Ingeniería Electrónica, Automática y Comunicaciones. 38 (3): 26–35..</span></p></li><li id="quote_4"><span class="references-num">[4]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Samet, H. (2006) Foundations of Multidimensional and Metric Data Structures. Morgan Kaufmann. ISBN 0-12-369446-9.</span></p></li><li id="quote_5"><span class="references-num">[5]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">C. Ding, X. He, H. Zha, H.D. Simon, Adaptive Dimension Reduction for Clustering High Dimensional Data, Proceedings of International Conference on Data Mining, 2002.</span></p></li><li id="quote_6"><span class="references-num">[6]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Lu, Haiping; Plataniotis, K.N.; Venetsanopoulos, A.N. (2011). "A Survey of Multilinear Subspace Learning for Tensor Data" (PDF). Pattern Recognition. 44 (7): 1540–1551. doi:10.1016/j.patcog.2011.01.004..</span></p></li><li id="quote_7"><span class="references-num">[7]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Daniel D. Lee &amp; H. Sebastian Seung (1999). "Learning the parts of objects by non-negative matrix factorization". Nature. 401 (6755): 788–791. Bibcode:1999Natur.401..788L. doi:10.1038/44565. PMID 10548103..</span></p></li><li id="quote_8"><span class="references-num">[8]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Daniel D. Lee &amp; H. Sebastian Seung (2001). Algorithms for Non-negative Matrix Factorization (PDF). Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference. MIT Press. pp. 556–562..</span></p></li><li id="quote_9"><span class="references-num">[9]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Blanton, Michael R.; Roweis, Sam (2007). "K-corrections and filter transformations in the ultraviolet, optical, and near infrared". The Astronomical Journal. 133 (2): 734–754. arXiv:astro-ph/0606170. Bibcode:2007AJ....133..734B. doi:10.1086/510127..</span></p></li><li id="quote_10"><span class="references-num">[10]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Ren, Bin; Pueyo, Laurent; Zhu, Guangtun B.; Duchêne, Gaspard (2018). "Non-negative Matrix Factorization: Robust Extraction of Extended Structures". The Astrophysical Journal. 852 (2): 104. arXiv:1712.10317. Bibcode:2018ApJ...852..104R. doi:10.3847/1538-4357/aaa1f2..</span></p></li><li id="quote_11"><span class="references-num">[11]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Zhu, Guangtun B. (2016-12-19). "Nonnegative Matrix Factorization (NMF) with Heteroscedastic Uncertainties and Missing data". arXiv:1612.06037 [astro-ph.IM]..</span></p></li><li id="quote_12"><span class="references-num">[12]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Zhang, Zhenyue; Zha, Hongyuan (2004). "Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment". SIAM Journal on Scientific Computing. 26 (1): 313–338. doi:10.1137/s1064827502419154..</span></p></li><li id="quote_13"><span class="references-num">[13]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Bengio, Yoshua; Monperrus, Martin; Larochelle, Hugo (2006). "Nonlocal Estimation of Manifold Structure". Neural Computation (in 英语). 18 (10): 2509–2528. CiteSeerX 10.1.1.116.4230. doi:10.1162/neco.2006.18.10.2509. PMID 16907635..</span></p></li><li id="quote_14"><span class="references-num">[14]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Hongbing Hu, Stephen A. Zahorian, (2010) "Dimensionality Reduction Methods for HMM Phonetic Recognition," ICASSP 2010, Dallas, TX.</span></p></li><li id="quote_15"><span class="references-num">[15]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Baudat, G.; Anouar, F. (2000). "Generalized Discriminant Analysis Using a Kernel Approach". Neural Computation. 12 (10): 2385–2404. CiteSeerX 10.1.1.412.760. doi:10.1162/089976600300014980..</span></p></li><li id="quote_16"><span class="references-num">[16]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Haghighat, Mohammad; Zonouz, Saman; Abdel-Mottaleb, Mohamed (2015). "CloudID: Trustworthy cloud-based and cross-enterprise biometric identification". Expert Systems with Applications. 42 (21): 7905–7916. doi:10.1016/j.eswa.2015.06.025..</span></p></li><li id="quote_17"><span class="references-num">[17]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, Uri Shaft (1999) "When is “nearest neighbor” meaningful?". Database Theory—ICDT99, 217–235.</span></p></li><li id="quote_18"><span class="references-num">[18]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Shaw, B.; Jebara, T. (2009). "Structure preserving embedding" (PDF). Proceedings of the 26th Annual International Conference on Machine Learning – ICML '09. p. 1. CiteSeerX 10.1.1.161.451. doi:10.1145/1553374.1553494. ISBN 9781605585161..</span></p></li><li id="quote_19"><span class="references-num">[19]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Bingham, E.; Mannila, H. (2001). "Random projection in dimensionality reduction". Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining – KDD '01. p. 245. doi:10.1145/502512.502546. ISBN 978-1581133912..</span></p></li><li id="quote_20"><span class="references-num">[20]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Shasha, D High (2004) Performance Discovery in Time Series Berlin: Springer. ISBN 0-387-00857-8.</span></p></li></ul></div><div class="read-num">阅读 <!-- -->2283</div></div><div class="right-side" id="rightSide"><div class="side" id="lemma-side"><div class="side-title">版本记录</div><ul class="side-lst"><li><p class="side-lst-txt">暂无</p></li></ul><div class="user-card userCard"></div></div><div class="side"><div class="side-event"></div></div></div></div><div class="footer-box"><div id="footer"><div class="footer-logo-wrap"><div class="footer-logo"></div><div class="footer-logo-text">知识·传播·科普</div></div><div class="footer-info">本网站内容采用<a target="_blank" href="https://web.archive.org/web/20221025115712/https://creativecommons.org/licenses/by-sa/3.0/deed.zh?tdsourcetag=s_pctim_aiomsg">CC-BY-SA 3.0</a>授权</div><div class="footer-btn-wrap"><a target="_blank" href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/help/#user_protocol">用户协议</a><a target="_blank" href="https://web.archive.org/web/20221025115712/http://www.sogou.com/docs/terms.htm?v=1">免责声明</a><a target="_blank" href="https://web.archive.org/web/20221025115712/http://corp.sogou.com/private.html">隐私政策</a><a target="_blank" href="https://web.archive.org/web/20221025115712/https://baike.sogou.com/kexue/intro.htm">关于我们</a></div></div></div><script>window.lemmaInfo ={"lemmaId":"10751","versionId":"53985499303436293","title":"降维","subtitle":"","abstracts":{"paragraphId":"53985499320213506","title":"摘要","versionId":"53985499303436294","lemmaId":10751,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82859321,"name":"━╋独特","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599473865,"comment":null,"dependVersionId":0,"contentType":2,"content":"<p>在统计学、机器学习和信息论中，降维是通过获得一组主要变量并减少所考虑的随机变量数量的过程<sup><a href=\"#quote_1\" class=\"kx_ref\">[1]</a></sup>。主要分为特征选择和特征提取。<sup><a href=\"#quote_2\" class=\"kx_ref\">[2]</a></sup> </p>","pics":null,"card":null,"references":[],"versionCount":0},"card":{"paragraphId":"0","title":"","versionId":"0","lemmaId":0,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":0,"comment":null,"dependVersionId":0,"contentType":0,"content":"","pics":null,"card":null,"references":null,"versionCount":0},"categories":[{"id":1,"name":"计算机","parents":[]}],"creator":{"uid":10145103,"name":"柚子otto","pic":"https://web.archive.org/web/20221025115712/https://img02.sogoucdn.com/app/a/200698/1152_1152_1864088_20200427232849-1518843971.png","introduction":"","educations":[{"schoolName":"中国地质大学（北京）","major":"","degree":"本科","universityId":22,"universityLogo":"https://web.archive.org/web/20221025115712/https://img01.sogoucdn.com/app/a/200943/d3465c1c-6011-11e9-b353-fc4dd4f70029","majorLevel1":"理学","majorLevel2":"地理学","majorLevel3":"地图学与地理信息系统","majorLevel1Id":1,"majorLevel2Id":103,"majorLevel3Id":109,"state":"毕业","lab":"","researchField":""}],"jobs":[{"company":"搜狗","title":"产品经理"}],"works":null,"educationBrief":"中国地质大学（北京）","jobBrief":"产品经理","role":0,"roleName":null,"title":"中国地质大学（北京） · 地理学本科","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":139,"partnerIdCreateTime":1595844881,"partnerIdPoped":true},"createTime":1568626647,"editor":{"uid":10145103,"name":"柚子otto","pic":"https://web.archive.org/web/20221025115712/https://img02.sogoucdn.com/app/a/200698/1152_1152_1864088_20200427232849-1518843971.png","introduction":"","educations":[{"schoolName":"中国地质大学（北京）","major":"","degree":"本科","universityId":22,"universityLogo":"https://web.archive.org/web/20221025115712/https://img01.sogoucdn.com/app/a/200943/d3465c1c-6011-11e9-b353-fc4dd4f70029","majorLevel1":"理学","majorLevel2":"地理学","majorLevel3":"地图学与地理信息系统","majorLevel1Id":1,"majorLevel2Id":103,"majorLevel3Id":109,"state":"毕业","lab":"","researchField":""}],"jobs":[{"company":"搜狗","title":"产品经理"}],"works":null,"educationBrief":"中国地质大学（北京）","jobBrief":"产品经理","role":0,"roleName":null,"title":"中国地质大学（北京） · 地理学本科","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":139,"partnerIdCreateTime":1595844881,"partnerIdPoped":true},"editTime":1576234008,"state":1,"versionCount":1,"upNum":3,"downNum":0,"pics":[],"catalogs":[{"level":1,"title":"特征选择","paragraphId":"14995490575745557","subCatalogs":null},{"level":1,"title":"特征投影","paragraphId":"14995490592522754","subCatalogs":[{"level":2,"title":"主成分分析(PCA)","paragraphId":"14995490592522754","subCatalogs":null},{"level":2,"title":"非负矩阵分解(NMF)","paragraphId":"14995490592522754","subCatalogs":null},{"level":2,"title":"内核主成分分析","paragraphId":"14995490592522754","subCatalogs":null},{"level":2,"title":"基于图的核主成分分析","paragraphId":"14995490592522754","subCatalogs":null},{"level":2,"title":"线性判别分析（LDA）","paragraphId":"14995490592522754","subCatalogs":null},{"level":2,"title":"广义判别分析(GDA)","paragraphId":"14995490592522754","subCatalogs":null},{"level":2,"title":"自动编码器","paragraphId":"14995490592522754","subCatalogs":null}]},{"level":1,"title":"降维","paragraphId":"14995490592522755","subCatalogs":null},{"level":1,"title":"降维的优势","paragraphId":"14995490592522756","subCatalogs":null},{"level":1,"title":"应用","paragraphId":"14995490592522757","subCatalogs":null},{"level":1,"title":"笔记","paragraphId":"14995490592522758","subCatalogs":null},{"level":1,"title":"参考文献","paragraphId":"-1","subCatalogs":null}],"paragraphs":[{"paragraphId":"14995490575745557","title":"特征选择","versionId":"53985499303436295","lemmaId":10751,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82859321,"name":"━╋独特","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599473865,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>特征选择方法试图找到原始变量的子集(也称为特征或属性)。有三种策略：过滤策略(如信息增益)，包装策略（如由准确性引导的搜索）和嵌入策略（如选择特征以在基于预测误差构建模型时添加或移除特征）。</p><p>在某些情况下，回归或分类等数据分析在缩小的空间中比在原始空间中可以更精确地完成。<sup><a href=\"#quote_3\" class=\"kx_ref\">[3]</a></sup> </p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14995490592522754","title":"特征投影","versionId":"53985499303436296","lemmaId":10751,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82859321,"name":"━╋独特","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599473865,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>特征投影(也称为特征提取)将高维空间中的数据转换为更少维度的空间。数据转换可以是线性的，如主成分分析，但也存在许多非线性降维技术。<sup><a href=\"#quote_4\" class=\"kx_ref\">[4]</a></sup><sup><a href=\"#quote_5\" class=\"kx_ref\">[5]</a></sup>对于多维数据，张量表示可以通过多线性子空间的学习进行降维。<sup><a href=\"#quote_6\" class=\"kx_ref\">[6]</a></sup> </p><p><h3>主成分分析(PCA)</h3></p><p>降维的主要线性技术，主成分分析，以使低维表示中的数据方差最大化的方式，执行数据到低维空间的线性映射。在实践中，构建数据的协方差(或相关性)矩阵，并计算该矩阵上的特征向量。与最大特征值(主成分)相对应的特征向量现在可以用于重构原始数据的大部分方差。此外，前几个特征向量通常可以根据系统的大规模物理行为来解释。原始空间(具有点数的维度)已经减少(存在数据丢失，但是希望保留最重要的方差)到由几个特征向量跨越的空间。</p><p><h3>非负矩阵分解(NMF)</h3></p><p>NMF将非负矩阵分解为两个非负矩阵的乘积，这在只有非负信号存在的领域是一个很有前途的工具。<sup><a href=\"#quote_7\" class=\"kx_ref\">[7]</a></sup><sup><a href=\"#quote_8\" class=\"kx_ref\">[8]</a></sup>比如天文学<sup><a href=\"#quote_9\" class=\"kx_ref\">[9]</a></sup><sup><a href=\"#quote_10\" class=\"kx_ref\">[10]</a></sup>。NMF是众所周知的， 自Lee &amp; Seung的乘法规则更新以来<sup><a href=\"#quote_7\" class=\"kx_ref\">[7]</a></sup>，NMF在不断发展：包含不确定性 <sup><a href=\"#quote_9\" class=\"kx_ref\">[9]</a></sup>、丢失数据和并行计算的考虑 <sup><a href=\"#quote_11\" class=\"kx_ref\">[11]</a></sup>，顺序构造<sup><a href=\"#quote_11\" class=\"kx_ref\">[11]</a></sup> 导致了NMF的稳定性和线性度<sup><a href=\"#quote_10\" class=\"kx_ref\">[10]</a></sup>，同时也有其他更新。</p><p>在构造期间具有稳定的组件基础，以及线性建模过程，顺序NMF<sup><a href=\"#quote_11\" class=\"kx_ref\">[11]</a></sup> 作为探测系外行星的方法之一，特别是用于星周盘的直接成像，能够保持天体中星周结构直接成像的通量<sup><a href=\"#quote_10\" class=\"kx_ref\">[10]</a></sup>。与主成分分析相比，NMF没有去除导致非物理非负通量的矩阵平均值，因此NMF能够保存比主成分分析更多的信息（Ren 等人所证明）<sup><a href=\"#quote_10\" class=\"kx_ref\">[10]</a></sup>。</p><p><h3>内核主成分分析</h3></p><p>主成分分析可以通过核技巧以非线性方式使用。由此产生的技术能够构造非线性映射，使数据中的方差最大化。这种技术被称为内核主成分分析。</p><p><h3>基于图的核主成分分析</h3></p><p>其他突出的非线性技术包括流形学习技术，如Isomap、局部线性嵌入(LLE)、Hessian LLE、拉普拉斯特征映射和基于tangent空间分析的方法<sup><a href=\"#quote_12\" class=\"kx_ref\">[12]</a></sup><sup><a href=\"#quote_13\" class=\"kx_ref\">[13]</a></sup>。这些技术使用保持数据局部属性的成本函数来构建低维数据表示，并且可以被视为基于图形内核的PCA定义。</p><p>最近，有人提出了一些技术，这些技术不是定义一个固定的内核，而是尝试使用半定编程来学习内核。这种技术最突出的例子是最大方差展开(MVU)。MVU的中心思想是精确地保留最近邻之间的所有成对距离(在内积空间中)，同时最大化不是最近邻的点之间的距离。</p><p>邻域保持的另一种方法是通过最小化测量输入和输出空间距离差异的成本函数。这种技术的重要例子包括：经典多维标度，与主成分分析相同；Isomap，使用数据空间中的测地距离；扩散图，使用数据空间中的扩散距离；t分布式随机近邻嵌入(t-SNE)，它最大限度的减少了点对分布之间的差异；曲元分析。</p><p>非线性降维的另一种方法是使用自动编码器，这是一种特殊的前馈神经网络，具有瓶颈隐藏层。<sup><a href=\"#quote_14\" class=\"kx_ref\">[14]</a></sup>深度编码器的训练通常使用贪婪的分层预训练(例如，使用一堆受限的玻尔兹曼机器)来执行，随后是基于反向传播的微调阶段。</p><p><h3>线性判别分析（LDA）</h3></p><p>线性判别式分析(LDA)是费希尔线性判别式(Fisher linear discrimina tion)的推广，费希尔线性判别式是一种用于统计、模式识别和机器学习的方法，用于寻找表征或分离两类或更多类对象或事件的特征的线性组合。</p><p><h3>广义判别分析(GDA)</h3></p><p>GDA利用核函数算子处理非线性判别分析。基础理论与支持向量机(SVM)很接近，因为GDA方法提供了输入向量到高维特征空间的映射。<sup><a href=\"#quote_15\" class=\"kx_ref\">[15]</a></sup><sup><a href=\"#quote_16\" class=\"kx_ref\">[16]</a></sup>与LDA相似，GDA的目标是通过最大化类间散射与类内散射的比率，找到特征空间在较低维空间中的投影。</p><p><h3>自动编码器</h3></p><p>自动编码器可用于学习非线性降维函数和编码以及从编码到原始表示的反函数。</p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14995490592522755","title":"降维","versionId":"53985499303436297","lemmaId":10751,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82859321,"name":"━╋独特","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599473865,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>对于高维数据集(即维数大于10的数据集)，为了避免维数灾难的影响，通常在应用K-最近邻算法(k-NN)之前进行降维。<sup><a href=\"#quote_17\" class=\"kx_ref\">[17]</a></sup> </p><p>使用主成分分析（PCA），线性判别分析（LDA），典型相关分析（CCA）或非负矩阵分解（NMF）技术作为预处理步骤，可以在一个步骤中通过K-NN对缩小空间中的特征向量进行聚类来组合特征提取和降维。 在机器学习中，这个过程也称为低维嵌入。<sup><a href=\"#quote_18\" class=\"kx_ref\">[18]</a></sup> </p><p>对于超高维数据集(例如，当对实时视频流、DNA数据或高维时间序列执行相似性搜索时)，使用局部敏感散列、随机投影，<sup><a href=\"#quote_19\" class=\"kx_ref\">[19]</a></sup> “草图” <sup><a href=\"#quote_20\" class=\"kx_ref\">[20]</a></sup> 或者其他高维相似性搜索时，采用VLDB工具箱技术可能是唯一可行的选择。</p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14995490592522756","title":"降维的优势","versionId":"53985499303436298","lemmaId":10751,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82859321,"name":"━╋独特","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599473865,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>\n <ol>\n  <li>减少了所需的时间和存储空间。</li>\n  <li>多重共线性的消除改善了机器学习模型参数的解释。</li>\n  <li>当数据缩小到非常低的维度(如2D或3D)时，更容易可视化。</li>\n  <li>避免维数灾难。</li>\n </ol></p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14995490592522757","title":"应用","versionId":"53985499303436299","lemmaId":10751,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82859321,"name":"━╋独特","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599473865,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>神经科学中有时使用的降维技术是信息量最大的维度，采用低维代表所收集的数据集，从而尽可能多地保留关于原始数据的信息。</p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14995490592522758","title":"笔记","versionId":"53985499303436300","lemmaId":10751,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82859321,"name":"━╋独特","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599473865,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>\n <ol>\n  <li id=\"cite-ref-2\">Roweis, S. T.; Saul, L. K. (2000). \"Nonlinear Dimensionality Reduction by Locally Linear Embedding\". <i>Science</i>. <b>290</b> (5500): 2323–2326. Bibcode:2000Sci...290.2323R. CiteSeerX&nbsp;10.1.1.111.3313. doi:10.1126/science.290.5500.2323. PMID&nbsp;11125150.</li>\n  <li id=\"cite-ref-4\">Pudil, P.; Novovičová, J. (1998). \"Novel Methods for Feature Subset Selection with Respect to Problem Knowledge\". In Liu, Huan; Motoda, Hiroshi. <i>Feature Extraction, Construction and Selection</i>. p.&nbsp;101. doi:10.1007/978-1-4615-5725-8_7. ISBN&nbsp;978-1-4613-7622-4. PMID&nbsp;[1] Check <code class=\"cs1-code\">|pmid=</code> value (help).</li>\n  <li id=\"cite-ref-6\">Rico-Sulayes, Antonio (2017). \"Reducing Vector Space Dimensionality in Automatic Classification for Authorship Attribution\". <i>Revista Ingeniería Electrónica, Automática y Comunicaciones</i>. <b>38</b> (3): 26–35.</li>\n  <li id=\"cite-ref-8\">Samet, H. (2006) <i>Foundations of Multidimensional and Metric Data Structures</i>. Morgan Kaufmann. ISBN 0-12-369446-9</li>\n  <li id=\"cite-ref-10\">C. Ding, X. He, H. Zha, H.D. Simon, Adaptive Dimension Reduction for Clustering High Dimensional Data, Proceedings of International Conference on Data Mining, 2002</li>\n  <li id=\"cite-ref-12\">Lu, Haiping; Plataniotis, K.N.; Venetsanopoulos, A.N. (2011). \"A Survey of Multilinear Subspace Learning for Tensor Data\" (PDF). <i>Pattern Recognition</i>. <b>44</b> (7): 1540–1551. doi:10.1016/j.patcog.2011.01.004.</li>\n  <li id=\"cite-ref-14\">Daniel D. Lee &amp; H. Sebastian Seung (1999). \"Learning the parts of objects by non-negative matrix factorization\". <i>Nature</i>. <b>401</b> (6755): 788–791. Bibcode:1999Natur.401..788L. doi:10.1038/44565. PMID&nbsp;10548103.</li>\n  <li id=\"cite-ref-16\">Daniel D. Lee &amp; H. Sebastian Seung (2001). <i>Algorithms for Non-negative Matrix Factorization</i> (PDF). Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference. MIT Press. pp.&nbsp;556–562.</li>\n  <li id=\"cite-ref-18\">Blanton, Michael R.; Roweis, Sam (2007). \"K-corrections and filter transformations in the ultraviolet, optical, and near infrared\". <i>The Astronomical Journal</i>. <b>133</b> (2): 734–754. arXiv:astro-ph/0606170. Bibcode:2007AJ....133..734B. doi:10.1086/510127.</li>\n  <li id=\"cite-ref-20\">Ren, Bin; Pueyo, Laurent; Zhu, Guangtun B.; Duchêne, Gaspard (2018). \"Non-negative Matrix Factorization: Robust Extraction of Extended Structures\". <i>The Astrophysical Journal</i>. <b>852</b> (2): 104. arXiv:1712.10317. Bibcode:2018ApJ...852..104R. doi:10.3847/1538-4357/aaa1f2.</li>\n  <li id=\"cite-ref-22\">Zhu, Guangtun B. (2016-12-19). \"Nonnegative Matrix Factorization (NMF) with Heteroscedastic Uncertainties and Missing data\". arXiv:1612.06037 [astro-ph.IM].</li>\n  <li id=\"cite-ref-24\">Zhang, Zhenyue; Zha, Hongyuan (2004). \"Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment\". <i>SIAM Journal on Scientific Computing</i>. <b>26</b> (1): 313–338. doi:10.1137/s1064827502419154.</li>\n  <li id=\"cite-ref-26\">Bengio, Yoshua; Monperrus, Martin; Larochelle, Hugo (2006). \"Nonlocal Estimation of Manifold Structure\". <i>Neural Computation</i> (in 英语). <b>18</b> (10): 2509–2528. CiteSeerX&nbsp;10.1.1.116.4230. doi:10.1162/neco.2006.18.10.2509. PMID&nbsp;16907635.</li>\n  <li id=\"cite-ref-28\">Hongbing Hu, Stephen A. Zahorian, (2010) \"Dimensionality Reduction Methods for HMM Phonetic Recognition,\" ICASSP 2010, Dallas, TX</li>\n  <li id=\"cite-ref-30\">Baudat, G.; Anouar, F. (2000). \"Generalized Discriminant Analysis Using a Kernel Approach\". <i>Neural Computation</i>. <b>12</b> (10): 2385–2404. CiteSeerX&nbsp;10.1.1.412.760. doi:10.1162/089976600300014980.</li>\n  <li id=\"cite-ref-32\">Haghighat, Mohammad; Zonouz, Saman; Abdel-Mottaleb, Mohamed (2015). \"CloudID: Trustworthy cloud-based and cross-enterprise biometric identification\". <i>Expert Systems with Applications</i>. <b>42</b> (21): 7905–7916. doi:10.1016/j.eswa.2015.06.025.</li>\n  <li id=\"cite-ref-34\">Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, Uri Shaft (1999) \"When is “nearest neighbor” meaningful?\". <i>Database Theory—ICDT99</i>, 217–235</li>\n  <li id=\"cite-ref-36\">Shaw, B.; Jebara, T. (2009). \"Structure preserving embedding\" (PDF). <i>Proceedings of the 26th Annual International Conference on Machine Learning – ICML '09</i>. p.&nbsp;1. CiteSeerX&nbsp;10.1.1.161.451. doi:10.1145/1553374.1553494. ISBN&nbsp;9781605585161.</li>\n  <li id=\"cite-ref-38\">Bingham, E.; Mannila, H. (2001). \"Random projection in dimensionality reduction\". <i>Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining – KDD '01</i>. p.&nbsp;245. doi:10.1145/502512.502546. ISBN&nbsp;978-1581133912.</li>\n  <li id=\"cite-ref-40\">Shasha, D High (2004) <i>Performance Discovery in Time Series</i> Berlin: Springer. ISBN 0-387-00857-8</li>\n </ol></p>","pics":null,"card":null,"references":[],"versionCount":0}],"references":[{"id":1,"type":"book","title":"Roweis, S. T.; Saul, L. K. (2000). \"Nonlinear Dimensionality Reduction by Locally Linear Embedding\". Science. 290 (5500): 2323–2326. Bibcode:2000Sci...290.2323R. CiteSeerX 10.1.1.111.3313. doi:10.1126/science.290.5500.2323. PMID 11125150.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":2,"type":"book","title":"Pudil, P.; Novovičová, J. (1998). \"Novel Methods for Feature Subset Selection with Respect to Problem Knowledge\". In Liu, Huan; Motoda, Hiroshi. Feature Extraction, Construction and Selection. p. 101. doi:10.1007/978-1-4615-5725-8_7. ISBN 978-1-4613-7622-4. PMID [1] Check |pmid= value (help).","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":3,"type":"book","title":"Rico-Sulayes, Antonio (2017). \"Reducing Vector Space Dimensionality in Automatic Classification for Authorship Attribution\". Revista Ingeniería Electrónica, Automática y Comunicaciones. 38 (3): 26–35.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":4,"type":"book","title":"Samet, H. (2006) Foundations of Multidimensional and Metric Data Structures. Morgan Kaufmann. ISBN 0-12-369446-9","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":5,"type":"book","title":"C. Ding, X. He, H. Zha, H.D. Simon, Adaptive Dimension Reduction for Clustering High Dimensional Data, Proceedings of International Conference on Data Mining, 2002","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":6,"type":"book","title":"Lu, Haiping; Plataniotis, K.N.; Venetsanopoulos, A.N. (2011). \"A Survey of Multilinear Subspace Learning for Tensor Data\" (PDF). Pattern Recognition. 44 (7): 1540–1551. doi:10.1016/j.patcog.2011.01.004.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":7,"type":"book","title":"Daniel D. Lee & H. Sebastian Seung (1999). \"Learning the parts of objects by non-negative matrix factorization\". Nature. 401 (6755): 788–791. Bibcode:1999Natur.401..788L. doi:10.1038/44565. PMID 10548103.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":8,"type":"book","title":"Daniel D. Lee & H. Sebastian Seung (2001). Algorithms for Non-negative Matrix Factorization (PDF). Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference. MIT Press. pp. 556–562.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":9,"type":"book","title":"Blanton, Michael R.; Roweis, Sam (2007). \"K-corrections and filter transformations in the ultraviolet, optical, and near infrared\". The Astronomical Journal. 133 (2): 734–754. arXiv:astro-ph/0606170. Bibcode:2007AJ....133..734B. doi:10.1086/510127.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":10,"type":"book","title":"Ren, Bin; Pueyo, Laurent; Zhu, Guangtun B.; Duchêne, Gaspard (2018). \"Non-negative Matrix Factorization: Robust Extraction of Extended Structures\". The Astrophysical Journal. 852 (2): 104. arXiv:1712.10317. Bibcode:2018ApJ...852..104R. doi:10.3847/1538-4357/aaa1f2.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":11,"type":"book","title":"Zhu, Guangtun B. (2016-12-19). \"Nonnegative Matrix Factorization (NMF) with Heteroscedastic Uncertainties and Missing data\". arXiv:1612.06037 [astro-ph.IM].","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":12,"type":"book","title":"Zhang, Zhenyue; Zha, Hongyuan (2004). \"Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment\". SIAM Journal on Scientific Computing. 26 (1): 313–338. doi:10.1137/s1064827502419154.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":13,"type":"book","title":"Bengio, Yoshua; Monperrus, Martin; Larochelle, Hugo (2006). \"Nonlocal Estimation of Manifold Structure\". Neural Computation (in 英语). 18 (10): 2509–2528. CiteSeerX 10.1.1.116.4230. doi:10.1162/neco.2006.18.10.2509. PMID 16907635.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":14,"type":"book","title":"Hongbing Hu, Stephen A. Zahorian, (2010) \"Dimensionality Reduction Methods for HMM Phonetic Recognition,\" ICASSP 2010, Dallas, TX","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":15,"type":"book","title":"Baudat, G.; Anouar, F. (2000). \"Generalized Discriminant Analysis Using a Kernel Approach\". Neural Computation. 12 (10): 2385–2404. CiteSeerX 10.1.1.412.760. doi:10.1162/089976600300014980.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":16,"type":"book","title":"Haghighat, Mohammad; Zonouz, Saman; Abdel-Mottaleb, Mohamed (2015). \"CloudID: Trustworthy cloud-based and cross-enterprise biometric identification\". Expert Systems with Applications. 42 (21): 7905–7916. doi:10.1016/j.eswa.2015.06.025.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":17,"type":"book","title":"Kevin Beyer, Jonathan Goldstein, Raghu Ramakrishnan, Uri Shaft (1999) \"When is “nearest neighbor” meaningful?\". Database Theory—ICDT99, 217–235","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":18,"type":"book","title":"Shaw, B.; Jebara, T. (2009). \"Structure preserving embedding\" (PDF). Proceedings of the 26th Annual International Conference on Machine Learning – ICML '09. p. 1. CiteSeerX 10.1.1.161.451. doi:10.1145/1553374.1553494. ISBN 9781605585161.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":19,"type":"book","title":"Bingham, E.; Mannila, H. (2001). \"Random projection in dimensionality reduction\". Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining – KDD '01. p. 245. doi:10.1145/502512.502546. ISBN 978-1581133912.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":20,"type":"book","title":"Shasha, D High (2004) Performance Discovery in Time Series Berlin: Springer. ISBN 0-387-00857-8","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false}],"recommendReferences":null,"auditState":2,"lemmaLevel":1,"origin":0,"originEnTitle":null,"originZhTitle":null,"pv":2283,"auditType":0,"synonyms":null,"showEditTime":"2019.12.13 18:46","auditors":[{"uid":0,"name":"Ki.κe","pic":"https://web.archive.org/web/20221025115712/https://wx.qlogo.cn/mmopen/vi_32/y67kfr32Doib4wg71Jiau7jVWvharic3nRKgdRRQSl6koeQJCo0GQs2Krw0vwdFRsOWnHIQOwAZsSg5lIkIrFCOcQ/132","introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":"","jobBrief":"","role":0,"roleName":null,"title":"","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false}],"hasZhishiNav":false,"auditInfos":{},"isHistory":false};</script><script crossorigin="anonymous" src="./751.降维 - 搜狗科学百科_files/aegis.min.js.download"></script><script crossorigin="anonymous" src="./751.降维 - 搜狗科学百科_files/main_2020092401.js.download"></script><script crossorigin="anonymous" src="./751.降维 - 搜狗科学百科_files/react.production.min.js.download"></script><script crossorigin="anonymous" src="./751.降维 - 搜狗科学百科_files/react-dom.production.min.js.download"></script><script crossorigin="anonymous" src="./751.降维 - 搜狗科学百科_files/jquery-1.11.1.min.js.download"></script><script crossorigin="anonymous" src="./751.降维 - 搜狗科学百科_files/main_2022062701.js.download"></script><script crossorigin="anonymous" src="./751.降维 - 搜狗科学百科_files/main_66bbe21.js.download"></script><script crossorigin="anonymous" src="./751.降维 - 搜狗科学百科_files/react.production.min.js.download"></script><script crossorigin="anonymous" src="./751.降维 - 搜狗科学百科_files/react-dom.production.min.js.download"></script><script crossorigin="anonymous" src="./751.降维 - 搜狗科学百科_files/main_edf0f08.js.download"></script><div id="popControl"><div class="pop-control-mask" style="display: none;"><div class="pop-container"><i class="close-icon"></i><div class="stop-title">搜狗科学百科停止服务运营公告</div><div class="stop-user">亲爱的用户：</div><div class="stop-content">因业务方向调整，搜狗科学百科将于2022年11月11日正式停止服务与运营。届时产品内所有数据将依据相关法律进行删除并关闭服务器，您将无法登录及使用搜狗科学百科，相关内容在平台下线后将无法找回。</div><div class="stop-content">搜狗科学百科是由搜狗推出的科学领域专业百科产品，与搜狗百科为两个独立产品，本次停止服务与运营仅对搜狗科学百科，并不影响搜狗百科产品正常使用，敬请知悉。</div><div class="stop-content">由此给您带来的不便我们深表歉意，敬请谅解。再次感谢您一直以来的陪伴与支持！</div><div class="stop-team">搜狗科学百科产品&amp;运营团队</div><div class="stop-time">2022年10月11日</div></div></div></div>
</body></html>