<!DOCTYPE html>
<!-- saved from url=(0095)https://web.archive.org/web/20221028211746/https://baike.sogou.com/kexue/d19606990082810886.htm -->
<html class="" data-reactroot=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./d19606990082810886.BERT - 搜狗科学百科_files/analytics.js.download" type="text/javascript"></script>
<script type="text/javascript">window.addEventListener('DOMContentLoaded',function(){var v=archive_analytics.values;v.service='wb';v.server_name='wwwb-app200.us.archive.org';v.server_ms=618;archive_analytics.send_pageview({});});</script>
<script type="text/javascript" src="./d19606990082810886.BERT - 搜狗科学百科_files/bundle-playback.js.download" charset="utf-8"></script>
<script type="text/javascript" src="./d19606990082810886.BERT - 搜狗科学百科_files/wombat.js.download" charset="utf-8"></script>
<script type="text/javascript">
  __wm.init("https://web.archive.org/web");
  __wm.wombat("https://baike.sogou.com/kexue/d19606990082810886.htm","20221028211746","https://web.archive.org/","web","/_static/",
	      "1666991866");
</script>
<link rel="stylesheet" type="text/css" href="./d19606990082810886.BERT - 搜狗科学百科_files/banner-styles.css">
<link rel="stylesheet" type="text/css" href="./d19606990082810886.BERT - 搜狗科学百科_files/iconochive.css">
<!-- End Wayback Rewrite JS Include -->
<meta name="save" content="history"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="VWGb6TyYx8"><meta content="BERT - 搜狗科学百科" name="keywords"><meta content="搜狗科学百科是一部有着平等、协作、分享、自由理念的网络科学全书，为每一个互联网用户创造一个涵盖所有领域知识、服务的中文知识性平台。" name="description"><meta http-equiv="x-dns-prefetch-control" content="on"><meta name="server" baike="235" ip="210" env="online"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://cache.soso.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://hhy.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://pic.baike.soso.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://ugc.qpic.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://xui.ptlogin2.qq.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://q1.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://q2.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://q3.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://q4.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://q.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://img02.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://img03.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028211746/https://img04.sogoucdn.com/"><link rel="Shortcut Icon" href="https://web.archive.org/web/20221028211746im_/https://www.sogou.com/images/logo/new/favicon.ico?v=4"><link rel="Bookmark" href="https://www.sogou.com/images/logo/new/favicon.ico?v=4"><link href="./d19606990082810886.BERT - 搜狗科学百科_files/base_b849887.css" rel="stylesheet"><link href="./d19606990082810886.BERT - 搜狗科学百科_files/detail_378aed5.css" rel="stylesheet"><link href="./d19606990082810886.BERT - 搜狗科学百科_files/inviteAudit_7894507.css" rel="stylesheet"><link rel="stylesheet" href="./d19606990082810886.BERT - 搜狗科学百科_files/highlight.min.css"><title>BERT - 搜狗科学百科</title><style>.onekey-close {
	position: absolute;
	top: 16px;
	right: 16px;
	width: 24px;
	height: 24px;
	background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	text-indent: -999em;
	background-size: 84px;
	background-position: -63px 0;
}

.onekey-login {
	position: absolute;
	top: 16.4%;
	left: 0;
	right: 0;
	width: 100%;
}

/* .onekey-login-img {
    width: 75px;
    height: 75px;
    background: url("https://web.archive.org/web/20221028211746/https://baike.sogou.com/kexue/images/sprite_wap_baike.png") no-repeat;
    background-size: 100px 91px;
    background-position: 0 0;
    background-repeat: no-repeat;
    margin: 0 auto;
} */

.onekey-login-title {
	text-align: center;
	padding-bottom: 3px;
	font-size: 21px;
	font-weight: bold;
	line-height: 30px;
	color: #000;
}

.onekey-login-txt {
	text-align: center;
	font-family: PingFangSC;
	font-size: 14px;
	line-height: 20px;
	color: #8f8f8f;
}

.onekey-login-qq,
.onekey-login-wx,
.onekey-login-phone {
	display: block;
	width: 245px;
	height: 54px;
	border-radius: 45px;
	text-align: center;

	margin: 0 auto;
	font-size: 17px;
	line-height: 24px;
	color: #000;
	/* padding: 16px 77px; */
	border-radius: 12px;
	border: solid 1px #e0e0e0;
}
.onekey-qq-content,
.onekey-vx-content,
.onekey-phone-content {
	display: inline-block;
	margin-top: 16px;
}
.onekey-qq-content {
	padding: 0 5px;
}

.onekey-login-qq {
	margin-top: 48px;
	margin-bottom: 24px;
}

.onekey-login-qq:before {
	display: inline-block;
	content: "";
	width: 20px;
	height: 20px;
	background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 80px;
	background-position: -20px 0;
	vertical-align: top;
	margin: 17px 8px 0 0;
}

.onekey-login-wx {
	margin-bottom: 24px;
}

.onekey-login-wx:before {
	display: inline-block;
	content: "";
	width: 21px;
	height: 21px;
	background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 84px;
	background-position: 0 0;
	vertical-align: top;
	margin: 17px 10px 0 0;
}

.onekey-login-phone {
}

.onekey-login-phone:before {
	display: inline-block;
	content: "";
	width: 21px;
	height: 21px;
	background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 84px;
	background-position: -42px 0;
	vertical-align: top;
	margin: 17px 10px 0 0;
}

.onekey-fixed {
	z-index: 100;
	position: fixed;
	top: 0;
	bottom: 0;
	left: 0;
	right: 0;
	background: #fff;
	width: 100%;
	height: 100%;
}

.onekey-fixed.forbid {
	z-index: 100;
	position: fixed;
	top: auto;
	bottom: 68px;
	left: 9%;
	right: 0;
	background: rgba(0, 0, 0, 0.7);
	width: 82%;
	height: 43px;
	border-radius: 25px;
	color: #ffffff;
}
.onekey-login-title.forbid {
	text-align: center;
	padding-bottom: 3px;
	font-size: 14px;
	font-weight: normal;
	line-height: 30px;
	color: white;
}
</style><style>#login_mask {
  background: #000;
  opacity: 0.5;
  filter: alpha(opacity=50);
  position: fixed;
  /*fixed好像在哪个IE上有BUG，先用用*/
  left: 0;
  top: 0;
  z-index: 999;
  height: 100%;
}

#login_iframe_container {
  position: fixed;
  width: 550px;
  height: 360px;
  z-index: 1020;
  background-color: #ffffff;
}

@media screen and (max-width: 828px) {
  #login_iframe_container {
    top: 50% !important;
    left: 50% !important;
    transform: translate(-50%, -50%);
  }
}

#login_iframe_container.new-login {
  width: 550px;
  height: 360px;
  background-image: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/background_2a4a8a6.png);
}

#login_iframe_container.new-login.no-bg {
  background: #fff;
}

#login_iframe_container.new-login .login-title {
  width: 100%;
  height: 42px;
  line-height: 42px;
  text-align: center;
  font-size: 30px;
  letter-spacing: 0.19px;
  color: #ffffff;
  margin-top: 62px;
}
#login_iframe_container.new-login .forbid-title {
  width: 100%;
  height: 42px;
  line-height: 42px;
  text-align: center;
  font-size: 24px;
  letter-spacing: 0.19px;
  color: #333333;
  margin-top: 150px;
}

#login_iframe_container.new-login.no-bg .login-title {
  color: #333333;
}

#login_iframe_container.new-login .login-subtitle {
  width: 100%;
  height: 18px;
  line-height: 18px;
  font-size: 13px;
  letter-spacing: 0.08px;
  color: #ffffff;
  text-align: center;
  margin-top: 9px;
  margin-bottom: 43px;
}

#login_iframe_container.new-login.no-bg .login-subtitle {
  color: #999999;
}

#login_iframe_container.new-login .login-subtitle::before {
  content: '';
  display: inline-block;
  width: 10px;
  height: 1px;
  background-color: #ffffff;
  position: relative;
  top: -4px;
  left: -5px;
}

#login_iframe_container.new-login .login-subtitle::after {
  content: '';
  display: inline-block;
  width: 10px;
  height: 1px;
  background-color: #ffffff;
  position: relative;
  top: -4px;
  left: 5px;
}

#login_iframe_container.new-login.no-bg .login-subtitle::before {
  background-color: #999999;
}

#login_iframe_container.new-login.no-bg .login-subtitle::after {
  background-color: #999999;
}

#login_iframe_container.new-login .close-btn {
  position: absolute;
  top: 20px;
  right: 20px;
  width: 12px;
  height: 12px;
  background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -59px -10px;
  background-size: 81px 91px;
  cursor: pointer;
}

#login_iframe_container.new-login .login-btn {
  width: 220px;
  height: 47px;
  border-radius: 24px;
  border: solid 1px #dddddd;
  background-color: #ffffff;
  margin: 0 auto;
  margin-top: 28px;
  position: relative;
  display: block;
}

#login_iframe_container.new-login .login-btn .login-icon {
  position: absolute;
}

#login_iframe_container.new-login .login-btn .login-text {
  width: 61px;
  height: 47px;
  line-height: 47px;
  vertical-align: middle;
  font-size: 15px;
  letter-spacing: 0.1px;
  color: #666666;
  position: absolute;
  right: 62px;
}

#login_iframe_container.new-login .login-btn.qq-btn .login-icon {
  width: 22px;
  height: 27px;
  top: 10px;
  left: 67px;
  background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -10px -54px;
  background-size: 81px 91px;
}

#login_iframe_container.new-login .login-btn.qq-btn .login-text {
  right: 59px;
}

#login_iframe_container.new-login .login-btn.wechat-btn .login-icon {
  width: 29px;
  height: 24px;
  top: 12px;
  left: 62px;
  background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -10px -10px;
  background-size: 81px 91px;
}</style><style>/* -- container -- */
.rodal,
.rodal-mask {
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 100;
}

.rodal {
    position: fixed;
}

/* -- mask -- */
.rodal-mask {
    position: fixed;
    background: rgba(0, 0, 0, .5);
}

/* -- dialog -- */
.rodal-dialog {
    position: absolute;
    z-index: 101;
    background: #fff;
    border-radius: 3px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, .2);
}

.rodal-center {
    top: 50%;
    transform: translateY(-50%);
    left: 0;
    right: 0;
    margin: 0 auto;
}

.rodal-bottom {
    left: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

.rodal-top {
    left: 0;
    right: 0;
    top: 0;
    margin: auto;
}

.rodal-left {
    top: 0;
    left: 0;
    bottom: 0;
    margin: auto;
}

.rodal-right {
    top: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

/* -- close button -- */
.rodal-close {
    position: absolute;
    cursor: pointer;
    top: 16px;
    right: 16px;
    width: 16px;
    height: 16px;
}

.rodal-close:before,
.rodal-close:after {
    position: absolute;
    content: '';
    height: 2px;
    width: 100%;
    top: 50%;
    left: 0;
    margin-top: -1px;
    background: #999;
    border-radius: 100%;
    -webkit-transition: background .2s;
    transition: background .2s;
}

.rodal-close:before {
    -webkit-transform: rotate(45deg);
    transform: rotate(45deg);
}

.rodal-close:after {
    -webkit-transform: rotate(-45deg);
    transform: rotate(-45deg);
}

.rodal-close:hover:before,
.rodal-close:hover:after {
    background: #333;
}

/* -- fade -- */
/* @-webkit-keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

@keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

.rodal-fade-enter {
    -webkit-animation: rodal-fade-enter both ease-in;
    animation: rodal-fade-enter both ease-in;
} */

@-webkit-keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

@keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

.rodal-fade-leave {
    -webkit-animation: rodal-fade-leave both ease-out;
    animation: rodal-fade-leave both ease-out;
}

/* -- zoom -- */
@-webkit-keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-enter {
    -webkit-animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-leave {
    -webkit-animation: rodal-zoom-leave both;
    animation: rodal-zoom-leave both;
}

/* -- slideDown -- */
@-webkit-keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-enter {
    -webkit-animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-leave {
    -webkit-animation: rodal-slideDown-leave both;
    animation: rodal-slideDown-leave both;
}

/* -- slideLeft -- */
@-webkit-keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-enter {
    -webkit-animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-leave {
    -webkit-animation: rodal-slideLeft-leave both;
    animation: rodal-slideLeft-leave both;
}

/* -- slideRight -- */
@-webkit-keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-enter {
    -webkit-animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-leave {
    -webkit-animation: rodal-slideRight-leave both;
    animation: rodal-slideRight-leave both;
}

/* -- slideUp -- */
@-webkit-keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-enter {
    -webkit-animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
    animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
}

@-webkit-keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-leave {
    -webkit-animation: rodal-slideUp-leave both;
    animation: rodal-slideUp-leave both;
}

/* -- flip -- */
@-webkit-keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

@keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

.rodal-flip-enter {
    -webkit-animation: rodal-flip-enter both ease-in;
    animation: rodal-flip-enter both ease-in;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

@-webkit-keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

@keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

.rodal-flip-leave {
    -webkit-animation: rodal-flip-leave both;
    animation: rodal-flip-leave both;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

/* -- rotate -- */
@-webkit-keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-enter {
    -webkit-animation: rodal-rotate-enter both;
    animation: rodal-rotate-enter both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

@-webkit-keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-leave {
    -webkit-animation: rodal-rotate-leave both;
    animation: rodal-rotate-leave both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

/* -- door -- */
@-webkit-keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

@keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

.rodal-door-enter {
    -webkit-animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

@keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

.rodal-door-leave {
    -webkit-animation: rodal-door-leave both;
    animation: rodal-door-leave both;
}</style><style>/* -- container -- */
.rodal,
.rodal-mask {
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 100;
}

.rodal {
    position: fixed;
}

/* -- mask -- */
.rodal-mask {
    position: fixed;
    background: rgba(0, 0, 0, .5);
}

/* -- dialog -- */
.rodal-dialog {
    position: absolute;
    z-index: 101;
    background: #fff;
    border-radius: 3px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, .2);
}

.rodal-center {
    top: 50%;
    transform: translateY(-50%);
    left: 0;
    right: 0;
    margin: 0 auto;
}

.rodal-bottom {
    left: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

.rodal-top {
    left: 0;
    right: 0;
    top: 0;
    margin: auto;
}

.rodal-left {
    top: 0;
    left: 0;
    bottom: 0;
    margin: auto;
}

.rodal-right {
    top: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

/* -- close button -- */
.rodal-close {
    position: absolute;
    cursor: pointer;
    top: 16px;
    right: 16px;
    width: 16px;
    height: 16px;
}

.rodal-close:before,
.rodal-close:after {
    position: absolute;
    content: '';
    height: 2px;
    width: 100%;
    top: 50%;
    left: 0;
    margin-top: -1px;
    background: #999;
    border-radius: 100%;
    -webkit-transition: background .2s;
    transition: background .2s;
}

.rodal-close:before {
    -webkit-transform: rotate(45deg);
    transform: rotate(45deg);
}

.rodal-close:after {
    -webkit-transform: rotate(-45deg);
    transform: rotate(-45deg);
}

.rodal-close:hover:before,
.rodal-close:hover:after {
    background: #333;
}

/* -- fade -- */
/* @-webkit-keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

@keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

.rodal-fade-enter {
    -webkit-animation: rodal-fade-enter both ease-in;
    animation: rodal-fade-enter both ease-in;
} */

@-webkit-keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

@keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

.rodal-fade-leave {
    -webkit-animation: rodal-fade-leave both ease-out;
    animation: rodal-fade-leave both ease-out;
}

/* -- zoom -- */
@-webkit-keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-enter {
    -webkit-animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-leave {
    -webkit-animation: rodal-zoom-leave both;
    animation: rodal-zoom-leave both;
}

/* -- slideDown -- */
@-webkit-keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-enter {
    -webkit-animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-leave {
    -webkit-animation: rodal-slideDown-leave both;
    animation: rodal-slideDown-leave both;
}

/* -- slideLeft -- */
@-webkit-keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-enter {
    -webkit-animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-leave {
    -webkit-animation: rodal-slideLeft-leave both;
    animation: rodal-slideLeft-leave both;
}

/* -- slideRight -- */
@-webkit-keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-enter {
    -webkit-animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-leave {
    -webkit-animation: rodal-slideRight-leave both;
    animation: rodal-slideRight-leave both;
}

/* -- slideUp -- */
@-webkit-keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-enter {
    -webkit-animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
    animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
}

@-webkit-keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-leave {
    -webkit-animation: rodal-slideUp-leave both;
    animation: rodal-slideUp-leave both;
}

/* -- flip -- */
@-webkit-keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

@keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

.rodal-flip-enter {
    -webkit-animation: rodal-flip-enter both ease-in;
    animation: rodal-flip-enter both ease-in;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

@-webkit-keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

@keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

.rodal-flip-leave {
    -webkit-animation: rodal-flip-leave both;
    animation: rodal-flip-leave both;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

/* -- rotate -- */
@-webkit-keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-enter {
    -webkit-animation: rodal-rotate-enter both;
    animation: rodal-rotate-enter both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

@-webkit-keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-leave {
    -webkit-animation: rodal-rotate-leave both;
    animation: rodal-rotate-leave both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

/* -- door -- */
@-webkit-keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

@keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

.rodal-door-enter {
    -webkit-animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

@keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

.rodal-door-leave {
    -webkit-animation: rodal-door-leave both;
    animation: rodal-door-leave both;
}</style>
<!--百度统计-->
<script>
   var _hmt = _hmt || [];
   (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?3c7614be3026469d5a60f41ab30b5082";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
      })();
</script>
</head>
<body class=""><!-- BEGIN WAYBACK TOOLBAR INSERT -->
<style type="text/css">
body {
  margin-top:0 !important;
  padding-top:0 !important;
  /*min-width:800px !important;*/
}
</style>
<script>__wm.rw(0);</script>
<div id="wm-ipp-base" lang="en" style="display: block; direction: ltr;">
</div><div id="wm-ipp-print">The Wayback Machine - https://web.archive.org/web/20221028211746/https://baike.sogou.com/kexue/d19606990082810886.htm</div>
<script type="text/javascript">//<![CDATA[
__wm.bt(675,27,25,2,"web","https://baike.sogou.com/kexue/d19606990082810886.htm","20221028211746",1996,"/_static/",["/_static/css/banner-styles.css?v=S1zqJCYt","/_static/css/iconochive.css?v=qtvMKcIJ"], false);
  __wm.rw(1);
//]]></script>
<!-- END WAYBACK TOOLBAR INSERT --><script>window._gtag=window._gtag||{};window._gtag.shouldGrayed = false;if ('46f6229d2a45455c9cd0e409ea16eb7a') window._gtag.traceId = '46f6229d2a45455c9cd0e409ea16eb7a';if ({"illegality":true}) window.userInfo = {"illegality":true};</script><div class="topnavbox"><ul class="topnav"><li><a href="https://web.archive.org/web/20221028211746/https://www.sogou.com/web?query=">网页</a></li><li><a href="https://web.archive.org/web/20221028211746/https://weixin.sogou.com/weixin?p=75351201">微信</a></li><li><a href="https://web.archive.org/web/20221028211746/https://zhihu.sogou.com/zhihu?p=75351218">知乎</a></li><li><a href="https://web.archive.org/web/20221028211746/https://pic.sogou.com/pics?query=">图片</a></li><li><a href="https://web.archive.org/web/20221028211746/https://v.sogou.com/v?query=">视频</a></li><li><a href="https://web.archive.org/web/20221028211746/https://mingyi.sogou.com/">医疗</a></li><li class="cur"><strong>科学</strong></li><li><a href="https://web.archive.org/web/20221028211746/https://hanyu.sogou.com/">汉语</a></li><li><a href="https://web.archive.org/web/20221028211746/https://wenwen.sogou.com/">问问</a></li><li><a href="https://web.archive.org/web/20221028211746/https://www.sogou.com/docs/more.htm">更多<span class="topraquo">»</span></a></li></ul></div><div id="header"><div class="header-wrap"><a class="header-logo" href="https://web.archive.org/web/20221028211746/https://baike.sogou.com/kexue"></a><div class="header-search"><div class="querybox" id="suggBox"><form><input id="searchInput" class="query" type="text" placeholder="搜科学领域专业百科词条" name="query" autocomplete="off" value=""><a href="javascript:;" class="query-search"></a></form></div></div><div class="header-rgt"><span class="btn-header-rgt btn-edit" id="editLemma">创建</span><div class="header-user no-login"></div></div></div></div><div class="fixed-placeholder" style="visibility:none"></div><div id="container" class=""><div class="content lemma-level1"><div class="detail-title" id="abstract-title"><h1>BERT</h1><a href="https://web.archive.org/web/20221028211746/https://baike.sogou.com/kexue/d19606990082810886.htm#!" class="detail-edit">编辑</a></div><div class="section_content" data-id="51937283271608329"><div class="text_img ed_imgfloat_right"><a class="ed_image_link" data-src="https://img03.sogoucdn.com/app/a/200698/484_377_20200115150052-431973862.png" data-bigsrc="https://img03.sogoucdn.com/app/a/200698/484_377_20200115150052-431973862.png?width=231&amp;height=174&amp;titlename=%E6%A8%A1%E5%9E%8B%E5%9B%BE&amp;w=484&amp;h=377" title="点击查看大图" data-w="484" data-h="377" style="background-image:url(https://web.archive.org/web/20221028211746im_/https://img03.sogoucdn.com/app/a/200698/484_377_20200115150052-431973862.png)" href="https://web.archive.org/web/20221028211746/https://baike.sogou.com/kexue/d19606990082810886.htm#!"></a><div class="text_img_title" style="text-align: center;">模型图</div></div><div><p>BERT是一种<strong>语言表示模型</strong>，BERT代表来自Transformer的双向编码器表示（Bidirectional Encoder Representations from Transformers）。BERT旨在通过联合调节所有层中的左右上下文来预训练深度双向表示。因此，只需要一个额外的输出层，就可以对预训练的BERT表示进行微调，从而为广泛的任务（比如回答问题和语言推断任务）创建最先进的模型，而无需对特定于任务进行大量模型结构的修改。</p>
<p>BERT的概念很简单，但实验效果很强大，截至2018年10月刷新了 11 个NLP任务的当前最优结果，是NLP领域一个突破性进展。<sup><a href="https://web.archive.org/web/20221028211746/https://baike.sogou.com/kexue/d19606990082810886.htm#quote_1" class="kx_ref">[1]</a></sup></p></div></div><div id="catalog"><h2 class="title2">目录<a href="javascript:" class="detail-edit">编辑</a></h2><div class="catalog_wrap" style=""><ul class="catalog_list col2"><li><span class="order">1</span><a href="javascript:" data-level="1" data-id="19615551580021520">背景</a></li><li><span class="order">2</span><a href="javascript:" data-level="1" data-id="19615551580021521">模型</a></li><li class="secondary_catalog"><span>2.1 </span><a href="javascript:" data-id="19615551580021521">输入表示</a></li><li class="secondary_catalog"><span>2.2 </span><a href="javascript:" data-id="19615551580021521">任务一：遮蔽语言模型</a></li><li class="secondary_catalog"><span>2.3 </span><a href="javascript:" data-id="19615551580021521">任务二：下一句预测</a></li><li class="secondary_catalog"><span>2.4 </span><a href="javascript:" data-id="19615551580021521">预训练过程</a></li></ul><ul class="catalog_list col2"><li class="secondary_catalog"><span>2.5 </span><a href="javascript:" data-id="19615551580021521">微调过程</a></li><li><span class="order">3</span><a href="javascript:" data-level="1" data-id="19615551580021522">关键创新</a></li><li><span class="order">4</span><a href="javascript:" data-level="1" data-id="19615551596798731">实验结果</a></li><li><span class="order">5</span><a href="javascript:" data-level="1" data-id="19615551596798733">变体</a></li><li><span class="order">6</span><a href="javascript:" data-level="1" data-id="references">参考文献</a></li></ul></div></div><div id="paragraphs"><div><div id="par_19615551580021520"><h2 class="title">1 背景<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>语言模型预训练可以显著提高许多自然语言处理任务的效果，现有的两种方法可以将预训练好的语言模型表示应用到下游任务中：<strong>基于特征的方法</strong>和<strong>微调方法</strong>。基于特征的方法比如 ELMo，提取某层或多层特征用于下游任务。而微调方法，如生成预训练 Transformer (OpenAI GPT) 模型，然后引入最小的特定于任务的参数，并通过简单地微调预训练模型的参数对下游任务进行训练。微调方法过去主要的局限性是标准语言模型是单向的，这就限制了可以在预训练期间可以使用的模型结构的选择。通过提出 BERT 改进了基于微调的方法：来自Transformer 的双向编码器表示。BERT、OpenAI GPT 和 ELMo 之间的比较如图所示。</p><p></p><div class="text_img ed_imgfloat_embedmiddle">
            <a class="ed_image_link" data-src="https://img01.sogoucdn.com/app/a/200698/1896_482_20200114144106-1774680094.png" data-bigsrc="https://img01.sogoucdn.com/app/a/200698/1896_482_20200114144106-1774680094.png" title="点击查看大图" href="javascript:">
                <img class="ed_imgfloat_embedmiddle currentPic" width="480" titlename="BERT、GPT、ELMo模型" bigsrc="https://img01.sogoucdn.com/app/a/200698/1896_482_20200114144106-1774680094.png" data-align="embedmiddle" data-src="https://img01.sogoucdn.com/app/a/200698/1896_482_20200114144106-1774680094.png" src="./d19606990082810886.BERT - 搜狗科学百科_files/1896_482_20200114144106-1774680094.png" data-observer="true">
            </a>
            <div class="text_img_title" style="text-align: center;">BERT、GPT、ELMo模型</div>
        </div><p></p></div></div><div id="par_19615551580021521"><h2 class="title">2 模型<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>BERT 的模型结构是一个<strong>多层双向 Transformer 编码器。</strong></p><p></p><h3>2.1 <span>输入表示</span></h3><p></p><p>输入表示能够在一个标记序列中清楚地表示单个文本句子或一对文本句子，输入嵌入是标记嵌入（词嵌入）、</p><div class="text_img ed_imgfloat_right">
            <a class="ed_image_link lazyLoad" data-src="https://img03.sogoucdn.com/app/a/200698/2446_806_20200114144701-1108847430.jpg" data-bigsrc="https://img03.sogoucdn.com/app/a/200698/2446_806_20200114144701-1108847430.jpg" title="点击查看大图" href="javascript:" data-observer="true"></a>
            <div class="text_img_title" style="text-align: center;">输入表示</div>   
        </div>句子嵌入和位置嵌入的总和。<p></p><p>
 </p><ol>
  <li><p>Token embeddings：词嵌入/词向量。</p></li>
  <li><p>Segment embeddings：句子嵌入，用来标记不同的两个句子。</p></li>
  <li><p>Positional embeddings：位置嵌入，将 positional embedding 添加到每个 token 中，以标示其在句子中的位置，通过模型学习出来。</p></li>
 </ol><p></p><p></p><h3>2.2 <span>任务一：遮蔽语言模型</span></h3><p></p><p>为了训练深度双向表示，采用了一种简单的方法，即随机遮蔽一定比例的输入标记，然后仅预测那些被遮蔽的标记。我们将这个过程称为“<strong>遮蔽语言模型</strong>”（MLM）。在每个序列中随机遮蔽 <strong>15% </strong>的标记。</p><p>数据生成不会总是用 [MASK] 替换被选择的单词，而是执行以下操作:</p><p>
 </p><ul>
  <li><p>80% 的情况下：用 [MASK] 替换被选择的单词，例如，my dog is hairy → my dog is [MASK]</p></li>
  <li><p>10% 的情况下：用一个随机单词替换被选择的单词，例如，my dog is hairy → my dog is apple</p></li>
  <li><p>10% 的情况下：保持被选择的单词不变，例如，my dog is hairy → my dog is hairy。这样做的目的是使表示偏向于实际观察到的词。</p></li>
 </ul><p></p><p>Transformer 编码器不知道它将被要求预测哪些单词，或者哪些单词已经被随机单词替换，因此它被迫保持每个输入标记的分布的上下文表示。另外，因为随机替换只发生在 1.5% 的标记（即，15% 的10%）这似乎不会损害模型的语言理解能力。</p><p></p><h3>2.3 <span>任务二：下一句预测</span></h3><p></p><p>许多重要的下游任务，如问题回答（QA）和自然语言推理（NLI），都是建立在理解两个文本句子之间的关系的基础上的，而这并不是语言建模直接捕捉到的。为了训练一个理解句子关系的模型，预训练了一个下一句预测的二元分类任务，这个任务可以从任何单语语料库中简单地归纳出来，预测输入BERT的两端文本是否为连续的文本。具体来说，在为每个训练前的例子选择句子 A 和 B 时，50% 的情况下 B 是真的在 A 后面的下一个句子，50% 的情况下是来自语料库的随机句子。</p><p></p><h3>2.4 <span>预训练过程</span></h3><p></p><p>预训练过程大体上遵循语言模型预训练过程。</p><p></p><h3>2.5 <span>微调过程</span></h3><p></p><p>对于序列级别的分类任务，BERT 微调非常简单。为了获得输入序列的固定维度的表示，我们取特殊标记</p><div class="text_img ed_imgfloat_right">
            <a class="ed_image_link lazyLoad" data-src="https://img01.sogoucdn.com/app/a/200698/720_722_20200114145551-886111459.jpg" data-bigsrc="https://img01.sogoucdn.com/app/a/200698/720_722_20200114145551-886111459.jpg" title="点击查看大图" href="javascript:" data-observer="true"></a>
            <div class="text_img_title" style="text-align: center;">微调</div>   
        </div>（[CLS]）构造相关的嵌入对应的最终的隐藏状态(即，为 Transformer 的输出)的池化后输出。对于区间级和标记级预测任务，必须以特定于任务的方式稍微修改上述过程。对于微调，除了批量大小、学习率和训练次数外，大多数模型超参数与预训练期间相同。BERT将传统大量在下游具体NLP任务中做的操作转移到预训练词向量中，在获得使用BERT词向量后，最终只需在词向量上加简单的MLP或线性分类器即可，论文中所给的几类任务：句子关系判断、分类任务、序列标注、命名体识别如图所示。<p></p></div></div><div id="par_19615551580021522"><h2 class="title">3 关键创新<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>
 </p><ul>
  <li><p>引入了Masked LM，使用双向LM做模型预训练，能够获取上下文相关的双向特征表示，是BERT的最大亮点。<sup><a href="https://web.archive.org/web/20221028211746/https://baike.sogou.com/kexue/d19606990082810886.htm#quote_2" class="kx_ref">[2]</a></sup></p></li>
  <li><p>为下游任务引入了很通用的求解框架，不再为任务做模型定制。</p></li>
  <li><p>微调成本低。</p></li>
 </ul><p></p></div></div><div id="par_19615551596798731"><h2 class="title">4 实验结果<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>BERT是截至2018年10月的最新state of the art模型，当时刷新的11项自然语言处理任务的最新记录包括：将GLUE基准推至80.4％（绝对改进7.6％），MultiNLI准确度达到86.7% （绝对改进率5.6％），将SQuAD v1.1问答测试F1得分纪录刷新为93.2分（绝对提升1.5分），超过人类表现2.0分</p></div></div><div id="par_19615551596798733"><h2 class="title">5 变体<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>bert自从横空出世以来，引起广泛关注，相关研究及bert变体/扩展喷涌而出，如蒸馏的DistilBERT、改进MASK的SpanBERT、精细调参的RoBERTa、改进生成任务方向的MASS、UNILM、以及引入知识的ERNIE、引入多任务的ERNIE2.0等。</p><div class="text_img ed_imgfloat_embedmiddle">
            <a class="ed_image_link" data-src="https://img01.sogoucdn.com/app/a/200698/2574_1342_20200114153110-1045256271.jpg" data-bigsrc="https://img01.sogoucdn.com/app/a/200698/2574_1342_20200114153110-1045256271.jpg" title="点击查看大图" href="javascript:">
                <img class="ed_imgfloat_embedmiddle" width="480" titlename="BERT变体" bigsrc="https://img01.sogoucdn.com/app/a/200698/2574_1342_20200114153110-1045256271.jpg" data-align="embedmiddle" data-src="https://img01.sogoucdn.com/app/a/200698/2574_1342_20200114153110-1045256271.jpg" src="./d19606990082810886.BERT - 搜狗科学百科_files/2574_1342_20200114153110-1045256271.jpg" data-observer="true">
            </a>
            <div class="text_img_title" style="text-align: center;">BERT变体</div>
        </div><p></p></div></div></div></div><div id="references"><h2 class="title" id="par_references">参考文献</h2><ul class="references"><li id="quote_1"><span class="references-num">[1]</span><p><a class="ref-back-btn">^</a><span data-url="https://web.archive.org/web/20221028211746/https://arxiv.org/abs/1810.04805" class="blue">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.</span>arXiv.</p></li><li id="quote_2"><span class="references-num">[2]</span><p><a class="ref-back-btn">^</a><span data-url="https://web.archive.org/web/20221028211746/https://zhuanlan.zhihu.com/p/76912493" class="blue">nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet).</span>知乎.</p></li></ul></div><div class="read-num">阅读 <!-- -->7215</div></div><div class="right-side" id="rightSide"><div class="side" id="lemma-side"><div class="side-title">版本记录</div><ul class="side-lst"><li><p class="side-lst-txt">暂无</p></li></ul><div class="user-card userCard"></div></div><div class="side"><div class="side-event"></div></div></div></div><div class="footer-box"><div id="footer"><div class="footer-logo-wrap"><div class="footer-logo"></div><div class="footer-logo-text">知识·传播·科普</div></div><div class="footer-info">本网站内容采用<a target="_blank" href="https://web.archive.org/web/20221028211746/https://creativecommons.org/licenses/by-sa/3.0/deed.zh?tdsourcetag=s_pctim_aiomsg">CC-BY-SA 3.0</a>授权</div><div class="footer-btn-wrap"><a target="_blank" href="https://web.archive.org/web/20221028211746/https://baike.sogou.com/help/#user_protocol">用户协议</a><a target="_blank" href="https://web.archive.org/web/20221028211746/http://www.sogou.com/docs/terms.htm?v=1">免责声明</a><a target="_blank" href="https://web.archive.org/web/20221028211746/http://corp.sogou.com/private.html">隐私政策</a><a target="_blank" href="https://web.archive.org/web/20221028211746/https://baike.sogou.com/kexue/intro.htm">关于我们</a></div></div></div><script>window.lemmaInfo ={"lemmaId":"19606990082810886","versionId":"57979705743177991","title":"BERT","subtitle":"","abstracts":{"paragraphId":"51937283271608329","title":"摘要","versionId":"54232348086034689","lemmaId":19606990082810890,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82859321,"name":"━╋独特","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599620998,"comment":null,"dependVersionId":0,"contentType":2,"content":"<p>BERT是一种<strong>语言表示模型</strong>，BERT代表来自Transformer的双向编码器表示（Bidirectional Encoder Representations from Transformers）。BERT旨在通过联合调节所有层中的左右上下文来预训练深度双向表示。因此，只需要一个额外的输出层，就可以对预训练的BERT表示进行微调，从而为广泛的任务（比如回答问题和语言推断任务）创建最先进的模型，而无需对特定于任务进行大量模型结构的修改。</p>\n<p>BERT的概念很简单，但实验效果很强大，截至2018年10月刷新了 11 个NLP任务的当前最优结果，是NLP领域一个突破性进展。<sup><a href=\"#quote_1\" class=\"kx_ref\">[1]</a></sup></p>","pics":[{"originalUrl":"https://web.archive.org/web/20221028211746/https://img03.sogoucdn.com/app/a/200698/484_377_20200115150052-431973862.png?width=231&height=174&titlename=%E6%A8%A1%E5%9E%8B%E5%9B%BE&w=484&h=377","url":"https://web.archive.org/web/20221028211746/https://img03.sogoucdn.com/app/a/200698/484_377_20200115150052-431973862.png","rw":484,"rh":377,"title":"模型图","alt":null,"width":231,"height":174}],"card":null,"references":[],"versionCount":0},"card":{"paragraphId":"19615551580021519","title":"基本信息","versionId":"51937283254831106","lemmaId":19606990082810890,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":17649216,"name":"LLLuna","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1598253033,"comment":null,"dependVersionId":0,"contentType":3,"content":"","pics":null,"card":{"cardItems":[{"key":"英文名","value":"BERT"},{"key":"英文全称","value":"Bidirectional Encoder Representations from Transformers"},{"key":"作者","value":"谷歌"},{"key":"类别","value":"预训练语言模型"},{"key":"提出时间","value":"2018年10月"}]},"references":[],"versionCount":0},"categories":[{"id":332,"name":"自然语言处理","parents":[[{"id":1,"name":"计算机","parents":null},{"id":330,"name":"人工智能","parents":null}]]}],"creator":{"uid":17649216,"name":"LLLuna","pic":"https://web.archive.org/web/20221028211746/https://cache.soso.com/qlogo/g?b=oidb&k=exJTySKVSs9q1YM4qvY6Eg&s=100&t=1568177201","introduction":null,"educations":[{"schoolName":"四川大学","major":"","degree":null,"universityId":90,"universityLogo":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200943/41b2078c-6012-11e9-8112-fc4dd4f70029","majorLevel1":"理学","majorLevel2":"计算机科学与技术","majorLevel3":"计算机科学与技术","majorLevel1Id":1,"majorLevel2Id":316,"majorLevel3Id":319,"state":null,"lab":null,"researchField":null}],"jobs":null,"works":null,"educationBrief":"四川大学","jobBrief":"","role":0,"roleName":null,"title":"四川大学 · 计算机科学与技术","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":78,"partnerIdCreateTime":1594286510,"partnerIdPoped":true},"createTime":1578982676,"editor":{"uid":17649216,"name":"LLLuna","pic":"https://web.archive.org/web/20221028211746/https://cache.soso.com/qlogo/g?b=oidb&k=exJTySKVSs9q1YM4qvY6Eg&s=100&t=1568177201","introduction":null,"educations":[{"schoolName":"四川大学","major":"","degree":null,"universityId":90,"universityLogo":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200943/41b2078c-6012-11e9-8112-fc4dd4f70029","majorLevel1":"理学","majorLevel2":"计算机科学与技术","majorLevel3":"计算机科学与技术","majorLevel1Id":1,"majorLevel2Id":316,"majorLevel3Id":319,"state":null,"lab":null,"researchField":null}],"jobs":null,"works":null,"educationBrief":"四川大学","jobBrief":"","role":0,"roleName":null,"title":"四川大学 · 计算机科学与技术","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":78,"partnerIdCreateTime":1594286510,"partnerIdPoped":true},"editTime":1598253033,"state":1,"versionCount":6,"upNum":19,"downNum":0,"pics":[{"originalUrl":"https://web.archive.org/web/20221028211746/https://img03.sogoucdn.com/app/a/200698/484_377_20200115150052-431973862.png?width=231&height=174&titlename=%E6%A8%A1%E5%9E%8B%E5%9B%BE&w=484&h=377","url":"https://web.archive.org/web/20221028211746/https://img03.sogoucdn.com/app/a/200698/484_377_20200115150052-431973862.png","rw":484,"rh":377,"title":"模型图","alt":null,"width":231,"height":174},{"originalUrl":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/1896_482_20200114144106-1774680094.png?w=1896&h=482&titlename=BERT%E3%80%81GPT%E3%80%81ELMo%E6%A8%A1%E5%9E%8B","url":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/1896_482_20200114144106-1774680094.png","rw":1896,"rh":482,"title":"BERT、GPT、ELMo模型","alt":null,"width":0,"height":0},{"originalUrl":"https://web.archive.org/web/20221028211746/https://img03.sogoucdn.com/app/a/200698/2446_806_20200114144701-1108847430.jpg?w=2446&h=806&titlename=%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA","url":"https://web.archive.org/web/20221028211746/https://img03.sogoucdn.com/app/a/200698/2446_806_20200114144701-1108847430.jpg","rw":2446,"rh":806,"title":"输入表示","alt":null,"width":0,"height":0},{"originalUrl":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/720_722_20200114145551-886111459.jpg?w=720&h=722&titlename=%E5%BE%AE%E8%B0%83","url":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/720_722_20200114145551-886111459.jpg","rw":720,"rh":722,"title":"微调","alt":null,"width":0,"height":0},{"originalUrl":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/2574_1342_20200114153110-1045256271.jpg?w=2574&h=1342&titlename=BERT%E5%8F%98%E4%BD%93","url":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/2574_1342_20200114153110-1045256271.jpg","rw":2574,"rh":1342,"title":"BERT变体","alt":null,"width":0,"height":0}],"catalogs":[{"level":1,"title":"背景","paragraphId":"19615551580021520","subCatalogs":null},{"level":1,"title":"模型","paragraphId":"19615551580021521","subCatalogs":[{"level":2,"title":"输入表示","paragraphId":"19615551580021521","subCatalogs":null},{"level":2,"title":"任务一：遮蔽语言模型","paragraphId":"19615551580021521","subCatalogs":null},{"level":2,"title":"任务二：下一句预测","paragraphId":"19615551580021521","subCatalogs":null},{"level":2,"title":"预训练过程","paragraphId":"19615551580021521","subCatalogs":null},{"level":2,"title":"微调过程","paragraphId":"19615551580021521","subCatalogs":null}]},{"level":1,"title":"关键创新","paragraphId":"19615551580021522","subCatalogs":null},{"level":1,"title":"实验结果","paragraphId":"19615551596798731","subCatalogs":null},{"level":1,"title":"变体","paragraphId":"19615551596798733","subCatalogs":null},{"level":1,"title":"参考文献","paragraphId":"-1","subCatalogs":null}],"paragraphs":[{"paragraphId":"19615551580021520","title":"背景","versionId":"57979705743177992","lemmaId":19606990082810890,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82815118,"name":"iscience","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1601854597,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>语言模型预训练可以显著提高许多自然语言处理任务的效果，现有的两种方法可以将预训练好的语言模型表示应用到下游任务中：<strong>基于特征的方法</strong>和<strong>微调方法</strong>。基于特征的方法比如 ELMo，提取某层或多层特征用于下游任务。而微调方法，如生成预训练 Transformer (OpenAI GPT) 模型，然后引入最小的特定于任务的参数，并通过简单地微调预训练模型的参数对下游任务进行训练。微调方法过去主要的局限性是标准语言模型是单向的，这就限制了可以在预训练期间可以使用的模型结构的选择。通过提出 BERT 改进了基于微调的方法：来自Transformer 的双向编码器表示。BERT、OpenAI GPT 和 ELMo 之间的比较如图所示。</p><p><img class=\"ed_imgfloat_embedmiddle currentPic\" width=\"480\" titlename=\"BERT、GPT、ELMo模型\" bigsrc=\"https://img01.sogoucdn.com/app/a/200698/1896_482_20200114144106-1774680094.png\" data-align=\"embedmiddle\" data-src=\"https://img01.sogoucdn.com/app/a/200698/1896_482_20200114144106-1774680094.png\"></p>","pics":[{"originalUrl":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/1896_482_20200114144106-1774680094.png?w=1896&h=482&titlename=BERT%E3%80%81GPT%E3%80%81ELMo%E6%A8%A1%E5%9E%8B","url":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/1896_482_20200114144106-1774680094.png","rw":1896,"rh":482,"title":"BERT、GPT、ELMo模型","alt":null,"width":0,"height":0}],"card":null,"references":[],"versionCount":0},{"paragraphId":"19615551580021521","title":"模型","versionId":"57979705743177993","lemmaId":19606990082810890,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82815118,"name":"iscience","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1601854597,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>BERT 的模型结构是一个<strong>多层双向 Transformer 编码器。</strong></p><p></p><h3>输入表示</h3><p></p><p>输入表示能够在一个标记序列中清楚地表示单个文本句子或一对文本句子，输入嵌入是标记嵌入（词嵌入）、<img class=\"kx_img ed_imgfloat_right\" width=\"232\" height=\"174\" titlename=\"输入表示\" bigsrc=\"https://img03.sogoucdn.com/app/a/200698/2446_806_20200114144701-1108847430.jpg\" data-src=\"https://img03.sogoucdn.com/app/a/200698/2446_806_20200114144701-1108847430.jpg\">句子嵌入和位置嵌入的总和。</p><p>\n </p><ol>\n  <li><p>Token embeddings：词嵌入/词向量。</p></li>\n  <li><p>Segment embeddings：句子嵌入，用来标记不同的两个句子。</p></li>\n  <li><p>Positional embeddings：位置嵌入，将 positional embedding 添加到每个 token 中，以标示其在句子中的位置，通过模型学习出来。</p></li>\n </ol><p></p><p></p><h3>任务一：遮蔽语言模型</h3><p></p><p>为了训练深度双向表示，采用了一种简单的方法，即随机遮蔽一定比例的输入标记，然后仅预测那些被遮蔽的标记。我们将这个过程称为“<strong>遮蔽语言模型</strong>”（MLM）。在每个序列中随机遮蔽 <strong>15% </strong>的标记。</p><p>数据生成不会总是用 [MASK] 替换被选择的单词，而是执行以下操作:</p><p>\n </p><ul>\n  <li><p>80% 的情况下：用 [MASK] 替换被选择的单词，例如，my dog is hairy → my dog is [MASK]</p></li>\n  <li><p>10% 的情况下：用一个随机单词替换被选择的单词，例如，my dog is hairy → my dog is apple</p></li>\n  <li><p>10% 的情况下：保持被选择的单词不变，例如，my dog is hairy → my dog is hairy。这样做的目的是使表示偏向于实际观察到的词。</p></li>\n </ul><p></p><p>Transformer 编码器不知道它将被要求预测哪些单词，或者哪些单词已经被随机单词替换，因此它被迫保持每个输入标记的分布的上下文表示。另外，因为随机替换只发生在 1.5% 的标记（即，15% 的10%）这似乎不会损害模型的语言理解能力。</p><p></p><h3>任务二：下一句预测</h3><p></p><p>许多重要的下游任务，如问题回答（QA）和自然语言推理（NLI），都是建立在理解两个文本句子之间的关系的基础上的，而这并不是语言建模直接捕捉到的。为了训练一个理解句子关系的模型，预训练了一个下一句预测的二元分类任务，这个任务可以从任何单语语料库中简单地归纳出来，预测输入BERT的两端文本是否为连续的文本。具体来说，在为每个训练前的例子选择句子 A 和 B 时，50% 的情况下 B 是真的在 A 后面的下一个句子，50% 的情况下是来自语料库的随机句子。</p><p></p><h3>预训练过程</h3><p></p><p>预训练过程大体上遵循语言模型预训练过程。</p><p></p><h3>微调过程</h3><p></p><p>对于序列级别的分类任务，BERT 微调非常简单。为了获得输入序列的固定维度的表示，我们取特殊标记<img class=\"kx_img ed_imgfloat_right\" width=\"232\" height=\"174\" titlename=\"微调\" bigsrc=\"https://img01.sogoucdn.com/app/a/200698/720_722_20200114145551-886111459.jpg\" data-src=\"https://img01.sogoucdn.com/app/a/200698/720_722_20200114145551-886111459.jpg\">（[CLS]）构造相关的嵌入对应的最终的隐藏状态(即，为 Transformer 的输出)的池化后输出。对于区间级和标记级预测任务，必须以特定于任务的方式稍微修改上述过程。对于微调，除了批量大小、学习率和训练次数外，大多数模型超参数与预训练期间相同。BERT将传统大量在下游具体NLP任务中做的操作转移到预训练词向量中，在获得使用BERT词向量后，最终只需在词向量上加简单的MLP或线性分类器即可，论文中所给的几类任务：句子关系判断、分类任务、序列标注、命名体识别如图所示。</p>","pics":[{"originalUrl":"https://web.archive.org/web/20221028211746/https://img03.sogoucdn.com/app/a/200698/2446_806_20200114144701-1108847430.jpg?w=2446&h=806&titlename=%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA","url":"https://web.archive.org/web/20221028211746/https://img03.sogoucdn.com/app/a/200698/2446_806_20200114144701-1108847430.jpg","rw":2446,"rh":806,"title":"输入表示","alt":null,"width":0,"height":0},{"originalUrl":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/720_722_20200114145551-886111459.jpg?w=720&h=722&titlename=%E5%BE%AE%E8%B0%83","url":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/720_722_20200114145551-886111459.jpg","rw":720,"rh":722,"title":"微调","alt":null,"width":0,"height":0}],"card":null,"references":[],"versionCount":0},{"paragraphId":"19615551580021522","title":"关键创新","versionId":"57979705743177994","lemmaId":19606990082810890,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82815118,"name":"iscience","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1601854597,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>\n <ul>\n  <li><p>引入了Masked LM，使用双向LM做模型预训练，能够获取上下文相关的双向特征表示，是BERT的最大亮点。<sup><a href=\"#quote_2\" class=\"kx_ref\">[2]</a></sup></p></li>\n  <li><p>为下游任务引入了很通用的求解框架，不再为任务做模型定制。</p></li>\n  <li><p>微调成本低。</p></li>\n </ul></p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"19615551596798731","title":"实验结果","versionId":"57979705743177995","lemmaId":19606990082810890,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82815118,"name":"iscience","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1601854597,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>BERT是截至2018年10月的最新state of the art模型，当时刷新的11项自然语言处理任务的最新记录包括：将GLUE基准推至80.4％（绝对改进7.6％），MultiNLI准确度达到86.7% （绝对改进率5.6％），将SQuAD v1.1问答测试F1得分纪录刷新为93.2分（绝对提升1.5分），超过人类表现2.0分</p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"19615551596798733","title":"变体","versionId":"57979705743177996","lemmaId":19606990082810890,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82815118,"name":"iscience","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1601854597,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>bert自从横空出世以来，引起广泛关注，相关研究及bert变体/扩展喷涌而出，如蒸馏的DistilBERT、改进MASK的SpanBERT、精细调参的RoBERTa、改进生成任务方向的MASS、UNILM、以及引入知识的ERNIE、引入多任务的ERNIE2.0等。<img class=\"ed_imgfloat_embedmiddle\" width=\"480\" titlename=\"BERT变体\" bigsrc=\"https://img01.sogoucdn.com/app/a/200698/2574_1342_20200114153110-1045256271.jpg\" data-align=\"embedmiddle\" data-src=\"https://img01.sogoucdn.com/app/a/200698/2574_1342_20200114153110-1045256271.jpg\"></p>","pics":[{"originalUrl":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/2574_1342_20200114153110-1045256271.jpg?w=2574&h=1342&titlename=BERT%E5%8F%98%E4%BD%93","url":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200698/2574_1342_20200114153110-1045256271.jpg","rw":2574,"rh":1342,"title":"BERT变体","alt":null,"width":0,"height":0}],"card":null,"references":[],"versionCount":0}],"references":[{"id":1,"type":"web","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","site":"arXiv","url":"https://web.archive.org/web/20221028211746/https://arxiv.org/abs/1810.04805","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":2,"type":"web","title":"nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)","site":"知乎","url":"https://web.archive.org/web/20221028211746/https://zhuanlan.zhihu.com/p/76912493","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false}],"recommendReferences":null,"auditState":2,"lemmaLevel":1,"origin":0,"originEnTitle":null,"originZhTitle":null,"pv":7215,"auditType":1,"synonyms":["Bidirectional Encoder Representation from Transformers"],"showEditTime":"2020.08.24 15:10","auditors":[{"uid":76031308,"name":"唐长成","pic":"https://web.archive.org/web/20221028211746/https://cache.soso.com/qlogo/g?b=oidb&k=hnXiab4vr0areKKo46k4wSQ&s=100&t=1600231953","introduction":null,"educations":[{"schoolName":"清华大学","major":"","degree":"硕士","universityId":1,"universityLogo":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200943/2d91f0cc-6013-11e9-aa85-fc4dd4f70029","majorLevel1":null,"majorLevel2":null,"majorLevel3":null,"majorLevel1Id":0,"majorLevel2Id":0,"majorLevel3Id":0,"state":null,"lab":null,"researchField":null}],"jobs":null,"works":null,"educationBrief":"清华大学","jobBrief":"","role":0,"roleName":null,"title":"清华大学 · 硕士","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":50,"partnerIdCreateTime":1594286504,"partnerIdPoped":true},{"uid":29409926,"name":"杨昱文","pic":"https://web.archive.org/web/20221028211746/https://cache.soso.com/qlogo/g?b=oidb&k=fkn7icMhxeUankib26vklMpA&s=100&t=1555991442","introduction":"","educations":[{"schoolName":"上海交通大学","major":"","degree":"博士","universityId":30,"universityLogo":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200943/b631e6dc-6011-11e9-a530-fc4dd4f70029","majorLevel1":"工学","majorLevel2":"计算机科学与技术","majorLevel3":"人工智能","majorLevel1Id":353,"majorLevel2Id":578,"majorLevel3Id":581,"state":"在读","lab":"","researchField":""}],"jobs":null,"works":null,"educationBrief":"上海交通大学","jobBrief":"","role":0,"roleName":null,"title":"上海交通大学 · 计算机科学与技术博士","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":79,"partnerIdCreateTime":1594286510,"partnerIdPoped":true}],"hasZhishiNav":false,"auditInfos":{"0":[{"versionId":"0","auditTime":1598513098,"auditorId":76031308,"auditUser":{"uid":76031308,"name":"唐长成","pic":"https://web.archive.org/web/20221028211746/https://cache.soso.com/qlogo/g?b=oidb&k=hnXiab4vr0areKKo46k4wSQ&s=100&t=1600231953","introduction":null,"educations":[{"schoolName":"清华大学","major":"","degree":"硕士","universityId":1,"universityLogo":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200943/2d91f0cc-6013-11e9-aa85-fc4dd4f70029","majorLevel1":null,"majorLevel2":null,"majorLevel3":null,"majorLevel1Id":0,"majorLevel2Id":0,"majorLevel3Id":0,"state":null,"lab":null,"researchField":null}],"jobs":null,"works":null,"educationBrief":"清华大学","jobBrief":"","role":0,"roleName":null,"title":"清华大学 · 硕士","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":50,"partnerIdCreateTime":1594286504,"partnerIdPoped":true},"auditReason":null,"auditSuggest":"1. 修改信息有效且准确","auditResult":2,"auditAnnotations":[]},{"versionId":"0","auditTime":1599532054,"auditorId":29409926,"auditUser":{"uid":29409926,"name":"杨昱文","pic":"https://web.archive.org/web/20221028211746/https://cache.soso.com/qlogo/g?b=oidb&k=fkn7icMhxeUankib26vklMpA&s=100&t=1555991442","introduction":"","educations":[{"schoolName":"上海交通大学","major":"","degree":"博士","universityId":30,"universityLogo":"https://web.archive.org/web/20221028211746/https://img01.sogoucdn.com/app/a/200943/b631e6dc-6011-11e9-a530-fc4dd4f70029","majorLevel1":"工学","majorLevel2":"计算机科学与技术","majorLevel3":"人工智能","majorLevel1Id":353,"majorLevel2Id":578,"majorLevel3Id":581,"state":"在读","lab":"","researchField":""}],"jobs":null,"works":null,"educationBrief":"上海交通大学","jobBrief":"","role":0,"roleName":null,"title":"上海交通大学 · 计算机科学与技术博士","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":79,"partnerIdCreateTime":1594286510,"partnerIdPoped":true},"auditReason":null,"auditSuggest":"整体合格的一个简单词条。\n如果能多阐述一些BERT在NLP领域的相关重要影响，就更棒啦","auditResult":2,"auditAnnotations":[]}]},"isHistory":false};</script><script crossorigin="anonymous" src="./d19606990082810886.BERT - 搜狗科学百科_files/aegis.min.js.download"></script><script crossorigin="anonymous" src="./d19606990082810886.BERT - 搜狗科学百科_files/main_2020092401.js.download"></script><script crossorigin="anonymous" src="./d19606990082810886.BERT - 搜狗科学百科_files/react.production.min.js.download"></script><script crossorigin="anonymous" src="./d19606990082810886.BERT - 搜狗科学百科_files/react-dom.production.min.js.download"></script><script crossorigin="anonymous" src="./d19606990082810886.BERT - 搜狗科学百科_files/jquery-1.11.1.min.js.download"></script><script crossorigin="anonymous" src="./d19606990082810886.BERT - 搜狗科学百科_files/main_2022062701.js.download"></script><script crossorigin="anonymous" src="./d19606990082810886.BERT - 搜狗科学百科_files/main_66bbe21.js.download"></script><script crossorigin="anonymous" src="./d19606990082810886.BERT - 搜狗科学百科_files/react.production.min.js.download"></script><script crossorigin="anonymous" src="./d19606990082810886.BERT - 搜狗科学百科_files/react-dom.production.min.js.download"></script><script crossorigin="anonymous" src="./d19606990082810886.BERT - 搜狗科学百科_files/main_edf0f08.js.download"></script>
</body></html>