<!DOCTYPE html>
<!-- saved from url=(0083)https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm -->
<html class="" data-reactroot=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./1066.计算机视觉 - 搜狗科学百科_files/analytics.js.download" type="text/javascript"></script>
<script type="text/javascript">window.addEventListener('DOMContentLoaded',function(){var v=archive_analytics.values;v.service='wb';v.server_name='wwwb-app203.us.archive.org';v.server_ms=425;archive_analytics.send_pageview({});});</script>
<script type="text/javascript" src="./1066.计算机视觉 - 搜狗科学百科_files/bundle-playback.js.download" charset="utf-8"></script>
<script type="text/javascript" src="./1066.计算机视觉 - 搜狗科学百科_files/wombat.js.download" charset="utf-8"></script>
<script type="text/javascript">
  __wm.init("https://web.archive.org/web");
  __wm.wombat("https://baike.sogou.com/kexue/d11066.htm","20221025122739","https://web.archive.org/","web","/_static/",
	      "1666700859");
</script>
<link rel="stylesheet" type="text/css" href="./1066.计算机视觉 - 搜狗科学百科_files/banner-styles.css">
<link rel="stylesheet" type="text/css" href="./1066.计算机视觉 - 搜狗科学百科_files/iconochive.css">
<!-- End Wayback Rewrite JS Include -->
<meta name="save" content="history"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="VWGb6TyYx8"><meta content="计算机视觉 - 搜狗科学百科" name="keywords"><meta content="搜狗科学百科是一部有着平等、协作、分享、自由理念的网络科学全书，为每一个互联网用户创造一个涵盖所有领域知识、服务的中文知识性平台。" name="description"><meta http-equiv="x-dns-prefetch-control" content="on"><meta name="server" baike="235" ip="210" env="online"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://cache.soso.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://hhy.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://pic.baike.soso.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://ugc.qpic.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://xui.ptlogin2.qq.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://q1.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://q2.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://q3.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://q4.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://q.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://img01.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://img03.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025122739/https://img04.sogoucdn.com/"><link rel="Shortcut Icon" href="https://web.archive.org/web/20221025122739im_/https://www.sogou.com/images/logo/new/favicon.ico?v=4"><link rel="Bookmark" href="https://www.sogou.com/images/logo/new/favicon.ico?v=4"><link href="./1066.计算机视觉 - 搜狗科学百科_files/base_b849887.css" rel="stylesheet"><link href="./1066.计算机视觉 - 搜狗科学百科_files/detail_378aed5.css" rel="stylesheet"><link href="./1066.计算机视觉 - 搜狗科学百科_files/inviteAudit_7894507.css" rel="stylesheet"><link rel="stylesheet" href="./1066.计算机视觉 - 搜狗科学百科_files/highlight.min.css"><title>计算机视觉 - 搜狗科学百科</title><style>.onekey-close {
	position: absolute;
	top: 16px;
	right: 16px;
	width: 24px;
	height: 24px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	text-indent: -999em;
	background-size: 84px;
	background-position: -63px 0;
}

.onekey-login {
	position: absolute;
	top: 16.4%;
	left: 0;
	right: 0;
	width: 100%;
}

/* .onekey-login-img {
    width: 75px;
    height: 75px;
    background: url("https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/images/sprite_wap_baike.png") no-repeat;
    background-size: 100px 91px;
    background-position: 0 0;
    background-repeat: no-repeat;
    margin: 0 auto;
} */

.onekey-login-title {
	text-align: center;
	padding-bottom: 3px;
	font-size: 21px;
	font-weight: bold;
	line-height: 30px;
	color: #000;
}

.onekey-login-txt {
	text-align: center;
	font-family: PingFangSC;
	font-size: 14px;
	line-height: 20px;
	color: #8f8f8f;
}

.onekey-login-qq,
.onekey-login-wx,
.onekey-login-phone {
	display: block;
	width: 245px;
	height: 54px;
	border-radius: 45px;
	text-align: center;

	margin: 0 auto;
	font-size: 17px;
	line-height: 24px;
	color: #000;
	/* padding: 16px 77px; */
	border-radius: 12px;
	border: solid 1px #e0e0e0;
}
.onekey-qq-content,
.onekey-vx-content,
.onekey-phone-content {
	display: inline-block;
	margin-top: 16px;
}
.onekey-qq-content {
	padding: 0 5px;
}

.onekey-login-qq {
	margin-top: 48px;
	margin-bottom: 24px;
}

.onekey-login-qq:before {
	display: inline-block;
	content: "";
	width: 20px;
	height: 20px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 80px;
	background-position: -20px 0;
	vertical-align: top;
	margin: 17px 8px 0 0;
}

.onekey-login-wx {
	margin-bottom: 24px;
}

.onekey-login-wx:before {
	display: inline-block;
	content: "";
	width: 21px;
	height: 21px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 84px;
	background-position: 0 0;
	vertical-align: top;
	margin: 17px 10px 0 0;
}

.onekey-login-phone {
}

.onekey-login-phone:before {
	display: inline-block;
	content: "";
	width: 21px;
	height: 21px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 84px;
	background-position: -42px 0;
	vertical-align: top;
	margin: 17px 10px 0 0;
}

.onekey-fixed {
	z-index: 100;
	position: fixed;
	top: 0;
	bottom: 0;
	left: 0;
	right: 0;
	background: #fff;
	width: 100%;
	height: 100%;
}

.onekey-fixed.forbid {
	z-index: 100;
	position: fixed;
	top: auto;
	bottom: 68px;
	left: 9%;
	right: 0;
	background: rgba(0, 0, 0, 0.7);
	width: 82%;
	height: 43px;
	border-radius: 25px;
	color: #ffffff;
}
.onekey-login-title.forbid {
	text-align: center;
	padding-bottom: 3px;
	font-size: 14px;
	font-weight: normal;
	line-height: 30px;
	color: white;
}
</style><style>#login_mask {
  background: #000;
  opacity: 0.5;
  filter: alpha(opacity=50);
  position: fixed;
  /*fixed好像在哪个IE上有BUG，先用用*/
  left: 0;
  top: 0;
  z-index: 999;
  height: 100%;
}

#login_iframe_container {
  position: fixed;
  width: 550px;
  height: 360px;
  z-index: 1020;
  background-color: #ffffff;
}

@media screen and (max-width: 828px) {
  #login_iframe_container {
    top: 50% !important;
    left: 50% !important;
    transform: translate(-50%, -50%);
  }
}

#login_iframe_container.new-login {
  width: 550px;
  height: 360px;
  background-image: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/background_2a4a8a6.png);
}

#login_iframe_container.new-login.no-bg {
  background: #fff;
}

#login_iframe_container.new-login .login-title {
  width: 100%;
  height: 42px;
  line-height: 42px;
  text-align: center;
  font-size: 30px;
  letter-spacing: 0.19px;
  color: #ffffff;
  margin-top: 62px;
}
#login_iframe_container.new-login .forbid-title {
  width: 100%;
  height: 42px;
  line-height: 42px;
  text-align: center;
  font-size: 24px;
  letter-spacing: 0.19px;
  color: #333333;
  margin-top: 150px;
}

#login_iframe_container.new-login.no-bg .login-title {
  color: #333333;
}

#login_iframe_container.new-login .login-subtitle {
  width: 100%;
  height: 18px;
  line-height: 18px;
  font-size: 13px;
  letter-spacing: 0.08px;
  color: #ffffff;
  text-align: center;
  margin-top: 9px;
  margin-bottom: 43px;
}

#login_iframe_container.new-login.no-bg .login-subtitle {
  color: #999999;
}

#login_iframe_container.new-login .login-subtitle::before {
  content: '';
  display: inline-block;
  width: 10px;
  height: 1px;
  background-color: #ffffff;
  position: relative;
  top: -4px;
  left: -5px;
}

#login_iframe_container.new-login .login-subtitle::after {
  content: '';
  display: inline-block;
  width: 10px;
  height: 1px;
  background-color: #ffffff;
  position: relative;
  top: -4px;
  left: 5px;
}

#login_iframe_container.new-login.no-bg .login-subtitle::before {
  background-color: #999999;
}

#login_iframe_container.new-login.no-bg .login-subtitle::after {
  background-color: #999999;
}

#login_iframe_container.new-login .close-btn {
  position: absolute;
  top: 20px;
  right: 20px;
  width: 12px;
  height: 12px;
  background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -59px -10px;
  background-size: 81px 91px;
  cursor: pointer;
}

#login_iframe_container.new-login .login-btn {
  width: 220px;
  height: 47px;
  border-radius: 24px;
  border: solid 1px #dddddd;
  background-color: #ffffff;
  margin: 0 auto;
  margin-top: 28px;
  position: relative;
  display: block;
}

#login_iframe_container.new-login .login-btn .login-icon {
  position: absolute;
}

#login_iframe_container.new-login .login-btn .login-text {
  width: 61px;
  height: 47px;
  line-height: 47px;
  vertical-align: middle;
  font-size: 15px;
  letter-spacing: 0.1px;
  color: #666666;
  position: absolute;
  right: 62px;
}

#login_iframe_container.new-login .login-btn.qq-btn .login-icon {
  width: 22px;
  height: 27px;
  top: 10px;
  left: 67px;
  background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -10px -54px;
  background-size: 81px 91px;
}

#login_iframe_container.new-login .login-btn.qq-btn .login-text {
  right: 59px;
}

#login_iframe_container.new-login .login-btn.wechat-btn .login-icon {
  width: 29px;
  height: 24px;
  top: 12px;
  left: 62px;
  background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -10px -10px;
  background-size: 81px 91px;
}</style><style>/* -- container -- */
.rodal,
.rodal-mask {
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 100;
}

.rodal {
    position: fixed;
}

/* -- mask -- */
.rodal-mask {
    position: fixed;
    background: rgba(0, 0, 0, .5);
}

/* -- dialog -- */
.rodal-dialog {
    position: absolute;
    z-index: 101;
    background: #fff;
    border-radius: 3px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, .2);
}

.rodal-center {
    top: 50%;
    transform: translateY(-50%);
    left: 0;
    right: 0;
    margin: 0 auto;
}

.rodal-bottom {
    left: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

.rodal-top {
    left: 0;
    right: 0;
    top: 0;
    margin: auto;
}

.rodal-left {
    top: 0;
    left: 0;
    bottom: 0;
    margin: auto;
}

.rodal-right {
    top: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

/* -- close button -- */
.rodal-close {
    position: absolute;
    cursor: pointer;
    top: 16px;
    right: 16px;
    width: 16px;
    height: 16px;
}

.rodal-close:before,
.rodal-close:after {
    position: absolute;
    content: '';
    height: 2px;
    width: 100%;
    top: 50%;
    left: 0;
    margin-top: -1px;
    background: #999;
    border-radius: 100%;
    -webkit-transition: background .2s;
    transition: background .2s;
}

.rodal-close:before {
    -webkit-transform: rotate(45deg);
    transform: rotate(45deg);
}

.rodal-close:after {
    -webkit-transform: rotate(-45deg);
    transform: rotate(-45deg);
}

.rodal-close:hover:before,
.rodal-close:hover:after {
    background: #333;
}

/* -- fade -- */
/* @-webkit-keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

@keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

.rodal-fade-enter {
    -webkit-animation: rodal-fade-enter both ease-in;
    animation: rodal-fade-enter both ease-in;
} */

@-webkit-keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

@keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

.rodal-fade-leave {
    -webkit-animation: rodal-fade-leave both ease-out;
    animation: rodal-fade-leave both ease-out;
}

/* -- zoom -- */
@-webkit-keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-enter {
    -webkit-animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-leave {
    -webkit-animation: rodal-zoom-leave both;
    animation: rodal-zoom-leave both;
}

/* -- slideDown -- */
@-webkit-keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-enter {
    -webkit-animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-leave {
    -webkit-animation: rodal-slideDown-leave both;
    animation: rodal-slideDown-leave both;
}

/* -- slideLeft -- */
@-webkit-keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-enter {
    -webkit-animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-leave {
    -webkit-animation: rodal-slideLeft-leave both;
    animation: rodal-slideLeft-leave both;
}

/* -- slideRight -- */
@-webkit-keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-enter {
    -webkit-animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-leave {
    -webkit-animation: rodal-slideRight-leave both;
    animation: rodal-slideRight-leave both;
}

/* -- slideUp -- */
@-webkit-keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-enter {
    -webkit-animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
    animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
}

@-webkit-keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-leave {
    -webkit-animation: rodal-slideUp-leave both;
    animation: rodal-slideUp-leave both;
}

/* -- flip -- */
@-webkit-keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

@keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

.rodal-flip-enter {
    -webkit-animation: rodal-flip-enter both ease-in;
    animation: rodal-flip-enter both ease-in;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

@-webkit-keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

@keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

.rodal-flip-leave {
    -webkit-animation: rodal-flip-leave both;
    animation: rodal-flip-leave both;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

/* -- rotate -- */
@-webkit-keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-enter {
    -webkit-animation: rodal-rotate-enter both;
    animation: rodal-rotate-enter both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

@-webkit-keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-leave {
    -webkit-animation: rodal-rotate-leave both;
    animation: rodal-rotate-leave both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

/* -- door -- */
@-webkit-keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

@keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

.rodal-door-enter {
    -webkit-animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

@keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

.rodal-door-leave {
    -webkit-animation: rodal-door-leave both;
    animation: rodal-door-leave both;
}</style><style>/* -- container -- */
.rodal,
.rodal-mask {
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 100;
}

.rodal {
    position: fixed;
}

/* -- mask -- */
.rodal-mask {
    position: fixed;
    background: rgba(0, 0, 0, .5);
}

/* -- dialog -- */
.rodal-dialog {
    position: absolute;
    z-index: 101;
    background: #fff;
    border-radius: 3px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, .2);
}

.rodal-center {
    top: 50%;
    transform: translateY(-50%);
    left: 0;
    right: 0;
    margin: 0 auto;
}

.rodal-bottom {
    left: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

.rodal-top {
    left: 0;
    right: 0;
    top: 0;
    margin: auto;
}

.rodal-left {
    top: 0;
    left: 0;
    bottom: 0;
    margin: auto;
}

.rodal-right {
    top: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

/* -- close button -- */
.rodal-close {
    position: absolute;
    cursor: pointer;
    top: 16px;
    right: 16px;
    width: 16px;
    height: 16px;
}

.rodal-close:before,
.rodal-close:after {
    position: absolute;
    content: '';
    height: 2px;
    width: 100%;
    top: 50%;
    left: 0;
    margin-top: -1px;
    background: #999;
    border-radius: 100%;
    -webkit-transition: background .2s;
    transition: background .2s;
}

.rodal-close:before {
    -webkit-transform: rotate(45deg);
    transform: rotate(45deg);
}

.rodal-close:after {
    -webkit-transform: rotate(-45deg);
    transform: rotate(-45deg);
}

.rodal-close:hover:before,
.rodal-close:hover:after {
    background: #333;
}

/* -- fade -- */
/* @-webkit-keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

@keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

.rodal-fade-enter {
    -webkit-animation: rodal-fade-enter both ease-in;
    animation: rodal-fade-enter both ease-in;
} */

@-webkit-keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

@keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

.rodal-fade-leave {
    -webkit-animation: rodal-fade-leave both ease-out;
    animation: rodal-fade-leave both ease-out;
}

/* -- zoom -- */
@-webkit-keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-enter {
    -webkit-animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-leave {
    -webkit-animation: rodal-zoom-leave both;
    animation: rodal-zoom-leave both;
}

/* -- slideDown -- */
@-webkit-keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-enter {
    -webkit-animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-leave {
    -webkit-animation: rodal-slideDown-leave both;
    animation: rodal-slideDown-leave both;
}

/* -- slideLeft -- */
@-webkit-keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-enter {
    -webkit-animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-leave {
    -webkit-animation: rodal-slideLeft-leave both;
    animation: rodal-slideLeft-leave both;
}

/* -- slideRight -- */
@-webkit-keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-enter {
    -webkit-animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-leave {
    -webkit-animation: rodal-slideRight-leave both;
    animation: rodal-slideRight-leave both;
}

/* -- slideUp -- */
@-webkit-keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-enter {
    -webkit-animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
    animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
}

@-webkit-keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-leave {
    -webkit-animation: rodal-slideUp-leave both;
    animation: rodal-slideUp-leave both;
}

/* -- flip -- */
@-webkit-keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

@keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

.rodal-flip-enter {
    -webkit-animation: rodal-flip-enter both ease-in;
    animation: rodal-flip-enter both ease-in;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

@-webkit-keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

@keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

.rodal-flip-leave {
    -webkit-animation: rodal-flip-leave both;
    animation: rodal-flip-leave both;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

/* -- rotate -- */
@-webkit-keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-enter {
    -webkit-animation: rodal-rotate-enter both;
    animation: rodal-rotate-enter both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

@-webkit-keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-leave {
    -webkit-animation: rodal-rotate-leave both;
    animation: rodal-rotate-leave both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

/* -- door -- */
@-webkit-keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

@keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

.rodal-door-enter {
    -webkit-animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

@keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

.rodal-door-leave {
    -webkit-animation: rodal-door-leave both;
    animation: rodal-door-leave both;
}</style></head><body class=""><!-- BEGIN WAYBACK TOOLBAR INSERT -->
<style type="text/css">
body {
  margin-top:0 !important;
  padding-top:0 !important;
  /*min-width:800px !important;*/
}
</style>
<script>__wm.rw(0);</script>
<div id="wm-ipp-base" lang="en" style="display: block; direction: ltr;">
</div><div id="wm-ipp-print">The Wayback Machine - https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm</div>
<script type="text/javascript">//<![CDATA[
__wm.bt(675,27,25,2,"web","https://baike.sogou.com/kexue/d11066.htm","20221025122739",1996,"/_static/",["/_static/css/banner-styles.css?v=S1zqJCYt","/_static/css/iconochive.css?v=qtvMKcIJ"], false);
  __wm.rw(1);
//]]></script>
<!-- END WAYBACK TOOLBAR INSERT --><script>window._gtag=window._gtag||{};window._gtag.shouldGrayed = false;if ('9ee5ece40e2d4cf6837bd3a9496ca408') window._gtag.traceId = '9ee5ece40e2d4cf6837bd3a9496ca408';if ({"illegality":true}) window.userInfo = {"illegality":true};</script><div class="topnavbox"><ul class="topnav"><li><a href="https://web.archive.org/web/20221025122739/https://www.sogou.com/web?query=">网页</a></li><li><a href="https://web.archive.org/web/20221025122739/https://weixin.sogou.com/weixin?p=75351201">微信</a></li><li><a href="https://web.archive.org/web/20221025122739/https://zhihu.sogou.com/zhihu?p=75351218">知乎</a></li><li><a href="https://web.archive.org/web/20221025122739/https://pic.sogou.com/pics?query=">图片</a></li><li><a href="https://web.archive.org/web/20221025122739/https://v.sogou.com/v?query=">视频</a></li><li><a href="https://web.archive.org/web/20221025122739/https://mingyi.sogou.com/">医疗</a></li><li class="cur"><strong>科学</strong></li><li><a href="https://web.archive.org/web/20221025122739/https://hanyu.sogou.com/">汉语</a></li><li><a href="https://web.archive.org/web/20221025122739/https://wenwen.sogou.com/">问问</a></li><li><a href="https://web.archive.org/web/20221025122739/https://www.sogou.com/docs/more.htm">更多<span class="topraquo">»</span></a></li></ul></div><div id="header"><div class="header-wrap"><a class="header-logo" href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue"></a><div class="header-search"><div class="querybox" id="suggBox"><form><input id="searchInput" class="query" type="text" placeholder="搜科学领域专业百科词条" name="query" autocomplete="off" value=""><a href="javascript:;" class="query-search"></a></form></div></div><div class="header-rgt"><span class="btn-header-rgt btn-edit" id="editLemma">创建</span><div class="header-user no-login"></div></div></div></div><div class="fixed-placeholder" style="visibility:none"></div><div id="container" class=""><div class="content lemma-level1"><div class="detail-title" id="abstract-title"><h1>计算机视觉</h1><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#!" class="detail-edit">编辑</a></div><div class="section_content" data-id="14996291318710541"><div><p>计算机视觉是一个跨学科的科学领域，研究如何让计算机从数字图像或视频中获得高水平的理解。从工程学的角度来看，它寻求人类视觉系统能够完成的自动化任务。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_1" class="kx_ref">[1]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_2" class="kx_ref">[2]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_3" class="kx_ref">[3]</a></sup> </p>
<p>计算机视觉任务包括获取、处理、分析和理解数字图像的方法，以及从现实世界中提取高维数据以便例如以决策的形式产生数字或符号信息的方法。<sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_4" class="kx_ref">[4]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_5" class="kx_ref">[5]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_6" class="kx_ref">[6]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_7" class="kx_ref">[7]</a></sup></sup> 在这种背景下，理解意味着将视觉图像(视网膜的输入)转化为对世界的描述，这种描述可以与其他思维过程相结合并引发适当的行动。这种图像理解可以被看作是利用借助几何、物理、统计和学习理论构建的模型从图像数据中分离符号信息。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_8" class="kx_ref">[8]</a></sup> </p>
<p>作为一门科学学科，计算机视觉涉及从图像中提取信息的人工系统背后的理论。图像数据可以采取多种形式，例如视频序列、来自多个摄像机的视图或者来自医学扫描仪的多维数据。作为一门技术学科，计算机视觉寻求将其理论和模型应用于计算机视觉系统的构建。 </p>
<p>计算机视觉的子领域包括场景重建、事件检测、视频跟踪、对象识别、3D姿态估计、学习、索引、运动估计和图像恢复。<sup>[6]</sup> </p></div></div><div id="catalog"><h2 class="title2">目录<a href="javascript:" class="detail-edit">编辑</a></h2><div class="catalog_wrap" style=""><ul class="catalog_list col3"><li><span class="order">1</span><a href="javascript:" data-level="1" data-id="14996291318710542">定义</a></li><li><span class="order">2</span><a href="javascript:" data-level="1" data-id="14996291318710543">历史</a></li><li><span class="order">3</span><a href="javascript:" data-level="1" data-id="14996291318710544">相关领域</a></li><li class="secondary_catalog"><span>3.1 </span><a href="javascript:" data-id="14996291318710544">人工智能</a></li><li class="secondary_catalog"><span>3.2 </span><a href="javascript:" data-id="14996291318710544">信息工程学</a></li><li class="secondary_catalog"><span>3.3 </span><a href="javascript:" data-id="14996291318710544">固态物理学</a></li><li class="secondary_catalog"><span>3.4 </span><a href="javascript:" data-id="14996291318710544">神经生物学</a></li><li class="secondary_catalog"><span>3.5 </span><a href="javascript:" data-id="14996291318710544">信号处理</a></li></ul><ul class="catalog_list col3"><li class="secondary_catalog"><span>3.6 </span><a href="javascript:" data-id="14996291318710544">其他领域</a></li><li class="secondary_catalog"><span>3.7 </span><a href="javascript:" data-id="14996291318710544">区别</a></li><li><span class="order">4</span><a href="javascript:" data-level="1" data-id="14996291335487751">应用程序</a></li><li><span class="order">5</span><a href="javascript:" data-level="1" data-id="14996291335487752">典型任务</a></li><li class="secondary_catalog"><span>5.1 </span><a href="javascript:" data-id="14996291335487752">识别</a></li><li class="secondary_catalog"><span>5.2 </span><a href="javascript:" data-id="14996291335487752">动作分析</a></li><li class="secondary_catalog"><span>5.3 </span><a href="javascript:" data-id="14996291335487752">场景重建</a></li><li class="secondary_catalog"><span>5.4 </span><a href="javascript:" data-id="14996291335487752">图像恢复</a></li></ul><ul class="catalog_list col3"><li><span class="order">6</span><a href="javascript:" data-level="1" data-id="14996291335487753">系统方法</a></li><li class="secondary_catalog"><span>6.1 </span><a href="javascript:" data-id="14996291335487753">图像理解系统</a></li><li><span class="order">7</span><a href="javascript:" data-level="1" data-id="14996291335487754">硬件</a></li><li class="secondary_catalog"><span>7.1 </span><a href="javascript:" data-id="14996291335487754">列表</a></li><li><span class="order">8</span><a href="javascript:" data-level="1" data-id="references">参考文献</a></li></ul></div></div><div id="paragraphs"><div><div id="par_14996291318710542"><h2 class="title">1 定义<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>计算机视觉是一个跨学科的领域，研究如何让计算机从数字图像或视频中获得高水平的理解。从工程学的角度来看，它寻求人类视觉系统能够完成的自动化任务。<sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_1" class="kx_ref">[1]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_2" class="kx_ref">[2]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_3" class="kx_ref">[3]</a></sup></sup> “计算机视觉涉及从单个图像或图像序列中自动提取、分析和理解有用信息。它涉及理论和算法基础的发展，以实现自动视觉理解。”<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_9" class="kx_ref">[9]</a></sup>作为一门科学学科，计算机视觉与从图像中提取信息的人工系统背后的理论有关。图像数据可以采取多种形式，例如视频序列、来自多个摄像机的视图或者来自医学扫描仪的多维数据。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_10" class="kx_ref">[10]</a></sup>作为一门技术学科，计算机视觉寻求将其理论和模型应用于计算机视觉系统的构建。 </p></div></div><div id="par_14996291318710543"><h2 class="title">2 历史<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>20世纪60年代末，计算机视觉开始出现在开创人工智能的大学。它旨在模仿人类视觉系统，作为赋予机器人智能行为的垫脚石。<sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_11" class="kx_ref">[11]</a></sup></sup> 1966年，人们认为这可以通过一个夏季项目来实现，方法是将一台照相机连接到计算机上，并让它“描述它所看到的”。<sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_12" class="kx_ref">[12]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_13" class="kx_ref">[13]</a></sup></sup> </p>
<p>将计算机视觉与当时流行的数字图像处理领域区别开来，是希望从图像中提取3D结构，其目标是实现对整个场景的理解。20世纪70年代的研究为当今存在的许多计算机视觉算法奠定了早期基础，包括从图像中提取边缘、标记线条、非多面体和多面体建模、将物体表示为较小结构的互连、光流和运动估计。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_11" class="kx_ref">[11]</a></sup> </p>
<p>接下来的十年见证了基于更严格的数学分析和计算机视觉定量方面的研究。这些包括尺度空间的概念，从阴影、纹理和焦点等各种线索推断形状，以及被称为蛇的轮廓模型。研究人员还意识到，许多这些数学概念可以在与正则化和马尔可夫随机场相同的优化框架内处理。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_14" class="kx_ref">[14]</a></sup>到20世纪90年代，以前的一些研究课题变得比其他的更加活跃。投影三维重建的研究使人们对摄像机标定有了更好的理解。随着摄像机标定优化方法的出现，人们认识到，在摄影测量领域，束平差理论已经探索了许多思路。这导致了用于来自多个图像的场景的稀疏3-D重建的方法。密集立体对应问题和进一步的多视图立体技术取得了进展。同时，利用图形切割的变化来解决图像分割问题。这十年也标志着统计学习技术首次在实践中被用于识别图像中的人脸。20世纪90年代末，随着计算机图形学和计算机视觉领域之间交互的增加，出现了一个重大的变化。这包括基于图像的渲染、图像变形、视图插值、全景图像拼接和早期光场渲染。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_11" class="kx_ref">[11]</a></sup> </p>
<p>最近的工作见证了基于特征的方法的复兴，这些方法与机器学习技术和复杂的优化框架结合使用。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_15" class="kx_ref">[15]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_16" class="kx_ref">[16]</a></sup> </p></div></div><div id="par_14996291318710544"><h2 class="title">3 相关领域<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><h3>3.1 <span>人工智能</span></h3> 
<p>人工智能领域涉及机器人系统在环境中导航的自主规划或审议。需要对这些环境有一个详细的了解来浏览它们。关于环境的信息可以由计算机视觉系统提供，该系统充当视觉传感器并提供关于环境和机器人的高级信息。 </p>
<p>人工智能和计算机视觉共享其他主题，如模式识别和学习技术。因此，计算机视觉有时被视为人工智能领域或一般计算机科学领域的一部分。 </p> 
<h3>3.2 <span>信息工程学</span></h3> 
<p>计算机视觉通常被认为是信息工程的一部分。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_17" class="kx_ref">[17]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_18" class="kx_ref">[18]</a></sup> </p> 
<h3>3.3 <span>固态物理学</span></h3> 
<p>固态物理是另一个与计算机视觉密切相关的领域。大多数计算机视觉系统依靠图像传感器来检测电磁辐射，电磁辐射通常以可见光或红外光的形式存在。传感器是用量子物理设计的。光与表面相互作用的过程用物理学来解释。物理学解释了光学的行为，光学是大多数成像系统的核心部分。复杂的图像传感器甚至需要量子力学来提供对图像形成过程的完整理解。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_11" class="kx_ref">[11]</a></sup>此外，物理中的各种测量问题可以用计算机视觉来解决，例如流体中的运动。 </p> 
<h3>3.4 <span>神经生物学</span></h3> 
<p>第三个发挥重要作用的领域是神经生物学，特别是生物视觉系统的研究。在上个世纪，人们对眼睛、神经元和大脑结构进行了广泛的研究，致力于处理人类和各种动物的视觉刺激。这导致了对“真实”视觉系统如何操作以解决某些视觉相关任务的粗略但复杂的描述。这些结果导致了计算机视觉中的一个子领域，其中人工系统被设计成在不同的复杂程度上模拟生物系统的处理和行为。此外，在计算机视觉中开发的一些基于学习的方法(例如，基于神经网络和深度学习的图像和特征分析和分类) 具有其生物学背景。 </p>
<p>计算机视觉研究的某些方面与生物视觉研究密切相关——事实上，正如人工智能研究的许多方面与人类意识研究以及使用存储的知识来解释、整合和利用视觉信息密切相关一样。生物视觉领域研究和模拟人类和其他动物视觉背后的生理过程。另一方面，计算机视觉研究和描述了人工视觉系统背后的软件和硬件实现的过程。生物学和计算机视觉之间的跨学科交流已被证明在这两个领域都卓有成效。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_19" class="kx_ref">[19]</a></sup> </p> 
<h3>3.5 <span>信号处理</span></h3> 
<p>与计算机视觉相关的另一个领域是信号处理。许多处理单变量信号(通常是时间信号)的方法可以以自然的方式扩展到计算机视觉中处理双变量信号或多变量信号。然而，由于图像的特殊性质，在计算机视觉中开发了许多方法，但在处理单变量信号方面没有对应的方法。结合信号的多维度，将信号处理中的子场定义为计算机视觉的一部分。 </p> 
<h3>3.6 <span>其他领域</span></h3> 
<p>除了上述关于计算机视觉的观点之外，许多相关的研究课题也可以从纯数学的角度来研究。例如，计算机视觉中的许多方法都是基于统计学、最优化或几何学的。最后，该领域的很大一部分致力于计算机视觉的实现方面；如何在软件和硬件的各种组合中实现现有方法，或者如何修改这些方法以获得处理速度而不损失太多性能。计算机视觉也用于时尚电子商务、库存管理、专利搜索、家具和美容行业。 </p> 
<h3>3.7 <span>区别</span></h3> 
<p>与计算机视觉最密切相关的领域是图像处理、图像分析和机器视觉。这些覆盖的技术和应用范围有很大的重叠。这意味着在这些领域中使用和开发的基本技术是相似的，这可以解释为只有一个不同名称的领域。另一方面，研究团体、科学期刊、会议和公司似乎有必要将自己展示或推销为属于这些领域中的一个，因此，已经展示了将每个领域与其他领域区分开来的各种特征。 </p>
<p>计算机图形学从3D模型中产生图像数据，计算机视觉经常从图像数据中产生3D模型。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_20" class="kx_ref">[20]</a></sup>还有一种趋势是将这两个学科结合起来，例如在增强现实中探索的那样。 </p>
<p>以下特征似乎相关，但不应被视为普遍接受： </p> 
<ul>
 <li>图像处理和图像分析倾向于关注2D图像，如何将一幅图像转换成另一幅图像，例如通过像素操作（如对比度增强）、局部操作，（如边缘提取或噪声去除），或者几何转换，（如旋转图像）。这种表征意味着图像处理/分析既不需要假设，也不产生关于图像内容的解释。</li> 
 <li>计算机视觉包括来自2D图像的3D分析。这分析投影到一个或几个图像上的3D场景，例如，如何从一个或几个图像重建关于3D场景的结构或其他信息。计算机视觉通常依赖于对图像中描绘的场景或多或少复杂的假设。</li> 
 <li>机器视觉是在工业应用中应用一系列技术和方法来提供基于图像的自动检查、过程控制和机器人导航的过程。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_21" class="kx_ref">[21]</a></sup>机器视觉往往侧重于应用，<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_19" class="kx_ref">[19]</a></sup>主要是在制造领域，例如，基于视觉的机器人和基于视觉的检查、测量或拣选系统(如拣箱系统<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_22" class="kx_ref">[22]</a></sup>)。这意味着图像传感器技术和控制理论通常与图像数据的处理相结合来控制机器人，并且通过硬件和软件中的有效实现来强调实时处理。这也意味着例如照明之类的外部条件可以并且经常比一般的计算机视觉更好地在机器视觉中控制，这使得能够使用不同的算法。</li> 
 <li>还有一个被称为成像的领域，它主要关注图像的产生过程，但有时也涉及图像的处理和分析。例如，医学成像包括医学应用中图像数据分析的大量工作。</li> 
 <li>最后，模式识别是一个普遍使用各种方法从信号中提取信息的领域，主要基于统计方法和人工神经网络。该领域的很大一部分致力于将这些方法应用于图像数据。</li>
</ul> 
<p>摄影测量也与计算机视觉重叠，例如立体摄影测量与计算机立体视觉。 </p></div></div><div id="par_14996291335487751"><h2 class="title">4 应用程序<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>应用范围从工业机器视觉系统(比如检测生产线上飞驰而过的瓶子)到研究人工智能和能够理解周围世界的计算机或机器人。计算机视觉和机器视觉领域有很大的重叠。计算机视觉涵盖了应用于许多领域的自动图像分析的核心技术。机器视觉通常指将自动图像分析与其他方法和技术相结合的过程，以在工业应用中提供自动检查和机器人引导。在许多计算机视觉应用中，计算机被预先编程以解决特定的任务，但是基于学习的方法现在变得越来越普遍。计算机视觉应用的例子包括以下系统： </p> 
<p></p><p></p><div class="text_img ed_imgfloat_right">
            <a class="ed_image_link lazyLoad" data-src="https://img02.sogoucdn.com/app/a/200698/sogou_science_16409" data-bigsrc="" title="点击查看大图" href="javascript:" data-observer="true"></a>
            <div class="text_img_title">学习3D形状一直是计算机视觉领域的一项具有挑战性的任务。深度学习的最新进展使研究人员能够构建模型，能够从单视图或多视图深度图或轮廓无缝高效地生成和重建3D形状模型 [1]</div>   
        </div> <p></p><p></p> 
<ul>
 <li>自动检查，例如在制造应用中；</li> 
 <li>协助人类完成识别任务，例如物种识别系统；<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_23" class="kx_ref">[23]</a></sup></li> 
 <li>控制过程，例如工业机器人；</li> 
 <li>检测事件，例如用于视觉监视或人员计数；</li> 
 <li>交互，例如，作为计算机-人交互设备的输入；</li> 
 <li>建模对象或环境，例如医学图像分析或地形建模；</li> 
 <li>导航，例如通过自主车辆或移动机器人；</li> 
 <li>组织信息，例如，用于索引图像和图像序列的数据库。</li>
</ul> 
<p></p><p></p><div class="text_img ed_imgfloat_right">
            <a class="ed_image_link lazyLoad" data-src="https://img04.sogoucdn.com/app/a/200698/sogou_science_16410" data-bigsrc="" title="点击查看大图" href="javascript:" data-observer="true"></a>
            <div class="text_img_title">国防高级研究计划局的视觉媒体推理概念视频</div>   
        </div> <p></p><p></p> 
<p>最突出的应用领域之一是医学计算机视觉或医学图像处理，其特征在于从图像数据中提取信息来诊断患者。这方面的一个例子是肿瘤、动脉硬化或其他恶性变化的检测；器官尺寸、血流等的测量是另一个例子。它还通过提供新的信息来支持医学研究:例如，关于大脑结构，或者关于医疗质量。计算机视觉在医学领域的应用还包括增强人类解释的图像——例如超声波图像或x光图像——以减少噪声的影响。 </p>
<p>计算机视觉的第二个应用领域是工业，有时也称为机器视觉，在工业中提取信息是为了支持制造过程。一个例子是质量控制，其中细节或最终产品被自动检查以发现缺陷。另一个例子是测量机器人手臂要拾取的细节的位置和方向。机器视觉也广泛应用于农业过程中，从散装材料中去除不需要的食物，这一过程称为光学分选。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_24" class="kx_ref">[24]</a></sup> </p>
<p>军事应用可能是计算机视觉最大的领域之一。显而易见的例子是敌兵或车辆的探测和导弹制导。更先进的导弹制导系统将导弹发送到一个区域，而不是一个特定的目标，当导弹到达该区域时，根据本地获取的图像数据进行目标选择。现代军事概念，如“战场感知”，意味着包括图像传感器在内的各种传感器提供了一组丰富的战斗场景信息，可用于支持战略决策。在这种情况下，数据的自动处理用于降低复杂性，并融合来自多个传感器的信息，以提高可靠性。 </p> 
<p></p><p></p><div class="text_img ed_imgfloat_right">
            <a class="ed_image_link lazyLoad" data-src="https://img02.sogoucdn.com/app/a/200698/sogou_science_16411" data-bigsrc="" title="点击查看大图" href="javascript:" data-observer="true"></a>
            <div class="text_img_title">艺术家对火星探测车的概念，无人驾驶陆基交通工具的一个例子。请注意安装在探测车顶部的立体摄像机。</div>   
        </div> <p></p><p></p> 
<p>较新的应用领域之一是自主飞行器，包括潜水器、陆基飞行器(带轮子、汽车或卡车的小型机器人)、飞行器和无人驾驶飞行器(UAV)。自主程度从完全自主(无人)车辆到基于计算机视觉的系统在各种情况下支持驾驶员或飞行员的车辆不等。全自动车辆通常使用计算机视觉进行导航，例如知道它在哪里，或者制作环境地图和探测障碍物。它还可以用于检测特定任务特定事件，例如无人机寻找森林火灾。支持系统的例子有汽车中的障碍物警告系统和飞机自主着陆系统。几家汽车制造商已经展示了汽车自动驾驶系统，但是这项技术还没有达到可以投放市场的水平。有很多军用自主飞行器的例子，从先进的导弹到用于侦察任务或导弹制导的无人机。太空探索已经在使用计算机视觉的自主飞行器进行，例如美国宇航局的火星探索漫游者和欧空局的外火星漫游者。 </p>
<p>其他应用领域包括： </p> 
<ul>
 <li>持电影和广播的视觉效果创建，例如摄像机跟踪(匹配移动)。</li> 
 <li>监控。</li> 
 <li>生物科学中的生物跟踪和计数<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_25" class="kx_ref">[25]</a></sup></li>
</ul></div></div><div id="par_14996291335487752"><h2 class="title">5 典型任务<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>上述每个应用领域都采用一系列计算机视觉任务；或多或少定义明确的测量问题或处理问题，可以使用多种方法来解决。典型计算机视觉任务的一些例子如下。 </p>
<p>计算机视觉任务包括获取、处理、分析和理解数字图像的方法，以及从现实世界中提取高维数据以产生数字或符号信息的方法，例如以决策的形式。<sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_4" class="kx_ref">[4]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_5" class="kx_ref">[5]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_6" class="kx_ref">[6]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_7" class="kx_ref">[7]</a></sup></sup> 在这种背景下，理解意味着将视觉图像(视网膜的输入)转化为对世界的描述，这种描述可以与其他思维过程相结合并引发适当的行动。这种图像理解可以被看作是利用借助几何、物理、统计和学习理论构建的模型从图像数据中分离符号信息。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_8" class="kx_ref">[8]</a></sup> </p> 
<h3>5.1 <span>识别</span></h3> 
<p>计算机视觉、图像处理和机器视觉中的经典问题是确定图像数据是否包含某些特定的对象、特征或活动。文献中描述了不同种类的识别问题: </p> 
<ul>
 <li>象识别(也称为对象分类)-可以识别一个或几个预先指定或学习的对象或对象类，通常连同它们在图像中的2D位置或场景中的3D姿态。Google Goggles和LikeThat提供了说明此功能的独立程序。</li> 
 <li>标识–识别对象的单个实例。示例包括特定人的面部或指纹的识别、手写数字的识别或特定车辆的识别。</li> 
 <li>检测–扫描图像数据以确定特定条件。例子包括在医学图像中检测可能的异常细胞或组织，或者在自动道路收费系统中检测车辆。基于相对简单和快速计算的检测有时用于寻找感兴趣图像数据的较小区域，这些区域可以通过计算要求更高的技术进一步分析，以产生正确的解释。</li>
</ul> 
<p>目前，这类任务的最佳算法是基于卷积神经网络的。ImageNet大规模视觉识别挑战展示了它们的能力；这是对象分类和检测的基准，有数百万张图像和数百个对象类别。在ImageNet测试中，卷积神经网络的性能现在接近人类。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_26" class="kx_ref">[26]</a></sup>最好的算法仍然与小而薄的物体斗争，比如花茎上的小蚂蚁或者手里拿着羽毛笔的人。他们也有被过滤器扭曲的图像的问题(这在现代数码相机中越来越普遍)。相比之下，这种图像很少困扰人类。然而，人类在其他问题上往往有困难。例如，他们不擅长将对象分类为细粒度的类，例如特定品种的狗或鸟，而卷积神经网络很容易处理。 </p>
<p>存在几项基于识别的专门任务，例如： </p> 
<ul>
 <li>基于内容的图像检索——在一组较大的图像中找到所有具有特定内容的图像。可以用不同的方式来指定内容，例如根据与目标图像的相似性(给我所有与图像X相似的图像)，或者根据作为文本输入给出的高级搜索标准(给我所有包含许多房屋的图像，这些图像是在冬天拍摄的，并且其中没有汽车)。</li>
</ul> 
<p></p><p></p><div class="text_img ed_imgfloat_right">
            <a class="ed_image_link lazyLoad" data-src="https://img02.sogoucdn.com/app/a/200698/sogou_science_16412" data-bigsrc="" title="点击查看大图" href="javascript:" data-observer="true"></a>
            <div class="text_img_title">在公共场所、商场、购物中心，人们使用计算机视觉可以达到相反的目的</div>   
        </div> <p></p><p></p> 
<ul>
 <li>姿势估计–估计特定对象相对于相机的位置或方向。这种技术的一个示例应用是在装配线情况下帮助机械臂从传送带上取回物体或者从箱子中拾取零件。</li> 
 <li>光学字符识别(OCR)-识别打印或手写文本图像中的字符，通常是为了以更易于编辑或索引(如ASCII)的格式对文本进行编码。</li> 
 <li>2D码读取–读取2D码，如数据矩阵和二维码。</li> 
 <li>面部识别</li> 
 <li>人体计数器系统中的形状识别技术(SRT)，用于区分人(头部和肩部模式)和物体</li>
</ul> 
<h3>5.2 <span>动作分析</span></h3> 
<p>几个任务涉及运动估计，其中处理图像序列以估计图像中或3D场景中的每个点的速度，或者甚至产生图像的照相机的速度。这类任务的例子有： </p> 
<ul>
 <li>自我运动——根据摄像机产生的图像序列确定摄像机的3D刚性运动(旋转和平移)。</li> 
 <li>跟踪–跟踪图像序列中(通常)一组较小的兴趣点或对象(例如，车辆、人类或其他生物<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_25" class="kx_ref">[25]</a></sup>)的运动。</li> 
 <li>光流——为图像中的每个点确定该点相对于图像平面的运动方式，即其表观运动。该运动是相应的3D点如何在场景中移动以及相机如何相对于场景移动的结果。</li>
</ul> 
<h3>5.3 <span>场景重建</span></h3> 
<p>给定场景或视频的一个或(通常)多个图像，场景重建旨在计算场景的3D模型。在最简单的情况下，模型可以是一组3D点。更复杂的方法产生一个完整的3D表面模型。不需要运动或扫描的3D成像以及相关处理算法的出现使得该领域能够快速发展。基于网格的3D传感可用于从多个角度获取3D图像。现在可以使用算法将多个3D图像拼接成点云和3D模型。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_20" class="kx_ref">[20]</a></sup> </p> 
<h3>5.4 <span>图像恢复</span></h3> 
<p>图像恢复的目的是从图像中去除噪声(传感器噪声、运动模糊等)。消除噪声的最简单方法是各种类型的滤波器，如低通滤波器或中值滤波器。更复杂的方法是假设一个局部图像结构的模型，将它们与噪声区分开。通过首先根据局部图像结构(例如线或边)分析图像数据，然后基于来自分析步骤的局部信息控制滤波，与更简单的方法相比，通常获得更好的噪声去除水平。 </p>
<p>这个领域的一个例子是修复。 </p></div></div><div id="par_14996291335487753"><h2 class="title">6 系统方法<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>计算机视觉系统的组织高度依赖于应用。一些系统是解决特定测量或检测问题的独立应用程序，而另一些系统构成更大设计的子系统，例如，该子系统还包含用于控制机械执行器、规划、信息数据库、人机界面等的子系统。计算机视觉系统的具体实现还取决于它的功能是否是预先指定的，或者它的某些部分是否可以在操作过程中学习或修改。许多功能对于应用程序来说是唯一的。然而，在许多计算机视觉系统中都有典型的功能。 </p> 
<ul>
 <li>图像采集——数字图像是由一个或几个图像传感器产生的，除了各种类型的光敏摄像机，还包括距离传感器、断层摄影设备、雷达、超声波摄像机等。根据传感器的类型，得到的图像数据是普通的2D图像、3D体积或图像序列。像素值通常对应于一个或几个光谱带(灰度图像或彩色图像)中的光强度，但是也可以与各种物理测量相关，例如声波或电磁波的深度、吸收或反射，或者核磁共振。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_24" class="kx_ref">[24]</a></sup></li> 
 <li>预处理——在将计算机视觉方法应用于图像数据以提取某些特定信息之前，通常需要对数据进行处理，以确保其满足该方法隐含的某些假设。例子有： 
  <ul>
   <li>重新采样以确保图像坐标系正确。</li> 
   <li>降噪以确保传感器噪声不会引入错误信息。</li> 
   <li>增强对比度，以确保相关信息能够被检测到。</li> 
   <li>缩放空间表示以在局部适当比例下增强图像结构。</li>
  </ul></li> 
 <li>特征提取——从图像数据中提取不同复杂程度的图像特征。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_24" class="kx_ref">[24]</a></sup> 这种特征的典型例子有： 
  <ul>
   <li>线条、边缘和脊。</li> 
   <li>局部兴趣点，如角、斑点或点。</li>
  </ul></li>
</ul> 
<dl>
 <dd>
  更复杂的特征可能与纹理、形状或运动有关。
 </dd>
</dl> 
<ul>
 <li>检测/分割–在处理过程中的某个时刻，决定图像的哪些图像点或区域与进一步处理相关。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_24" class="kx_ref">[24]</a></sup> 例子有： 
  <ul>
   <li>选择一组特定的兴趣点。</li> 
   <li>包含特定感兴趣对象的一个或多个图像区域的分割。</li> 
   <li>将图像分割成嵌套场景结构，包括前景、对象组、单个对象或显著对象部分(也称为空间分类场景层次结构)，<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_27" class="kx_ref">[27]</a></sup>而视觉显著性通常被实现为空间和时间注意。</li> 
   <li>将一个或多个视频分割或共同分割成一系列每帧前景遮罩，同时保持其时间语义连续性。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_28" class="kx_ref">[28]</a></sup><sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_29" class="kx_ref">[29]</a></sup></li>
  </ul></li> 
 <li>高级处理——在这一步骤中，输入通常是一小组数据，例如一组点或一个图像区域，假设它包含一个特定的对象。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_24" class="kx_ref">[24]</a></sup>其余的处理涉及，例如： 
  <ul>
   <li>验证数据是否满足基于模型和特定应用的假设。</li> 
   <li>特定于应用程序的参数评估，如对象姿态或对象大小。</li> 
   <li>图像识别–将检测到的对象分为不同的类别。</li> 
   <li>图像配准–比较和组合同一对象的两个不同视图。</li>
  </ul></li> 
 <li>决策制定应用程序所需的最终决策，<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_24" class="kx_ref">[24]</a></sup> 例如： 
  <ul>
   <li>自动检查应用的通过/失败。</li> 
   <li>识别应用中的匹配/不匹配。</li> 
   <li>医学、军事、安全和识别应用中的进一步人类审查标志。</li>
  </ul></li>
</ul> 
<h3>6.1 <span>图像理解系统</span></h3> 
<p>图像理解系统(IUS)包括如下三个抽象层次:低层次包括图像图元，如边缘、纹理元素或区域；中间层包括边界、表面和体积；高层次包括对象、场景或事件。这些要求中的许多都是需要进一步研究的课题。 </p>
<p>IUS设计中对这些层次的表征要求是:原型概念的表征、概念组织、空间知识、时间知识、标度以及通过比较和区分的描述。 </p>
<p>推理指的是从当前已知的事实中导出新的、未明确表示的事实的过程，而控制指的是选择在处理的特定阶段应该应用多种推理、搜索和匹配技术中的哪一种的过程。IUS的推理和控制要求是:搜索和假设激活、匹配和假设检验、期望的产生和使用、注意力的改变和集中、信念的确定性和强度、推理和目标满意度。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_30" class="kx_ref">[30]</a></sup> </p></div></div><div id="par_14996291335487754"><h2 class="title">7 硬件<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>计算机视觉系统有很多种；然而，它们都包含这些基本元素:电源、至少一个图像采集设备(照相机、ccd等)。)、处理器以及控制和通信电缆或某种无线互连机制。此外，一个实用的视觉系统包含软件，以及一个显示器以便监控系统。与大多数工业空间一样，内部空间的视觉系统包含照明系统，并可置于受控环境中。此外，完整的系统包括许多附件，如摄像机支架、电缆和连接器。 </p>
<p>大多数计算机视觉系统使用可见光相机以每秒最多60帧(通常慢得多)的帧速率被动地观看场景。 </p>
<p>一些计算机视觉系统使用具有主动照明的图像采集硬件或除可见光以外的其他东西或两者兼有，例如结构光3D扫描仪、热像照相机、高光谱成像仪、雷达成像、激光雷达扫描仪、磁共振图像、侧扫声纳、合成孔径声纳等。这种硬件捕捉“图像”，然后通常使用与处理可见光图像相同的计算机视觉算法进行处理。 </p>
<p>虽然传统的广播和消费视频系统以每秒30帧的速度运行，但是数字信号处理和消费图形硬件的进步使得实时系统能够以每秒数百至数千帧的速度进行高速图像采集、处理和显示。对于机器人应用来说，快速、实时的视频系统至关重要，通常可以简化某些算法所需的处理。当与高速投影仪结合时，快速图像采集允许实现3D测量和特征跟踪。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_31" class="kx_ref">[31]</a></sup> </p>
<p>以自我为中心的视觉系统由一台可穿戴式的照相机组成，它可以从第一人称的角度自动拍照。 </p>
<p>截至2016年，视觉处理单元正在成为一种新型处理器，以补充该角色中的CPU和图形处理单元(GPU)。<sup><a href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/d11066.htm#quote_32" class="kx_ref">[32]</a></sup> </p> 
<h3>7.1 <span>列表</span></h3> 
<ul>
 <li>计算机视觉主题列表</li> 
 <li>新兴技术列表</li> 
 <li>人工智能概述</li>
</ul></div></div></div></div><div id="references"><h2 class="title" id="par_references">参考文献</h2><ul class="references"><li id="quote_1"><span class="references-num">[1]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Dana H. Ballard; Christopher M. Brown (1982). Computer Vision. Prentice Hall. ISBN 978-0-13-165316-0..</span></p></li><li id="quote_2"><span class="references-num">[2]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Huang, T. (1996-11-19). Vandoni, Carlo, E, ed. Computer Vision : Evolution And Promise (PDF). 19th CERN School of Computing. Geneva: CERN. pp. 21–25. doi:10.5170/CERN-1996-008.21. ISBN 978-9290830955..</span></p></li><li id="quote_3"><span class="references-num">[3]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Milan Sonka; Vaclav Hlavac; Roger Boyle (2008). Image Processing, Analysis, and Machine Vision. Thomson. ISBN 978-0-495-08252-1..</span></p></li><li id="quote_4"><span class="references-num">[4]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Reinhard Klette (2014). Concise Computer Vision. Springer. ISBN 978-1-4471-6320-6..</span></p></li><li id="quote_5"><span class="references-num">[5]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Linda G. Shapiro; George C. Stockman (2001). Computer Vision. Prentice Hall. ISBN 978-0-13-030796-5..</span></p></li><li id="quote_6"><span class="references-num">[6]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Tim Morris (2004). Computer Vision and Image Processing. Palgrave Macmillan. ISBN 978-0-333-99451-1..</span></p></li><li id="quote_7"><span class="references-num">[7]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Bernd Jähne; Horst Haußecker (2000). Computer Vision and Applications, A Guide for Students and Practitioners. Academic Press. ISBN 978-0-13-085198-7..</span></p></li><li id="quote_8"><span class="references-num">[8]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">David A. Forsyth; Jean Ponce (2003). Computer Vision, A Modern Approach. Prentice Hall. ISBN 978-0-13-085198-7..</span></p></li><li id="quote_9"><span class="references-num">[9]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">https://web.archive.org/web/20221025122739/http://www.bmva.org/visionoverview The British Machine Vision Association and Society for Pattern Recognition Retrieved February 20, 2017.</span></p></li><li id="quote_10"><span class="references-num">[10]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Murphy, Mike. "Star Trek's "tricorder" medical scanner just got closer to becoming a reality"..</span></p></li><li id="quote_11"><span class="references-num">[11]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Richard Szeliski (30 September 2010). Computer Vision: Algorithms and Applications. Springer Science &amp; Business Media. pp. 10–16. ISBN 978-1-84882-935-0..</span></p></li><li id="quote_12"><span class="references-num">[12]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Papert, Seymour (1966-07-01). "The Summer Vision Project". MIT AI Memos (1959 - 2004). hdl:1721.1/6125..</span></p></li><li id="quote_13"><span class="references-num">[13]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Margaret Ann Boden (2006). Mind as Machine: A History of Cognitive Science. Clarendon Press. p. 781. ISBN 978-0-19-954316-8..</span></p></li><li id="quote_14"><span class="references-num">[14]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Takeo Kanade (6 December 2012). Three-Dimensional Machine Vision. Springer Science &amp; Business Media. ISBN 978-1-4613-1981-8..</span></p></li><li id="quote_15"><span class="references-num">[15]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Nicu Sebe; Ira Cohen; Ashutosh Garg; Thomas S. Huang (3 June 2005). Machine Learning in Computer Vision. Springer Science &amp; Business Media. ISBN 978-1-4020-3274-5..</span></p></li><li id="quote_16"><span class="references-num">[16]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">William Freeman; Pietro Perona; Bernhard Scholkopf (2008). "Guest Editorial: Machine Learning for Computer Vision". International Journal of Computer Vision. 77 (1): 1. doi:10.1007/s11263-008-0127-7. ISSN 1573-1405..</span></p></li><li id="quote_17"><span class="references-num">[17]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Information Engineering | Department of Engineering". www.eng.cam.ac.uk (in 英语). Retrieved 2018-10-03..</span></p></li><li id="quote_18"><span class="references-num">[18]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Information Engineering Main/Home Page". www.robots.ox.ac.uk (in 英语). Retrieved 2018-10-03..</span></p></li><li id="quote_19"><span class="references-num">[19]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Steger, Carsten; Markus Ulrich; Christian Wiedemann (2018). Machine Vision Algorithms and Applications (2nd ed.). Weinheim: Wiley-VCH. p. 1. ISBN 978-3-527-41365-2. Retrieved 2018-01-30..</span></p></li><li id="quote_20"><span class="references-num">[20]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Soltani, A. A., Huang, H., Wu, J., Kulkarni, T. D., &amp; Tenenbaum, J. B. Synthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes With Deep Generative Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1511-1519)". 2019-01-25..</span></p></li><li id="quote_21"><span class="references-num">[21]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Turek, Fred (June 2011). "Machine Vision Fundamentals, How to Make Robots See". NASA Tech Briefs Magazine. 35 (6). pages 60–62.</span></p></li><li id="quote_22"><span class="references-num">[22]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">https://web.archive.org/web/20221025122739/https://www.robots.com/blog/viewing/the-future-of-automated-random-bin-picking.</span></p></li><li id="quote_23"><span class="references-num">[23]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Wäldchen, Jana; Mäder, Patrick (2017-01-07). "Plant Species Identification Using Computer Vision Techniques: A Systematic Literature Review". Archives of Computational Methods in Engineering (in 英语). 25 (2): 507–543. doi:10.1007/s11831-016-9206-z. ISSN 1134-3060..</span></p></li><li id="quote_24"><span class="references-num">[24]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">E. Roy Davies (2005). Machine Vision: Theory, Algorithms, Practicalities. Morgan Kaufmann. ISBN 978-0-12-206093-9..</span></p></li><li id="quote_25"><span class="references-num">[25]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Bruijning, Marjolein; Visser, Marco D.; Hallmann, Caspar A.; Jongejans, Eelke; Golding, Nick (2018). "trackdem: Automated particle tracking to obtain population counts and size distributions from videos in r". Methods in Ecology and Evolution. 9 (4): 965–973. doi:10.1111/2041-210X.12975. ISSN 2041-210X..</span></p></li><li id="quote_26"><span class="references-num">[26]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">O. Russakovsky et al., "ImageNet Large Scale Visual Recognition Challenge", 2014..</span></p></li><li id="quote_27"><span class="references-num">[27]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Barghout, Lauren. "Visual Taxometric Approach to Image Segmentation Using Fuzzy-Spatial Taxon Cut Yields Contextually Relevant Regions." Information Processing and Management of Uncertainty in Knowledge-Based Systems. Springer International Publishing, 2014..</span></p></li><li id="quote_28"><span class="references-num">[28]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Liu, Ziyi; Wang, Le; Hua, Gang; Zhang, Qilin; Niu, Zhenxing; Wu, Ying; Zheng, Nanning (2018). "Joint Video Object Discovery and Segmentation by Coupled Dynamic Markov Networks" (PDF). IEEE Transactions on Image Processing. 27 (12): 5840–5853. doi:10.1109/tip.2018.2859622. ISSN 1057-7149. PMID 30059300..</span></p></li><li id="quote_29"><span class="references-num">[29]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Wang, Le; Duan, Xuhuan; Zhang, Qilin; Niu, Zhenxing; Hua, Gang; Zheng, Nanning (2018-05-22). "Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation" (PDF). Sensors. 18 (5): 1657. doi:10.3390/s18051657. ISSN 1424-8220. PMC 5982167. PMID 29789447..</span></p></li><li id="quote_30"><span class="references-num">[30]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Shapiro, Stuart C. (1992). Encyclopedia of Artificial Intelligence, Volume 1. New York: John WIley &amp; Sons, Inc. pp. 643–646. ISBN 978-0-471-50306-4..</span></p></li><li id="quote_31"><span class="references-num">[31]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Kagami, Shingo (2010). High-speed vision systems and projectors for real-time perception of the world. IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops. 2010. pp. 100–107. doi:10.1109/CVPRW.2010.5543776. ISBN 978-1-4244-7029-7. Retrieved 2 May 2016..</span></p></li><li id="quote_32"><span class="references-num">[32]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Seth Colaner (January 3, 2016). "A Third Type Of Processor For VR/AR: Movidius' Myriad 2 VPU". www.tomshardware.com..</span></p></li></ul></div><div class="read-num">阅读 <!-- -->1.1<!-- -->w</div></div><div class="right-side" id="rightSide"><div class="side" id="lemma-side"><div class="side-title">版本记录</div><ul class="side-lst"><li><p class="side-lst-txt">暂无</p></li></ul><div class="user-card userCard"></div></div><div class="side"><div class="side-event"></div></div></div></div><div class="footer-box"><div id="footer"><div class="footer-logo-wrap"><div class="footer-logo"></div><div class="footer-logo-text">知识·传播·科普</div></div><div class="footer-info">本网站内容采用<a target="_blank" href="https://web.archive.org/web/20221025122739/https://creativecommons.org/licenses/by-sa/3.0/deed.zh?tdsourcetag=s_pctim_aiomsg">CC-BY-SA 3.0</a>授权</div><div class="footer-btn-wrap"><a target="_blank" href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/help/#user_protocol">用户协议</a><a target="_blank" href="https://web.archive.org/web/20221025122739/http://www.sogou.com/docs/terms.htm?v=1">免责声明</a><a target="_blank" href="https://web.archive.org/web/20221025122739/http://corp.sogou.com/private.html">隐私政策</a><a target="_blank" href="https://web.archive.org/web/20221025122739/https://baike.sogou.com/kexue/intro.htm">关于我们</a></div></div></div><script>window.lemmaInfo ={"lemmaId":"11066","versionId":"14996291301933316","title":"计算机视觉","subtitle":"","abstracts":{"paragraphId":"14996291318710541","title":"简介","versionId":"14996291301933317","lemmaId":11066,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":76036814,"name":"王鹏","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1576234485,"comment":null,"dependVersionId":0,"contentType":2,"content":"<p>计算机视觉是一个跨学科的科学领域，研究如何让计算机从数字图像或视频中获得高水平的理解。从工程学的角度来看，它寻求人类视觉系统能够完成的自动化任务。<sup><a href=\"#quote_1\" class=\"kx_ref\">[1]</a></sup><sup><a href=\"#quote_2\" class=\"kx_ref\">[2]</a></sup><sup><a href=\"#quote_3\" class=\"kx_ref\">[3]</a></sup> </p>\n<p>计算机视觉任务包括获取、处理、分析和理解数字图像的方法，以及从现实世界中提取高维数据以便例如以决策的形式产生数字或符号信息的方法。<sup><sup><a href=\"#quote_4\" class=\"kx_ref\">[4]</a></sup><sup><a href=\"#quote_5\" class=\"kx_ref\">[5]</a></sup><sup><a href=\"#quote_6\" class=\"kx_ref\">[6]</a></sup><sup><a href=\"#quote_7\" class=\"kx_ref\">[7]</a></sup></sup> 在这种背景下，理解意味着将视觉图像(视网膜的输入)转化为对世界的描述，这种描述可以与其他思维过程相结合并引发适当的行动。这种图像理解可以被看作是利用借助几何、物理、统计和学习理论构建的模型从图像数据中分离符号信息。<sup><a href=\"#quote_8\" class=\"kx_ref\">[8]</a></sup> </p>\n<p>作为一门科学学科，计算机视觉涉及从图像中提取信息的人工系统背后的理论。图像数据可以采取多种形式，例如视频序列、来自多个摄像机的视图或者来自医学扫描仪的多维数据。作为一门技术学科，计算机视觉寻求将其理论和模型应用于计算机视觉系统的构建。 </p>\n<p>计算机视觉的子领域包括场景重建、事件检测、视频跟踪、对象识别、3D姿态估计、学习、索引、运动估计和图像恢复。<sup>[6]</sup> </p>","pics":null,"card":null,"references":[],"versionCount":0},"card":{"paragraphId":"0","title":null,"versionId":"0","lemmaId":0,"createType":0,"creator":null,"createTime":0,"versionEditor":null,"editTime":0,"comment":null,"dependVersionId":0,"contentType":0,"content":null,"pics":null,"card":null,"references":null,"versionCount":0},"categories":[{"id":1,"name":"计算机","parents":[]}],"creator":{"uid":76036814,"name":"王鹏","pic":"https://web.archive.org/web/20221025122739/https://cache.soso.com/qlogo/g?b=oidb&k=vPrXEleaGUXJKTkpD5MQsw&s=100&t=1555931528","introduction":null,"educations":[{"schoolName":"中国地质大学（北京）","major":"","degree":"硕士","universityId":22,"universityLogo":"https://web.archive.org/web/20221025122739/https://img01.sogoucdn.com/app/a/200943/d3465c1c-6011-11e9-b353-fc4dd4f70029","majorLevel1":null,"majorLevel2":null,"majorLevel3":null,"majorLevel1Id":0,"majorLevel2Id":0,"majorLevel3Id":0,"state":null,"lab":null,"researchField":null}],"jobs":null,"works":null,"educationBrief":"中国地质大学（北京）","jobBrief":"","role":0,"roleName":null,"title":"中国地质大学（北京） · 硕士","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":41,"partnerIdCreateTime":1594286502,"partnerIdPoped":false},"createTime":1571038509,"editor":{"uid":76036814,"name":"王鹏","pic":"https://web.archive.org/web/20221025122739/https://cache.soso.com/qlogo/g?b=oidb&k=vPrXEleaGUXJKTkpD5MQsw&s=100&t=1555931528","introduction":null,"educations":[{"schoolName":"中国地质大学（北京）","major":"","degree":"硕士","universityId":22,"universityLogo":"https://web.archive.org/web/20221025122739/https://img01.sogoucdn.com/app/a/200943/d3465c1c-6011-11e9-b353-fc4dd4f70029","majorLevel1":null,"majorLevel2":null,"majorLevel3":null,"majorLevel1Id":0,"majorLevel2Id":0,"majorLevel3Id":0,"state":null,"lab":null,"researchField":null}],"jobs":null,"works":null,"educationBrief":"中国地质大学（北京）","jobBrief":"","role":0,"roleName":null,"title":"中国地质大学（北京） · 硕士","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":41,"partnerIdCreateTime":1594286502,"partnerIdPoped":false},"editTime":1576234485,"state":1,"versionCount":1,"upNum":5,"downNum":0,"pics":[{"originalUrl":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16409?w=300&h=202&titlename=%E5%AD%A6%E4%B9%A03D%E5%BD%A2%E7%8A%B6%E4%B8%80%E7%9B%B4%E6%98%AF%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E9%A1%B9%E5%85%B7%E6%9C%89%E6%8C%91%E6%88%98%E6%80%A7%E7%9A%84%E4%BB%BB%E5%8A%A1%E3%80%82%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E4%BD%BF%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E8%83%BD%E5%A4%9F%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%83%BD%E5%A4%9F%E4%BB%8E%E5%8D%95%E8%A7%86%E5%9B%BE%E6%88%96%E5%A4%9A%E8%A7%86%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%9B%BE%E6%88%96%E8%BD%AE%E5%BB%93%E6%97%A0%E7%BC%9D%E9%AB%98%E6%95%88%E5%9C%B0%E7%94%9F%E6%88%90%E5%92%8C%E9%87%8D%E5%BB%BA3D%E5%BD%A2%E7%8A%B6%E6%A8%A1%E5%9E%8B%20%5B1%5D","url":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16409","rw":300,"rh":202,"title":"学习3D形状一直是计算机视觉领域的一项具有挑战性的任务。深度学习的最新进展使研究人员能够构建模型，能够从单视图或多视图深度图或轮廓无缝高效地生成和重建3D形状模型 [1]","alt":null,"width":0,"height":0},{"originalUrl":"https://web.archive.org/web/20221025122739/https://img04.sogoucdn.com/app/a/200698/sogou_science_16410?w=300&h=169&titlename=%E5%9B%BD%E9%98%B2%E9%AB%98%E7%BA%A7%E7%A0%94%E7%A9%B6%E8%AE%A1%E5%88%92%E5%B1%80%E7%9A%84%E8%A7%86%E8%A7%89%E5%AA%92%E4%BD%93%E6%8E%A8%E7%90%86%E6%A6%82%E5%BF%B5%E8%A7%86%E9%A2%91","url":"https://web.archive.org/web/20221025122739/https://img04.sogoucdn.com/app/a/200698/sogou_science_16410","rw":300,"rh":169,"title":"国防高级研究计划局的视觉媒体推理概念视频","alt":null,"width":0,"height":0},{"originalUrl":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16411?w=200&h=160&titlename=%E8%89%BA%E6%9C%AF%E5%AE%B6%E5%AF%B9%E7%81%AB%E6%98%9F%E6%8E%A2%E6%B5%8B%E8%BD%A6%E7%9A%84%E6%A6%82%E5%BF%B5%EF%BC%8C%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E9%99%86%E5%9F%BA%E4%BA%A4%E9%80%9A%E5%B7%A5%E5%85%B7%E7%9A%84%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E3%80%82%E8%AF%B7%E6%B3%A8%E6%84%8F%E5%AE%89%E8%A3%85%E5%9C%A8%E6%8E%A2%E6%B5%8B%E8%BD%A6%E9%A1%B6%E9%83%A8%E7%9A%84%E7%AB%8B%E4%BD%93%E6%91%84%E5%83%8F%E6%9C%BA%E3%80%82","url":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16411","rw":200,"rh":160,"title":"艺术家对火星探测车的概念，无人驾驶陆基交通工具的一个例子。请注意安装在探测车顶部的立体摄像机。","alt":null,"width":0,"height":0},{"originalUrl":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16412?w=300&h=139&titlename=%E5%9C%A8%E5%85%AC%E5%85%B1%E5%9C%BA%E6%89%80%E3%80%81%E5%95%86%E5%9C%BA%E3%80%81%E8%B4%AD%E7%89%A9%E4%B8%AD%E5%BF%83%EF%BC%8C%E4%BA%BA%E4%BB%AC%E4%BD%BF%E7%94%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%8F%AF%E4%BB%A5%E8%BE%BE%E5%88%B0%E7%9B%B8%E5%8F%8D%E7%9A%84%E7%9B%AE%E7%9A%84","url":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16412","rw":300,"rh":139,"title":"在公共场所、商场、购物中心，人们使用计算机视觉可以达到相反的目的","alt":null,"width":0,"height":0}],"catalogs":[{"level":1,"title":"定义","paragraphId":"14996291318710542","subCatalogs":null},{"level":1,"title":"历史","paragraphId":"14996291318710543","subCatalogs":null},{"level":1,"title":"相关领域","paragraphId":"14996291318710544","subCatalogs":[{"level":2,"title":"人工智能","paragraphId":"14996291318710544","subCatalogs":null},{"level":2,"title":"信息工程学","paragraphId":"14996291318710544","subCatalogs":null},{"level":2,"title":"固态物理学","paragraphId":"14996291318710544","subCatalogs":null},{"level":2,"title":"神经生物学","paragraphId":"14996291318710544","subCatalogs":null},{"level":2,"title":"信号处理","paragraphId":"14996291318710544","subCatalogs":null},{"level":2,"title":"其他领域","paragraphId":"14996291318710544","subCatalogs":null},{"level":2,"title":"区别","paragraphId":"14996291318710544","subCatalogs":null}]},{"level":1,"title":"应用程序","paragraphId":"14996291335487751","subCatalogs":null},{"level":1,"title":"典型任务","paragraphId":"14996291335487752","subCatalogs":[{"level":2,"title":"识别","paragraphId":"14996291335487752","subCatalogs":null},{"level":2,"title":"动作分析","paragraphId":"14996291335487752","subCatalogs":null},{"level":2,"title":"场景重建","paragraphId":"14996291335487752","subCatalogs":null},{"level":2,"title":"图像恢复","paragraphId":"14996291335487752","subCatalogs":null}]},{"level":1,"title":"系统方法","paragraphId":"14996291335487753","subCatalogs":[{"level":2,"title":"图像理解系统","paragraphId":"14996291335487753","subCatalogs":null}]},{"level":1,"title":"硬件","paragraphId":"14996291335487754","subCatalogs":[{"level":2,"title":"列表","paragraphId":"14996291335487754","subCatalogs":null}]},{"level":1,"title":"参考文献","paragraphId":"-1","subCatalogs":null}],"paragraphs":[{"paragraphId":"14996291318710542","title":"定义","versionId":"14996291301933318","lemmaId":11066,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":76036814,"name":"王鹏","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1576234485,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>计算机视觉是一个跨学科的领域，研究如何让计算机从数字图像或视频中获得高水平的理解。从工程学的角度来看，它寻求人类视觉系统能够完成的自动化任务。<sup><sup><a href=\"#quote_1\" class=\"kx_ref\">[1]</a></sup><sup><a href=\"#quote_2\" class=\"kx_ref\">[2]</a></sup><sup><a href=\"#quote_3\" class=\"kx_ref\">[3]</a></sup></sup> “计算机视觉涉及从单个图像或图像序列中自动提取、分析和理解有用信息。它涉及理论和算法基础的发展，以实现自动视觉理解。”<sup><a href=\"#quote_9\" class=\"kx_ref\">[9]</a></sup>作为一门科学学科，计算机视觉与从图像中提取信息的人工系统背后的理论有关。图像数据可以采取多种形式，例如视频序列、来自多个摄像机的视图或者来自医学扫描仪的多维数据。<sup><a href=\"#quote_10\" class=\"kx_ref\">[10]</a></sup>作为一门技术学科，计算机视觉寻求将其理论和模型应用于计算机视觉系统的构建。 </p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14996291318710543","title":"历史","versionId":"14996291301933319","lemmaId":11066,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":76036814,"name":"王鹏","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1576234485,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>20世纪60年代末，计算机视觉开始出现在开创人工智能的大学。它旨在模仿人类视觉系统，作为赋予机器人智能行为的垫脚石。<sup><sup><a href=\"#quote_11\" class=\"kx_ref\">[11]</a></sup></sup> 1966年，人们认为这可以通过一个夏季项目来实现，方法是将一台照相机连接到计算机上，并让它“描述它所看到的”。<sup><sup><a href=\"#quote_12\" class=\"kx_ref\">[12]</a></sup><sup><a href=\"#quote_13\" class=\"kx_ref\">[13]</a></sup></sup> </p>\n<p>将计算机视觉与当时流行的数字图像处理领域区别开来，是希望从图像中提取3D结构，其目标是实现对整个场景的理解。20世纪70年代的研究为当今存在的许多计算机视觉算法奠定了早期基础，包括从图像中提取边缘、标记线条、非多面体和多面体建模、将物体表示为较小结构的互连、光流和运动估计。<sup><a href=\"#quote_11\" class=\"kx_ref\">[11]</a></sup> </p>\n<p>接下来的十年见证了基于更严格的数学分析和计算机视觉定量方面的研究。这些包括尺度空间的概念，从阴影、纹理和焦点等各种线索推断形状，以及被称为蛇的轮廓模型。研究人员还意识到，许多这些数学概念可以在与正则化和马尔可夫随机场相同的优化框架内处理。<sup><a href=\"#quote_14\" class=\"kx_ref\">[14]</a></sup>到20世纪90年代，以前的一些研究课题变得比其他的更加活跃。投影三维重建的研究使人们对摄像机标定有了更好的理解。随着摄像机标定优化方法的出现，人们认识到，在摄影测量领域，束平差理论已经探索了许多思路。这导致了用于来自多个图像的场景的稀疏3-D重建的方法。密集立体对应问题和进一步的多视图立体技术取得了进展。同时，利用图形切割的变化来解决图像分割问题。这十年也标志着统计学习技术首次在实践中被用于识别图像中的人脸。20世纪90年代末，随着计算机图形学和计算机视觉领域之间交互的增加，出现了一个重大的变化。这包括基于图像的渲染、图像变形、视图插值、全景图像拼接和早期光场渲染。<sup><a href=\"#quote_11\" class=\"kx_ref\">[11]</a></sup> </p>\n<p>最近的工作见证了基于特征的方法的复兴，这些方法与机器学习技术和复杂的优化框架结合使用。<sup><a href=\"#quote_15\" class=\"kx_ref\">[15]</a></sup><sup><a href=\"#quote_16\" class=\"kx_ref\">[16]</a></sup> </p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14996291318710544","title":"相关领域","versionId":"14996291301933320","lemmaId":11066,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":76036814,"name":"王鹏","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1576234485,"comment":null,"dependVersionId":0,"contentType":1,"content":"<h3>人工智能</h3> \n<p>人工智能领域涉及机器人系统在环境中导航的自主规划或审议。需要对这些环境有一个详细的了解来浏览它们。关于环境的信息可以由计算机视觉系统提供，该系统充当视觉传感器并提供关于环境和机器人的高级信息。 </p>\n<p>人工智能和计算机视觉共享其他主题，如模式识别和学习技术。因此，计算机视觉有时被视为人工智能领域或一般计算机科学领域的一部分。 </p> \n<h3>信息工程学</h3> \n<p>计算机视觉通常被认为是信息工程的一部分。<sup><a href=\"#quote_17\" class=\"kx_ref\">[17]</a></sup><sup><a href=\"#quote_18\" class=\"kx_ref\">[18]</a></sup> </p> \n<h3>固态物理学</h3> \n<p>固态物理是另一个与计算机视觉密切相关的领域。大多数计算机视觉系统依靠图像传感器来检测电磁辐射，电磁辐射通常以可见光或红外光的形式存在。传感器是用量子物理设计的。光与表面相互作用的过程用物理学来解释。物理学解释了光学的行为，光学是大多数成像系统的核心部分。复杂的图像传感器甚至需要量子力学来提供对图像形成过程的完整理解。<sup><a href=\"#quote_11\" class=\"kx_ref\">[11]</a></sup>此外，物理中的各种测量问题可以用计算机视觉来解决，例如流体中的运动。 </p> \n<h3>神经生物学</h3> \n<p>第三个发挥重要作用的领域是神经生物学，特别是生物视觉系统的研究。在上个世纪，人们对眼睛、神经元和大脑结构进行了广泛的研究，致力于处理人类和各种动物的视觉刺激。这导致了对“真实”视觉系统如何操作以解决某些视觉相关任务的粗略但复杂的描述。这些结果导致了计算机视觉中的一个子领域，其中人工系统被设计成在不同的复杂程度上模拟生物系统的处理和行为。此外，在计算机视觉中开发的一些基于学习的方法(例如，基于神经网络和深度学习的图像和特征分析和分类) 具有其生物学背景。 </p>\n<p>计算机视觉研究的某些方面与生物视觉研究密切相关——事实上，正如人工智能研究的许多方面与人类意识研究以及使用存储的知识来解释、整合和利用视觉信息密切相关一样。生物视觉领域研究和模拟人类和其他动物视觉背后的生理过程。另一方面，计算机视觉研究和描述了人工视觉系统背后的软件和硬件实现的过程。生物学和计算机视觉之间的跨学科交流已被证明在这两个领域都卓有成效。<sup><a href=\"#quote_19\" class=\"kx_ref\">[19]</a></sup> </p> \n<h3>信号处理</h3> \n<p>与计算机视觉相关的另一个领域是信号处理。许多处理单变量信号(通常是时间信号)的方法可以以自然的方式扩展到计算机视觉中处理双变量信号或多变量信号。然而，由于图像的特殊性质，在计算机视觉中开发了许多方法，但在处理单变量信号方面没有对应的方法。结合信号的多维度，将信号处理中的子场定义为计算机视觉的一部分。 </p> \n<h3>其他领域</h3> \n<p>除了上述关于计算机视觉的观点之外，许多相关的研究课题也可以从纯数学的角度来研究。例如，计算机视觉中的许多方法都是基于统计学、最优化或几何学的。最后，该领域的很大一部分致力于计算机视觉的实现方面；如何在软件和硬件的各种组合中实现现有方法，或者如何修改这些方法以获得处理速度而不损失太多性能。计算机视觉也用于时尚电子商务、库存管理、专利搜索、家具和美容行业。 </p> \n<h3>区别</h3> \n<p>与计算机视觉最密切相关的领域是图像处理、图像分析和机器视觉。这些覆盖的技术和应用范围有很大的重叠。这意味着在这些领域中使用和开发的基本技术是相似的，这可以解释为只有一个不同名称的领域。另一方面，研究团体、科学期刊、会议和公司似乎有必要将自己展示或推销为属于这些领域中的一个，因此，已经展示了将每个领域与其他领域区分开来的各种特征。 </p>\n<p>计算机图形学从3D模型中产生图像数据，计算机视觉经常从图像数据中产生3D模型。<sup><a href=\"#quote_20\" class=\"kx_ref\">[20]</a></sup>还有一种趋势是将这两个学科结合起来，例如在增强现实中探索的那样。 </p>\n<p>以下特征似乎相关，但不应被视为普遍接受： </p> \n<ul>\n <li>图像处理和图像分析倾向于关注2D图像，如何将一幅图像转换成另一幅图像，例如通过像素操作（如对比度增强）、局部操作，（如边缘提取或噪声去除），或者几何转换，（如旋转图像）。这种表征意味着图像处理/分析既不需要假设，也不产生关于图像内容的解释。</li> \n <li>计算机视觉包括来自2D图像的3D分析。这分析投影到一个或几个图像上的3D场景，例如，如何从一个或几个图像重建关于3D场景的结构或其他信息。计算机视觉通常依赖于对图像中描绘的场景或多或少复杂的假设。</li> \n <li>机器视觉是在工业应用中应用一系列技术和方法来提供基于图像的自动检查、过程控制和机器人导航的过程。<sup><a href=\"#quote_21\" class=\"kx_ref\">[21]</a></sup>机器视觉往往侧重于应用，<sup><a href=\"#quote_19\" class=\"kx_ref\">[19]</a></sup>主要是在制造领域，例如，基于视觉的机器人和基于视觉的检查、测量或拣选系统(如拣箱系统<sup><a href=\"#quote_22\" class=\"kx_ref\">[22]</a></sup>)。这意味着图像传感器技术和控制理论通常与图像数据的处理相结合来控制机器人，并且通过硬件和软件中的有效实现来强调实时处理。这也意味着例如照明之类的外部条件可以并且经常比一般的计算机视觉更好地在机器视觉中控制，这使得能够使用不同的算法。</li> \n <li>还有一个被称为成像的领域，它主要关注图像的产生过程，但有时也涉及图像的处理和分析。例如，医学成像包括医学应用中图像数据分析的大量工作。</li> \n <li>最后，模式识别是一个普遍使用各种方法从信号中提取信息的领域，主要基于统计方法和人工神经网络。该领域的很大一部分致力于将这些方法应用于图像数据。</li>\n</ul> \n<p>摄影测量也与计算机视觉重叠，例如立体摄影测量与计算机立体视觉。 </p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14996291335487751","title":"应用程序","versionId":"14996291301933321","lemmaId":11066,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":76036814,"name":"王鹏","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1576234485,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>应用范围从工业机器视觉系统(比如检测生产线上飞驰而过的瓶子)到研究人工智能和能够理解周围世界的计算机或机器人。计算机视觉和机器视觉领域有很大的重叠。计算机视觉涵盖了应用于许多领域的自动图像分析的核心技术。机器视觉通常指将自动图像分析与其他方法和技术相结合的过程，以在工业应用中提供自动检查和机器人引导。在许多计算机视觉应用中，计算机被预先编程以解决特定的任务，但是基于学习的方法现在变得越来越普遍。计算机视觉应用的例子包括以下系统： </p> \n<p></p><p><img alt=\"\" class=\"fileimage kx_img ed_imgfloat_right\" img_height=\"202\" img_width=\"300\" titlename=\"学习3D形状一直是计算机视觉领域的一项具有挑战性的任务。深度学习的最新进展使研究人员能够构建模型，能够从单视图或多视图深度图或轮廓无缝高效地生成和重建3D形状模型 [1]\" data-src=\"https://img02.sogoucdn.com/app/a/200698/sogou_science_16409\"> </p><p></p> \n<ul>\n <li>自动检查，例如在制造应用中；</li> \n <li>协助人类完成识别任务，例如物种识别系统；<sup><a href=\"#quote_23\" class=\"kx_ref\">[23]</a></sup></li> \n <li>控制过程，例如工业机器人；</li> \n <li>检测事件，例如用于视觉监视或人员计数；</li> \n <li>交互，例如，作为计算机-人交互设备的输入；</li> \n <li>建模对象或环境，例如医学图像分析或地形建模；</li> \n <li>导航，例如通过自主车辆或移动机器人；</li> \n <li>组织信息，例如，用于索引图像和图像序列的数据库。</li>\n</ul> \n<p></p><p><img class=\"videoimg kx_img ed_imgfloat_right\" img_height=\"169\" img_width=\"300\" titlename=\"国防高级研究计划局的视觉媒体推理概念视频\" data-src=\"https://img04.sogoucdn.com/app/a/200698/sogou_science_16410\"> </p><p></p> \n<p>最突出的应用领域之一是医学计算机视觉或医学图像处理，其特征在于从图像数据中提取信息来诊断患者。这方面的一个例子是肿瘤、动脉硬化或其他恶性变化的检测；器官尺寸、血流等的测量是另一个例子。它还通过提供新的信息来支持医学研究:例如，关于大脑结构，或者关于医疗质量。计算机视觉在医学领域的应用还包括增强人类解释的图像——例如超声波图像或x光图像——以减少噪声的影响。 </p>\n<p>计算机视觉的第二个应用领域是工业，有时也称为机器视觉，在工业中提取信息是为了支持制造过程。一个例子是质量控制，其中细节或最终产品被自动检查以发现缺陷。另一个例子是测量机器人手臂要拾取的细节的位置和方向。机器视觉也广泛应用于农业过程中，从散装材料中去除不需要的食物，这一过程称为光学分选。<sup><a href=\"#quote_24\" class=\"kx_ref\">[24]</a></sup> </p>\n<p>军事应用可能是计算机视觉最大的领域之一。显而易见的例子是敌兵或车辆的探测和导弹制导。更先进的导弹制导系统将导弹发送到一个区域，而不是一个特定的目标，当导弹到达该区域时，根据本地获取的图像数据进行目标选择。现代军事概念，如“战场感知”，意味着包括图像传感器在内的各种传感器提供了一组丰富的战斗场景信息，可用于支持战略决策。在这种情况下，数据的自动处理用于降低复杂性，并融合来自多个传感器的信息，以提高可靠性。 </p> \n<p></p><p><img alt=\"\" class=\"fileimage kx_img ed_imgfloat_right\" img_height=\"160\" img_width=\"200\" titlename=\"艺术家对火星探测车的概念，无人驾驶陆基交通工具的一个例子。请注意安装在探测车顶部的立体摄像机。\" data-src=\"https://img02.sogoucdn.com/app/a/200698/sogou_science_16411\"> </p><p></p> \n<p>较新的应用领域之一是自主飞行器，包括潜水器、陆基飞行器(带轮子、汽车或卡车的小型机器人)、飞行器和无人驾驶飞行器(UAV)。自主程度从完全自主(无人)车辆到基于计算机视觉的系统在各种情况下支持驾驶员或飞行员的车辆不等。全自动车辆通常使用计算机视觉进行导航，例如知道它在哪里，或者制作环境地图和探测障碍物。它还可以用于检测特定任务特定事件，例如无人机寻找森林火灾。支持系统的例子有汽车中的障碍物警告系统和飞机自主着陆系统。几家汽车制造商已经展示了汽车自动驾驶系统，但是这项技术还没有达到可以投放市场的水平。有很多军用自主飞行器的例子，从先进的导弹到用于侦察任务或导弹制导的无人机。太空探索已经在使用计算机视觉的自主飞行器进行，例如美国宇航局的火星探索漫游者和欧空局的外火星漫游者。 </p>\n<p>其他应用领域包括： </p> \n<ul>\n <li>持电影和广播的视觉效果创建，例如摄像机跟踪(匹配移动)。</li> \n <li>监控。</li> \n <li>生物科学中的生物跟踪和计数<sup><a href=\"#quote_25\" class=\"kx_ref\">[25]</a></sup></li>\n</ul>","pics":[{"originalUrl":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16409?w=300&h=202&titlename=%E5%AD%A6%E4%B9%A03D%E5%BD%A2%E7%8A%B6%E4%B8%80%E7%9B%B4%E6%98%AF%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%80%E9%A1%B9%E5%85%B7%E6%9C%89%E6%8C%91%E6%88%98%E6%80%A7%E7%9A%84%E4%BB%BB%E5%8A%A1%E3%80%82%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%E4%BD%BF%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98%E8%83%BD%E5%A4%9F%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%83%BD%E5%A4%9F%E4%BB%8E%E5%8D%95%E8%A7%86%E5%9B%BE%E6%88%96%E5%A4%9A%E8%A7%86%E5%9B%BE%E6%B7%B1%E5%BA%A6%E5%9B%BE%E6%88%96%E8%BD%AE%E5%BB%93%E6%97%A0%E7%BC%9D%E9%AB%98%E6%95%88%E5%9C%B0%E7%94%9F%E6%88%90%E5%92%8C%E9%87%8D%E5%BB%BA3D%E5%BD%A2%E7%8A%B6%E6%A8%A1%E5%9E%8B%20%5B1%5D","url":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16409","rw":300,"rh":202,"title":"学习3D形状一直是计算机视觉领域的一项具有挑战性的任务。深度学习的最新进展使研究人员能够构建模型，能够从单视图或多视图深度图或轮廓无缝高效地生成和重建3D形状模型 [1]","alt":null,"width":0,"height":0},{"originalUrl":"https://web.archive.org/web/20221025122739/https://img04.sogoucdn.com/app/a/200698/sogou_science_16410?w=300&h=169&titlename=%E5%9B%BD%E9%98%B2%E9%AB%98%E7%BA%A7%E7%A0%94%E7%A9%B6%E8%AE%A1%E5%88%92%E5%B1%80%E7%9A%84%E8%A7%86%E8%A7%89%E5%AA%92%E4%BD%93%E6%8E%A8%E7%90%86%E6%A6%82%E5%BF%B5%E8%A7%86%E9%A2%91","url":"https://web.archive.org/web/20221025122739/https://img04.sogoucdn.com/app/a/200698/sogou_science_16410","rw":300,"rh":169,"title":"国防高级研究计划局的视觉媒体推理概念视频","alt":null,"width":0,"height":0},{"originalUrl":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16411?w=200&h=160&titlename=%E8%89%BA%E6%9C%AF%E5%AE%B6%E5%AF%B9%E7%81%AB%E6%98%9F%E6%8E%A2%E6%B5%8B%E8%BD%A6%E7%9A%84%E6%A6%82%E5%BF%B5%EF%BC%8C%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6%E9%99%86%E5%9F%BA%E4%BA%A4%E9%80%9A%E5%B7%A5%E5%85%B7%E7%9A%84%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E3%80%82%E8%AF%B7%E6%B3%A8%E6%84%8F%E5%AE%89%E8%A3%85%E5%9C%A8%E6%8E%A2%E6%B5%8B%E8%BD%A6%E9%A1%B6%E9%83%A8%E7%9A%84%E7%AB%8B%E4%BD%93%E6%91%84%E5%83%8F%E6%9C%BA%E3%80%82","url":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16411","rw":200,"rh":160,"title":"艺术家对火星探测车的概念，无人驾驶陆基交通工具的一个例子。请注意安装在探测车顶部的立体摄像机。","alt":null,"width":0,"height":0}],"card":null,"references":[],"versionCount":0},{"paragraphId":"14996291335487752","title":"典型任务","versionId":"14996291301933322","lemmaId":11066,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":76036814,"name":"王鹏","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1576234485,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>上述每个应用领域都采用一系列计算机视觉任务；或多或少定义明确的测量问题或处理问题，可以使用多种方法来解决。典型计算机视觉任务的一些例子如下。 </p>\n<p>计算机视觉任务包括获取、处理、分析和理解数字图像的方法，以及从现实世界中提取高维数据以产生数字或符号信息的方法，例如以决策的形式。<sup><sup><a href=\"#quote_4\" class=\"kx_ref\">[4]</a></sup><sup><a href=\"#quote_5\" class=\"kx_ref\">[5]</a></sup><sup><a href=\"#quote_6\" class=\"kx_ref\">[6]</a></sup><sup><a href=\"#quote_7\" class=\"kx_ref\">[7]</a></sup></sup> 在这种背景下，理解意味着将视觉图像(视网膜的输入)转化为对世界的描述，这种描述可以与其他思维过程相结合并引发适当的行动。这种图像理解可以被看作是利用借助几何、物理、统计和学习理论构建的模型从图像数据中分离符号信息。<sup><a href=\"#quote_8\" class=\"kx_ref\">[8]</a></sup> </p> \n<h3>识别</h3> \n<p>计算机视觉、图像处理和机器视觉中的经典问题是确定图像数据是否包含某些特定的对象、特征或活动。文献中描述了不同种类的识别问题: </p> \n<ul>\n <li>象识别(也称为对象分类)-可以识别一个或几个预先指定或学习的对象或对象类，通常连同它们在图像中的2D位置或场景中的3D姿态。Google Goggles和LikeThat提供了说明此功能的独立程序。</li> \n <li>标识–识别对象的单个实例。示例包括特定人的面部或指纹的识别、手写数字的识别或特定车辆的识别。</li> \n <li>检测–扫描图像数据以确定特定条件。例子包括在医学图像中检测可能的异常细胞或组织，或者在自动道路收费系统中检测车辆。基于相对简单和快速计算的检测有时用于寻找感兴趣图像数据的较小区域，这些区域可以通过计算要求更高的技术进一步分析，以产生正确的解释。</li>\n</ul> \n<p>目前，这类任务的最佳算法是基于卷积神经网络的。ImageNet大规模视觉识别挑战展示了它们的能力；这是对象分类和检测的基准，有数百万张图像和数百个对象类别。在ImageNet测试中，卷积神经网络的性能现在接近人类。<sup><a href=\"#quote_26\" class=\"kx_ref\">[26]</a></sup>最好的算法仍然与小而薄的物体斗争，比如花茎上的小蚂蚁或者手里拿着羽毛笔的人。他们也有被过滤器扭曲的图像的问题(这在现代数码相机中越来越普遍)。相比之下，这种图像很少困扰人类。然而，人类在其他问题上往往有困难。例如，他们不擅长将对象分类为细粒度的类，例如特定品种的狗或鸟，而卷积神经网络很容易处理。 </p>\n<p>存在几项基于识别的专门任务，例如： </p> \n<ul>\n <li>基于内容的图像检索——在一组较大的图像中找到所有具有特定内容的图像。可以用不同的方式来指定内容，例如根据与目标图像的相似性(给我所有与图像X相似的图像)，或者根据作为文本输入给出的高级搜索标准(给我所有包含许多房屋的图像，这些图像是在冬天拍摄的，并且其中没有汽车)。</li>\n</ul> \n<p></p><p><img alt=\"\" class=\"fileimage kx_img ed_imgfloat_right\" img_height=\"139\" img_width=\"300\" titlename=\"在公共场所、商场、购物中心，人们使用计算机视觉可以达到相反的目的\" data-src=\"https://img02.sogoucdn.com/app/a/200698/sogou_science_16412\"> </p><p></p> \n<ul>\n <li>姿势估计–估计特定对象相对于相机的位置或方向。这种技术的一个示例应用是在装配线情况下帮助机械臂从传送带上取回物体或者从箱子中拾取零件。</li> \n <li>光学字符识别(OCR)-识别打印或手写文本图像中的字符，通常是为了以更易于编辑或索引(如ASCII)的格式对文本进行编码。</li> \n <li>2D码读取–读取2D码，如数据矩阵和二维码。</li> \n <li>面部识别</li> \n <li>人体计数器系统中的形状识别技术(SRT)，用于区分人(头部和肩部模式)和物体</li>\n</ul> \n<h3>动作分析</h3> \n<p>几个任务涉及运动估计，其中处理图像序列以估计图像中或3D场景中的每个点的速度，或者甚至产生图像的照相机的速度。这类任务的例子有： </p> \n<ul>\n <li>自我运动——根据摄像机产生的图像序列确定摄像机的3D刚性运动(旋转和平移)。</li> \n <li>跟踪–跟踪图像序列中(通常)一组较小的兴趣点或对象(例如，车辆、人类或其他生物<sup><a href=\"#quote_25\" class=\"kx_ref\">[25]</a></sup>)的运动。</li> \n <li>光流——为图像中的每个点确定该点相对于图像平面的运动方式，即其表观运动。该运动是相应的3D点如何在场景中移动以及相机如何相对于场景移动的结果。</li>\n</ul> \n<h3>场景重建</h3> \n<p>给定场景或视频的一个或(通常)多个图像，场景重建旨在计算场景的3D模型。在最简单的情况下，模型可以是一组3D点。更复杂的方法产生一个完整的3D表面模型。不需要运动或扫描的3D成像以及相关处理算法的出现使得该领域能够快速发展。基于网格的3D传感可用于从多个角度获取3D图像。现在可以使用算法将多个3D图像拼接成点云和3D模型。<sup><a href=\"#quote_20\" class=\"kx_ref\">[20]</a></sup> </p> \n<h3>图像恢复</h3> \n<p>图像恢复的目的是从图像中去除噪声(传感器噪声、运动模糊等)。消除噪声的最简单方法是各种类型的滤波器，如低通滤波器或中值滤波器。更复杂的方法是假设一个局部图像结构的模型，将它们与噪声区分开。通过首先根据局部图像结构(例如线或边)分析图像数据，然后基于来自分析步骤的局部信息控制滤波，与更简单的方法相比，通常获得更好的噪声去除水平。 </p>\n<p>这个领域的一个例子是修复。 </p>","pics":[{"originalUrl":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16412?w=300&h=139&titlename=%E5%9C%A8%E5%85%AC%E5%85%B1%E5%9C%BA%E6%89%80%E3%80%81%E5%95%86%E5%9C%BA%E3%80%81%E8%B4%AD%E7%89%A9%E4%B8%AD%E5%BF%83%EF%BC%8C%E4%BA%BA%E4%BB%AC%E4%BD%BF%E7%94%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%8F%AF%E4%BB%A5%E8%BE%BE%E5%88%B0%E7%9B%B8%E5%8F%8D%E7%9A%84%E7%9B%AE%E7%9A%84","url":"https://web.archive.org/web/20221025122739/https://img02.sogoucdn.com/app/a/200698/sogou_science_16412","rw":300,"rh":139,"title":"在公共场所、商场、购物中心，人们使用计算机视觉可以达到相反的目的","alt":null,"width":0,"height":0}],"card":null,"references":[],"versionCount":0},{"paragraphId":"14996291335487753","title":"系统方法","versionId":"14996291301933323","lemmaId":11066,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":76036814,"name":"王鹏","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1576234485,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>计算机视觉系统的组织高度依赖于应用。一些系统是解决特定测量或检测问题的独立应用程序，而另一些系统构成更大设计的子系统，例如，该子系统还包含用于控制机械执行器、规划、信息数据库、人机界面等的子系统。计算机视觉系统的具体实现还取决于它的功能是否是预先指定的，或者它的某些部分是否可以在操作过程中学习或修改。许多功能对于应用程序来说是唯一的。然而，在许多计算机视觉系统中都有典型的功能。 </p> \n<ul>\n <li>图像采集——数字图像是由一个或几个图像传感器产生的，除了各种类型的光敏摄像机，还包括距离传感器、断层摄影设备、雷达、超声波摄像机等。根据传感器的类型，得到的图像数据是普通的2D图像、3D体积或图像序列。像素值通常对应于一个或几个光谱带(灰度图像或彩色图像)中的光强度，但是也可以与各种物理测量相关，例如声波或电磁波的深度、吸收或反射，或者核磁共振。<sup><a href=\"#quote_24\" class=\"kx_ref\">[24]</a></sup></li> \n <li>预处理——在将计算机视觉方法应用于图像数据以提取某些特定信息之前，通常需要对数据进行处理，以确保其满足该方法隐含的某些假设。例子有： \n  <ul>\n   <li>重新采样以确保图像坐标系正确。</li> \n   <li>降噪以确保传感器噪声不会引入错误信息。</li> \n   <li>增强对比度，以确保相关信息能够被检测到。</li> \n   <li>缩放空间表示以在局部适当比例下增强图像结构。</li>\n  </ul></li> \n <li>特征提取——从图像数据中提取不同复杂程度的图像特征。<sup><a href=\"#quote_24\" class=\"kx_ref\">[24]</a></sup> 这种特征的典型例子有： \n  <ul>\n   <li>线条、边缘和脊。</li> \n   <li>局部兴趣点，如角、斑点或点。</li>\n  </ul></li>\n</ul> \n<dl>\n <dd>\n  更复杂的特征可能与纹理、形状或运动有关。\n </dd>\n</dl> \n<ul>\n <li>检测/分割–在处理过程中的某个时刻，决定图像的哪些图像点或区域与进一步处理相关。<sup><a href=\"#quote_24\" class=\"kx_ref\">[24]</a></sup> 例子有： \n  <ul>\n   <li>选择一组特定的兴趣点。</li> \n   <li>包含特定感兴趣对象的一个或多个图像区域的分割。</li> \n   <li>将图像分割成嵌套场景结构，包括前景、对象组、单个对象或显著对象部分(也称为空间分类场景层次结构)，<sup><a href=\"#quote_27\" class=\"kx_ref\">[27]</a></sup>而视觉显著性通常被实现为空间和时间注意。</li> \n   <li>将一个或多个视频分割或共同分割成一系列每帧前景遮罩，同时保持其时间语义连续性。<sup><a href=\"#quote_28\" class=\"kx_ref\">[28]</a></sup><sup><a href=\"#quote_29\" class=\"kx_ref\">[29]</a></sup></li>\n  </ul></li> \n <li>高级处理——在这一步骤中，输入通常是一小组数据，例如一组点或一个图像区域，假设它包含一个特定的对象。<sup><a href=\"#quote_24\" class=\"kx_ref\">[24]</a></sup>其余的处理涉及，例如： \n  <ul>\n   <li>验证数据是否满足基于模型和特定应用的假设。</li> \n   <li>特定于应用程序的参数评估，如对象姿态或对象大小。</li> \n   <li>图像识别–将检测到的对象分为不同的类别。</li> \n   <li>图像配准–比较和组合同一对象的两个不同视图。</li>\n  </ul></li> \n <li>决策制定应用程序所需的最终决策，<sup><a href=\"#quote_24\" class=\"kx_ref\">[24]</a></sup> 例如： \n  <ul>\n   <li>自动检查应用的通过/失败。</li> \n   <li>识别应用中的匹配/不匹配。</li> \n   <li>医学、军事、安全和识别应用中的进一步人类审查标志。</li>\n  </ul></li>\n</ul> \n<h3>图像理解系统</h3> \n<p>图像理解系统(IUS)包括如下三个抽象层次:低层次包括图像图元，如边缘、纹理元素或区域；中间层包括边界、表面和体积；高层次包括对象、场景或事件。这些要求中的许多都是需要进一步研究的课题。 </p>\n<p>IUS设计中对这些层次的表征要求是:原型概念的表征、概念组织、空间知识、时间知识、标度以及通过比较和区分的描述。 </p>\n<p>推理指的是从当前已知的事实中导出新的、未明确表示的事实的过程，而控制指的是选择在处理的特定阶段应该应用多种推理、搜索和匹配技术中的哪一种的过程。IUS的推理和控制要求是:搜索和假设激活、匹配和假设检验、期望的产生和使用、注意力的改变和集中、信念的确定性和强度、推理和目标满意度。<sup><a href=\"#quote_30\" class=\"kx_ref\">[30]</a></sup> </p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14996291335487754","title":"硬件","versionId":"14996291301933324","lemmaId":11066,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":76036814,"name":"王鹏","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1576234485,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>计算机视觉系统有很多种；然而，它们都包含这些基本元素:电源、至少一个图像采集设备(照相机、ccd等)。)、处理器以及控制和通信电缆或某种无线互连机制。此外，一个实用的视觉系统包含软件，以及一个显示器以便监控系统。与大多数工业空间一样，内部空间的视觉系统包含照明系统，并可置于受控环境中。此外，完整的系统包括许多附件，如摄像机支架、电缆和连接器。 </p>\n<p>大多数计算机视觉系统使用可见光相机以每秒最多60帧(通常慢得多)的帧速率被动地观看场景。 </p>\n<p>一些计算机视觉系统使用具有主动照明的图像采集硬件或除可见光以外的其他东西或两者兼有，例如结构光3D扫描仪、热像照相机、高光谱成像仪、雷达成像、激光雷达扫描仪、磁共振图像、侧扫声纳、合成孔径声纳等。这种硬件捕捉“图像”，然后通常使用与处理可见光图像相同的计算机视觉算法进行处理。 </p>\n<p>虽然传统的广播和消费视频系统以每秒30帧的速度运行，但是数字信号处理和消费图形硬件的进步使得实时系统能够以每秒数百至数千帧的速度进行高速图像采集、处理和显示。对于机器人应用来说，快速、实时的视频系统至关重要，通常可以简化某些算法所需的处理。当与高速投影仪结合时，快速图像采集允许实现3D测量和特征跟踪。<sup><a href=\"#quote_31\" class=\"kx_ref\">[31]</a></sup> </p>\n<p>以自我为中心的视觉系统由一台可穿戴式的照相机组成，它可以从第一人称的角度自动拍照。 </p>\n<p>截至2016年，视觉处理单元正在成为一种新型处理器，以补充该角色中的CPU和图形处理单元(GPU)。<sup><a href=\"#quote_32\" class=\"kx_ref\">[32]</a></sup> </p> \n<h3>列表</h3> \n<ul>\n <li>计算机视觉主题列表</li> \n <li>新兴技术列表</li> \n <li>人工智能概述</li>\n</ul>","pics":null,"card":null,"references":[],"versionCount":0}],"references":[{"id":1,"type":"book","title":"Dana H. Ballard; Christopher M. Brown (1982). Computer Vision. Prentice Hall. ISBN 978-0-13-165316-0.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":2,"type":"book","title":"Huang, T. (1996-11-19). Vandoni, Carlo, E, ed. Computer Vision : Evolution And Promise (PDF). 19th CERN School of Computing. Geneva: CERN. pp. 21–25. doi:10.5170/CERN-1996-008.21. ISBN 978-9290830955.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":3,"type":"book","title":"Milan Sonka; Vaclav Hlavac; Roger Boyle (2008). Image Processing, Analysis, and Machine Vision. Thomson. ISBN 978-0-495-08252-1.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":4,"type":"book","title":"Reinhard Klette (2014). Concise Computer Vision. Springer. ISBN 978-1-4471-6320-6.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":5,"type":"book","title":"Linda G. Shapiro; George C. Stockman (2001). Computer Vision. Prentice Hall. ISBN 978-0-13-030796-5.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":6,"type":"book","title":"Tim Morris (2004). Computer Vision and Image Processing. Palgrave Macmillan. ISBN 978-0-333-99451-1.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":7,"type":"book","title":"Bernd Jähne; Horst Haußecker (2000). Computer Vision and Applications, A Guide for Students and Practitioners. Academic Press. ISBN 978-0-13-085198-7.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":8,"type":"book","title":"David A. Forsyth; Jean Ponce (2003). Computer Vision, A Modern Approach. Prentice Hall. ISBN 978-0-13-085198-7.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":9,"type":"book","title":"https://web.archive.org/web/20221025122739/http://www.bmva.org/visionoverview The British Machine Vision Association and Society for Pattern Recognition Retrieved February 20, 2017","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":10,"type":"book","title":"Murphy, Mike. \"Star Trek's \"tricorder\" medical scanner just got closer to becoming a reality\".","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":11,"type":"book","title":"Richard Szeliski (30 September 2010). Computer Vision: Algorithms and Applications. Springer Science & Business Media. pp. 10–16. ISBN 978-1-84882-935-0.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":12,"type":"book","title":"Papert, Seymour (1966-07-01). \"The Summer Vision Project\". MIT AI Memos (1959 - 2004). hdl:1721.1/6125.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":13,"type":"book","title":"Margaret Ann Boden (2006). Mind as Machine: A History of Cognitive Science. Clarendon Press. p. 781. ISBN 978-0-19-954316-8.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":14,"type":"book","title":"Takeo Kanade (6 December 2012). Three-Dimensional Machine Vision. Springer Science & Business Media. ISBN 978-1-4613-1981-8.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":15,"type":"book","title":"Nicu Sebe; Ira Cohen; Ashutosh Garg; Thomas S. Huang (3 June 2005). Machine Learning in Computer Vision. Springer Science & Business Media. ISBN 978-1-4020-3274-5.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":16,"type":"book","title":"William Freeman; Pietro Perona; Bernhard Scholkopf (2008). \"Guest Editorial: Machine Learning for Computer Vision\". International Journal of Computer Vision. 77 (1): 1. doi:10.1007/s11263-008-0127-7. ISSN 1573-1405.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":17,"type":"book","title":"\"Information Engineering | Department of Engineering\". www.eng.cam.ac.uk (in 英语). Retrieved 2018-10-03.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":18,"type":"book","title":"\"Information Engineering Main/Home Page\". www.robots.ox.ac.uk (in 英语). Retrieved 2018-10-03.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":19,"type":"book","title":"Steger, Carsten; Markus Ulrich; Christian Wiedemann (2018). Machine Vision Algorithms and Applications (2nd ed.). Weinheim: Wiley-VCH. p. 1. ISBN 978-3-527-41365-2. Retrieved 2018-01-30.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":20,"type":"book","title":"\"Soltani, A. A., Huang, H., Wu, J., Kulkarni, T. D., & Tenenbaum, J. B. Synthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes With Deep Generative Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1511-1519)\". 2019-01-25.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":21,"type":"book","title":"Turek, Fred (June 2011). \"Machine Vision Fundamentals, How to Make Robots See\". NASA Tech Briefs Magazine. 35 (6). pages 60–62","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":22,"type":"book","title":"https://web.archive.org/web/20221025122739/https://www.robots.com/blog/viewing/the-future-of-automated-random-bin-picking","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":23,"type":"book","title":"Wäldchen, Jana; Mäder, Patrick (2017-01-07). \"Plant Species Identification Using Computer Vision Techniques: A Systematic Literature Review\". Archives of Computational Methods in Engineering (in 英语). 25 (2): 507–543. doi:10.1007/s11831-016-9206-z. ISSN 1134-3060.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":24,"type":"book","title":"E. Roy Davies (2005). Machine Vision: Theory, Algorithms, Practicalities. Morgan Kaufmann. ISBN 978-0-12-206093-9.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":25,"type":"book","title":"Bruijning, Marjolein; Visser, Marco D.; Hallmann, Caspar A.; Jongejans, Eelke; Golding, Nick (2018). \"trackdem: Automated particle tracking to obtain population counts and size distributions from videos in r\". Methods in Ecology and Evolution. 9 (4): 965–973. doi:10.1111/2041-210X.12975. ISSN 2041-210X.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":26,"type":"book","title":"O. Russakovsky et al., \"ImageNet Large Scale Visual Recognition Challenge\", 2014.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":27,"type":"book","title":"Barghout, Lauren. \"Visual Taxometric Approach to Image Segmentation Using Fuzzy-Spatial Taxon Cut Yields Contextually Relevant Regions.\" Information Processing and Management of Uncertainty in Knowledge-Based Systems. Springer International Publishing, 2014.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":28,"type":"book","title":"Liu, Ziyi; Wang, Le; Hua, Gang; Zhang, Qilin; Niu, Zhenxing; Wu, Ying; Zheng, Nanning (2018). \"Joint Video Object Discovery and Segmentation by Coupled Dynamic Markov Networks\" (PDF). IEEE Transactions on Image Processing. 27 (12): 5840–5853. doi:10.1109/tip.2018.2859622. ISSN 1057-7149. PMID 30059300.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":29,"type":"book","title":"Wang, Le; Duan, Xuhuan; Zhang, Qilin; Niu, Zhenxing; Hua, Gang; Zheng, Nanning (2018-05-22). \"Segment-Tube: Spatio-Temporal Action Localization in Untrimmed Videos with Per-Frame Segmentation\" (PDF). Sensors. 18 (5): 1657. doi:10.3390/s18051657. ISSN 1424-8220. PMC 5982167. PMID 29789447.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":30,"type":"book","title":"Shapiro, Stuart C. (1992). Encyclopedia of Artificial Intelligence, Volume 1. New York: John WIley & Sons, Inc. pp. 643–646. ISBN 978-0-471-50306-4.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":31,"type":"book","title":"Kagami, Shingo (2010). High-speed vision systems and projectors for real-time perception of the world. IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops. 2010. pp. 100–107. doi:10.1109/CVPRW.2010.5543776. ISBN 978-1-4244-7029-7. Retrieved 2 May 2016.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":32,"type":"book","title":"Seth Colaner (January 3, 2016). \"A Third Type Of Processor For VR/AR: Movidius' Myriad 2 VPU\". www.tomshardware.com.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false}],"recommendReferences":null,"auditState":2,"lemmaLevel":1,"origin":0,"originEnTitle":null,"originZhTitle":null,"pv":11063,"auditType":0,"synonyms":["Computer Vision"],"showEditTime":"2019.12.13 18:54","auditors":[{"uid":0,"name":"Ki.κe","pic":"https://web.archive.org/web/20221025122739/https://wx.qlogo.cn/mmopen/vi_32/y67kfr32Doib4wg71Jiau7jVWvharic3nRKgdRRQSl6koeQJCo0GQs2Krw0vwdFRsOWnHIQOwAZsSg5lIkIrFCOcQ/132","introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":"","jobBrief":"","role":0,"roleName":null,"title":"","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false}],"hasZhishiNav":false,"auditInfos":{},"isHistory":false};</script><script crossorigin="anonymous" src="./1066.计算机视觉 - 搜狗科学百科_files/aegis.min.js.download"></script><script crossorigin="anonymous" src="./1066.计算机视觉 - 搜狗科学百科_files/main_2020092401.js.download"></script><script crossorigin="anonymous" src="./1066.计算机视觉 - 搜狗科学百科_files/react.production.min.js.download"></script><script crossorigin="anonymous" src="./1066.计算机视觉 - 搜狗科学百科_files/react-dom.production.min.js.download"></script><script crossorigin="anonymous" src="./1066.计算机视觉 - 搜狗科学百科_files/jquery-1.11.1.min.js.download"></script><script crossorigin="anonymous" src="./1066.计算机视觉 - 搜狗科学百科_files/main_2022062701.js.download"></script><script crossorigin="anonymous" src="./1066.计算机视觉 - 搜狗科学百科_files/main_66bbe21.js.download"></script><script crossorigin="anonymous" src="./1066.计算机视觉 - 搜狗科学百科_files/react.production.min.js.download"></script><script crossorigin="anonymous" src="./1066.计算机视觉 - 搜狗科学百科_files/react-dom.production.min.js.download"></script><script crossorigin="anonymous" src="./1066.计算机视觉 - 搜狗科学百科_files/main_edf0f08.js.download"></script>
</body></html>