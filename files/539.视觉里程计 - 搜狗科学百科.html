<!DOCTYPE html>
<!-- saved from url=(0083)https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm -->
<html class="" data-reactroot=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./539.视觉里程计 - 搜狗科学百科_files/analytics.js.download" type="text/javascript"></script>
<script type="text/javascript">window.addEventListener('DOMContentLoaded',function(){var v=archive_analytics.values;v.service='wb';v.server_name='wwwb-app213.us.archive.org';v.server_ms=4379;archive_analytics.send_pageview({});});</script>
<script type="text/javascript" src="./539.视觉里程计 - 搜狗科学百科_files/bundle-playback.js.download" charset="utf-8"></script>
<script type="text/javascript" src="./539.视觉里程计 - 搜狗科学百科_files/wombat.js.download" charset="utf-8"></script>
<script type="text/javascript">
  __wm.init("https://web.archive.org/web");
  __wm.wombat("https://baike.sogou.com/kexue/d10539.htm","20221025114450","https://web.archive.org/","web","/_static/",
	      "1666698290");
</script>
<link rel="stylesheet" type="text/css" href="./539.视觉里程计 - 搜狗科学百科_files/banner-styles.css">
<link rel="stylesheet" type="text/css" href="./539.视觉里程计 - 搜狗科学百科_files/iconochive.css">
<!-- End Wayback Rewrite JS Include -->
<meta name="save" content="history"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="VWGb6TyYx8"><meta content="视觉里程计 - 搜狗科学百科" name="keywords"><meta content="搜狗科学百科是一部有着平等、协作、分享、自由理念的网络科学全书，为每一个互联网用户创造一个涵盖所有领域知识、服务的中文知识性平台。" name="description"><meta http-equiv="x-dns-prefetch-control" content="on"><meta name="server" baike="235" ip="210" env="online"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://cache.soso.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://hhy.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://pic.baike.soso.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://ugc.qpic.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://xui.ptlogin2.qq.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://q1.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://q2.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://q3.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://q4.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://q.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://img01.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://img02.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://img03.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221025114450/https://img04.sogoucdn.com/"><link rel="Shortcut Icon" href="https://web.archive.org/web/20221025114450im_/https://www.sogou.com/images/logo/new/favicon.ico?v=4"><link rel="Bookmark" href="https://www.sogou.com/images/logo/new/favicon.ico?v=4"><link href="./539.视觉里程计 - 搜狗科学百科_files/base_b849887.css" rel="stylesheet"><link href="./539.视觉里程计 - 搜狗科学百科_files/detail_378aed5.css" rel="stylesheet"><link href="./539.视觉里程计 - 搜狗科学百科_files/inviteAudit_7894507.css" rel="stylesheet"><link rel="stylesheet" href="./539.视觉里程计 - 搜狗科学百科_files/highlight.min.css"><title>视觉里程计 - 搜狗科学百科</title><style>.onekey-close {
	position: absolute;
	top: 16px;
	right: 16px;
	width: 24px;
	height: 24px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	text-indent: -999em;
	background-size: 84px;
	background-position: -63px 0;
}

.onekey-login {
	position: absolute;
	top: 16.4%;
	left: 0;
	right: 0;
	width: 100%;
}

/* .onekey-login-img {
    width: 75px;
    height: 75px;
    background: url("https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/images/sprite_wap_baike.png") no-repeat;
    background-size: 100px 91px;
    background-position: 0 0;
    background-repeat: no-repeat;
    margin: 0 auto;
} */

.onekey-login-title {
	text-align: center;
	padding-bottom: 3px;
	font-size: 21px;
	font-weight: bold;
	line-height: 30px;
	color: #000;
}

.onekey-login-txt {
	text-align: center;
	font-family: PingFangSC;
	font-size: 14px;
	line-height: 20px;
	color: #8f8f8f;
}

.onekey-login-qq,
.onekey-login-wx,
.onekey-login-phone {
	display: block;
	width: 245px;
	height: 54px;
	border-radius: 45px;
	text-align: center;

	margin: 0 auto;
	font-size: 17px;
	line-height: 24px;
	color: #000;
	/* padding: 16px 77px; */
	border-radius: 12px;
	border: solid 1px #e0e0e0;
}
.onekey-qq-content,
.onekey-vx-content,
.onekey-phone-content {
	display: inline-block;
	margin-top: 16px;
}
.onekey-qq-content {
	padding: 0 5px;
}

.onekey-login-qq {
	margin-top: 48px;
	margin-bottom: 24px;
}

.onekey-login-qq:before {
	display: inline-block;
	content: "";
	width: 20px;
	height: 20px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 80px;
	background-position: -20px 0;
	vertical-align: top;
	margin: 17px 8px 0 0;
}

.onekey-login-wx {
	margin-bottom: 24px;
}

.onekey-login-wx:before {
	display: inline-block;
	content: "";
	width: 21px;
	height: 21px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 84px;
	background-position: 0 0;
	vertical-align: top;
	margin: 17px 10px 0 0;
}

.onekey-login-phone {
}

.onekey-login-phone:before {
	display: inline-block;
	content: "";
	width: 21px;
	height: 21px;
	background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 84px;
	background-position: -42px 0;
	vertical-align: top;
	margin: 17px 10px 0 0;
}

.onekey-fixed {
	z-index: 100;
	position: fixed;
	top: 0;
	bottom: 0;
	left: 0;
	right: 0;
	background: #fff;
	width: 100%;
	height: 100%;
}

.onekey-fixed.forbid {
	z-index: 100;
	position: fixed;
	top: auto;
	bottom: 68px;
	left: 9%;
	right: 0;
	background: rgba(0, 0, 0, 0.7);
	width: 82%;
	height: 43px;
	border-radius: 25px;
	color: #ffffff;
}
.onekey-login-title.forbid {
	text-align: center;
	padding-bottom: 3px;
	font-size: 14px;
	font-weight: normal;
	line-height: 30px;
	color: white;
}
</style><style>#login_mask {
  background: #000;
  opacity: 0.5;
  filter: alpha(opacity=50);
  position: fixed;
  /*fixed好像在哪个IE上有BUG，先用用*/
  left: 0;
  top: 0;
  z-index: 999;
  height: 100%;
}

#login_iframe_container {
  position: fixed;
  width: 550px;
  height: 360px;
  z-index: 1020;
  background-color: #ffffff;
}

@media screen and (max-width: 828px) {
  #login_iframe_container {
    top: 50% !important;
    left: 50% !important;
    transform: translate(-50%, -50%);
  }
}

#login_iframe_container.new-login {
  width: 550px;
  height: 360px;
  background-image: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/background_2a4a8a6.png);
}

#login_iframe_container.new-login.no-bg {
  background: #fff;
}

#login_iframe_container.new-login .login-title {
  width: 100%;
  height: 42px;
  line-height: 42px;
  text-align: center;
  font-size: 30px;
  letter-spacing: 0.19px;
  color: #ffffff;
  margin-top: 62px;
}
#login_iframe_container.new-login .forbid-title {
  width: 100%;
  height: 42px;
  line-height: 42px;
  text-align: center;
  font-size: 24px;
  letter-spacing: 0.19px;
  color: #333333;
  margin-top: 150px;
}

#login_iframe_container.new-login.no-bg .login-title {
  color: #333333;
}

#login_iframe_container.new-login .login-subtitle {
  width: 100%;
  height: 18px;
  line-height: 18px;
  font-size: 13px;
  letter-spacing: 0.08px;
  color: #ffffff;
  text-align: center;
  margin-top: 9px;
  margin-bottom: 43px;
}

#login_iframe_container.new-login.no-bg .login-subtitle {
  color: #999999;
}

#login_iframe_container.new-login .login-subtitle::before {
  content: '';
  display: inline-block;
  width: 10px;
  height: 1px;
  background-color: #ffffff;
  position: relative;
  top: -4px;
  left: -5px;
}

#login_iframe_container.new-login .login-subtitle::after {
  content: '';
  display: inline-block;
  width: 10px;
  height: 1px;
  background-color: #ffffff;
  position: relative;
  top: -4px;
  left: 5px;
}

#login_iframe_container.new-login.no-bg .login-subtitle::before {
  background-color: #999999;
}

#login_iframe_container.new-login.no-bg .login-subtitle::after {
  background-color: #999999;
}

#login_iframe_container.new-login .close-btn {
  position: absolute;
  top: 20px;
  right: 20px;
  width: 12px;
  height: 12px;
  background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -59px -10px;
  background-size: 81px 91px;
  cursor: pointer;
}

#login_iframe_container.new-login .login-btn {
  width: 220px;
  height: 47px;
  border-radius: 24px;
  border: solid 1px #dddddd;
  background-color: #ffffff;
  margin: 0 auto;
  margin-top: 28px;
  position: relative;
  display: block;
}

#login_iframe_container.new-login .login-btn .login-icon {
  position: absolute;
}

#login_iframe_container.new-login .login-btn .login-text {
  width: 61px;
  height: 47px;
  line-height: 47px;
  vertical-align: middle;
  font-size: 15px;
  letter-spacing: 0.1px;
  color: #666666;
  position: absolute;
  right: 62px;
}

#login_iframe_container.new-login .login-btn.qq-btn .login-icon {
  width: 22px;
  height: 27px;
  top: 10px;
  left: 67px;
  background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -10px -54px;
  background-size: 81px 91px;
}

#login_iframe_container.new-login .login-btn.qq-btn .login-text {
  right: 59px;
}

#login_iframe_container.new-login .login-btn.wechat-btn .login-icon {
  width: 29px;
  height: 24px;
  top: 12px;
  left: 62px;
  background: url(//web.archive.org/web/20221025113757/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -10px -10px;
  background-size: 81px 91px;
}</style><style>/* -- container -- */
.rodal,
.rodal-mask {
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 100;
}

.rodal {
    position: fixed;
}

/* -- mask -- */
.rodal-mask {
    position: fixed;
    background: rgba(0, 0, 0, .5);
}

/* -- dialog -- */
.rodal-dialog {
    position: absolute;
    z-index: 101;
    background: #fff;
    border-radius: 3px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, .2);
}

.rodal-center {
    top: 50%;
    transform: translateY(-50%);
    left: 0;
    right: 0;
    margin: 0 auto;
}

.rodal-bottom {
    left: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

.rodal-top {
    left: 0;
    right: 0;
    top: 0;
    margin: auto;
}

.rodal-left {
    top: 0;
    left: 0;
    bottom: 0;
    margin: auto;
}

.rodal-right {
    top: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

/* -- close button -- */
.rodal-close {
    position: absolute;
    cursor: pointer;
    top: 16px;
    right: 16px;
    width: 16px;
    height: 16px;
}

.rodal-close:before,
.rodal-close:after {
    position: absolute;
    content: '';
    height: 2px;
    width: 100%;
    top: 50%;
    left: 0;
    margin-top: -1px;
    background: #999;
    border-radius: 100%;
    -webkit-transition: background .2s;
    transition: background .2s;
}

.rodal-close:before {
    -webkit-transform: rotate(45deg);
    transform: rotate(45deg);
}

.rodal-close:after {
    -webkit-transform: rotate(-45deg);
    transform: rotate(-45deg);
}

.rodal-close:hover:before,
.rodal-close:hover:after {
    background: #333;
}

/* -- fade -- */
/* @-webkit-keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

@keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

.rodal-fade-enter {
    -webkit-animation: rodal-fade-enter both ease-in;
    animation: rodal-fade-enter both ease-in;
} */

@-webkit-keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

@keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

.rodal-fade-leave {
    -webkit-animation: rodal-fade-leave both ease-out;
    animation: rodal-fade-leave both ease-out;
}

/* -- zoom -- */
@-webkit-keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-enter {
    -webkit-animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-leave {
    -webkit-animation: rodal-zoom-leave both;
    animation: rodal-zoom-leave both;
}

/* -- slideDown -- */
@-webkit-keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-enter {
    -webkit-animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-leave {
    -webkit-animation: rodal-slideDown-leave both;
    animation: rodal-slideDown-leave both;
}

/* -- slideLeft -- */
@-webkit-keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-enter {
    -webkit-animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-leave {
    -webkit-animation: rodal-slideLeft-leave both;
    animation: rodal-slideLeft-leave both;
}

/* -- slideRight -- */
@-webkit-keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-enter {
    -webkit-animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-leave {
    -webkit-animation: rodal-slideRight-leave both;
    animation: rodal-slideRight-leave both;
}

/* -- slideUp -- */
@-webkit-keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-enter {
    -webkit-animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
    animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
}

@-webkit-keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-leave {
    -webkit-animation: rodal-slideUp-leave both;
    animation: rodal-slideUp-leave both;
}

/* -- flip -- */
@-webkit-keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

@keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

.rodal-flip-enter {
    -webkit-animation: rodal-flip-enter both ease-in;
    animation: rodal-flip-enter both ease-in;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

@-webkit-keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

@keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

.rodal-flip-leave {
    -webkit-animation: rodal-flip-leave both;
    animation: rodal-flip-leave both;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

/* -- rotate -- */
@-webkit-keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-enter {
    -webkit-animation: rodal-rotate-enter both;
    animation: rodal-rotate-enter both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

@-webkit-keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-leave {
    -webkit-animation: rodal-rotate-leave both;
    animation: rodal-rotate-leave both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

/* -- door -- */
@-webkit-keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

@keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

.rodal-door-enter {
    -webkit-animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

@keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

.rodal-door-leave {
    -webkit-animation: rodal-door-leave both;
    animation: rodal-door-leave both;
}</style><style>/* -- container -- */
.rodal,
.rodal-mask {
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 100;
}

.rodal {
    position: fixed;
}

/* -- mask -- */
.rodal-mask {
    position: fixed;
    background: rgba(0, 0, 0, .5);
}

/* -- dialog -- */
.rodal-dialog {
    position: absolute;
    z-index: 101;
    background: #fff;
    border-radius: 3px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, .2);
}

.rodal-center {
    top: 50%;
    transform: translateY(-50%);
    left: 0;
    right: 0;
    margin: 0 auto;
}

.rodal-bottom {
    left: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

.rodal-top {
    left: 0;
    right: 0;
    top: 0;
    margin: auto;
}

.rodal-left {
    top: 0;
    left: 0;
    bottom: 0;
    margin: auto;
}

.rodal-right {
    top: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

/* -- close button -- */
.rodal-close {
    position: absolute;
    cursor: pointer;
    top: 16px;
    right: 16px;
    width: 16px;
    height: 16px;
}

.rodal-close:before,
.rodal-close:after {
    position: absolute;
    content: '';
    height: 2px;
    width: 100%;
    top: 50%;
    left: 0;
    margin-top: -1px;
    background: #999;
    border-radius: 100%;
    -webkit-transition: background .2s;
    transition: background .2s;
}

.rodal-close:before {
    -webkit-transform: rotate(45deg);
    transform: rotate(45deg);
}

.rodal-close:after {
    -webkit-transform: rotate(-45deg);
    transform: rotate(-45deg);
}

.rodal-close:hover:before,
.rodal-close:hover:after {
    background: #333;
}

/* -- fade -- */
/* @-webkit-keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

@keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

.rodal-fade-enter {
    -webkit-animation: rodal-fade-enter both ease-in;
    animation: rodal-fade-enter both ease-in;
} */

@-webkit-keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

@keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

.rodal-fade-leave {
    -webkit-animation: rodal-fade-leave both ease-out;
    animation: rodal-fade-leave both ease-out;
}

/* -- zoom -- */
@-webkit-keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-enter {
    -webkit-animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-leave {
    -webkit-animation: rodal-zoom-leave both;
    animation: rodal-zoom-leave both;
}

/* -- slideDown -- */
@-webkit-keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-enter {
    -webkit-animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-leave {
    -webkit-animation: rodal-slideDown-leave both;
    animation: rodal-slideDown-leave both;
}

/* -- slideLeft -- */
@-webkit-keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-enter {
    -webkit-animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-leave {
    -webkit-animation: rodal-slideLeft-leave both;
    animation: rodal-slideLeft-leave both;
}

/* -- slideRight -- */
@-webkit-keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-enter {
    -webkit-animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-leave {
    -webkit-animation: rodal-slideRight-leave both;
    animation: rodal-slideRight-leave both;
}

/* -- slideUp -- */
@-webkit-keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-enter {
    -webkit-animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
    animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
}

@-webkit-keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-leave {
    -webkit-animation: rodal-slideUp-leave both;
    animation: rodal-slideUp-leave both;
}

/* -- flip -- */
@-webkit-keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

@keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

.rodal-flip-enter {
    -webkit-animation: rodal-flip-enter both ease-in;
    animation: rodal-flip-enter both ease-in;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

@-webkit-keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

@keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

.rodal-flip-leave {
    -webkit-animation: rodal-flip-leave both;
    animation: rodal-flip-leave both;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

/* -- rotate -- */
@-webkit-keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-enter {
    -webkit-animation: rodal-rotate-enter both;
    animation: rodal-rotate-enter both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

@-webkit-keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-leave {
    -webkit-animation: rodal-rotate-leave both;
    animation: rodal-rotate-leave both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

/* -- door -- */
@-webkit-keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

@keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

.rodal-door-enter {
    -webkit-animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

@keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

.rodal-door-leave {
    -webkit-animation: rodal-door-leave both;
    animation: rodal-door-leave both;
}</style>
<!--百度统计-->
<script>
   var _hmt = _hmt || [];
   (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?3c7614be3026469d5a60f41ab30b5082";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
      })();
</script>

	<!--百度统计-->
	<script>
		var _hmt = _hmt || [];
		(function() {
			var hm = document.createElement("script");
			hm.src = "https://hm.baidu.com/hm.js?3c7614be3026469d5a60f41ab30b5082";
			var s = document.getElementsByTagName("script")[0]; 
			s.parentNode.insertBefore(hm, s);
			})();
	</script>
</head>
<body class=""><!-- BEGIN WAYBACK TOOLBAR INSERT -->
<style type="text/css">
body {
  margin-top:0 !important;
  padding-top:0 !important;
  /*min-width:800px !important;*/
}
</style>
<script>__wm.rw(0);</script>
<div id="wm-ipp-base" lang="en" style="display: block; direction: ltr;">
</div><div id="wm-ipp-print">The Wayback Machine - https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm</div>
<script type="text/javascript">//<![CDATA[
__wm.bt(675,27,25,2,"web","https://baike.sogou.com/kexue/d10539.htm","20221025114450",1996,"/_static/",["/_static/css/banner-styles.css?v=S1zqJCYt","/_static/css/iconochive.css?v=qtvMKcIJ"], false);
  __wm.rw(1);
//]]></script>
<!-- END WAYBACK TOOLBAR INSERT --><script>window._gtag=window._gtag||{};window._gtag.shouldGrayed = false;if ('26ecb46b783e4d0787917c08dfe03607') window._gtag.traceId = '26ecb46b783e4d0787917c08dfe03607';if ({"illegality":true}) window.userInfo = {"illegality":true};</script><div class="topnavbox"><ul class="topnav"><li><a href="https://web.archive.org/web/20221025114450/https://www.sogou.com/web?query=">网页</a></li><li><a href="https://web.archive.org/web/20221025114450/https://weixin.sogou.com/weixin?p=75351201">微信</a></li><li><a href="https://web.archive.org/web/20221025114450/https://zhihu.sogou.com/zhihu?p=75351218">知乎</a></li><li><a href="https://web.archive.org/web/20221025114450/https://pic.sogou.com/pics?query=">图片</a></li><li><a href="https://web.archive.org/web/20221025114450/https://v.sogou.com/v?query=">视频</a></li><li><a href="https://web.archive.org/web/20221025114450/https://mingyi.sogou.com/">医疗</a></li><li class="cur"><strong>科学</strong></li><li><a href="https://web.archive.org/web/20221025114450/https://hanyu.sogou.com/">汉语</a></li><li><a href="https://web.archive.org/web/20221025114450/https://wenwen.sogou.com/">问问</a></li><li><a href="https://web.archive.org/web/20221025114450/https://www.sogou.com/docs/more.htm">更多<span class="topraquo">»</span></a></li></ul></div><div id="header"><div class="header-wrap"><a class="header-logo" href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue"></a><div class="header-search"><div class="querybox" id="suggBox"><form><input id="searchInput" class="query" type="text" placeholder="搜科学领域专业百科词条" name="query" autocomplete="off" value=""><a href="javascript:;" class="query-search"></a></form></div></div><div class="header-rgt"><span class="btn-header-rgt btn-edit" id="editLemma">创建</span><div class="header-user no-login"></div></div></div></div><div class="fixed-placeholder" style="visibility:none"></div><div id="container" class=""><div class="content lemma-level1"><div class="detail-title" id="abstract-title"><h1>视觉里程计</h1><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#!" class="detail-edit">编辑</a></div><div class="section_content" data-id="60030653424533250"><div class="text_img ed_imgfloat_right"><a class="ed_image_link" data-src="https://img02.sogoucdn.com/app/a/200698/sogou_science_12995" data-bigsrc="https://img02.sogoucdn.com/app/a/200698/sogou_science_12995?width=231&amp;height=174&amp;titlename=%E8%A7%86%E9%A2%91%E5%BA%8F%E5%88%97%E4%B8%AD%E8%BF%90%E5%8A%A8%E7%89%A9%E4%BD%93%E7%9A%84%E5%85%89%E6%B5%81%E7%9F%A2%E9%87%8F&amp;w=300&amp;h=139" title="点击查看大图" data-w="300" data-h="139" style="background-image:url(https://web.archive.org/web/20221025114450im_/https://img02.sogoucdn.com/app/a/200698/sogou_science_12995)" href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#!"></a><div class="text_img_title" style="text-align: center;">视频序列中运动物体的光流矢量</div></div><div><p>在机器人学和计算机视觉中，<b>视觉里程计</b>是通过分析相关的相机图像以确定机器人位置和方向的过程。视觉里程计已被广泛应用于机器人领域，例如火星探测车。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_1" class="kx_ref">[1]</a></sup> </p></div></div><div id="catalog"><h2 class="title2">目录<a href="javascript:" class="detail-edit">编辑</a></h2><div class="catalog_wrap" style=""><ul class="catalog_list col2"><li><span class="order">1</span><a href="javascript:" data-level="1" data-id="14995396271014164">概览</a></li><li><span class="order">2</span><a href="javascript:" data-level="1" data-id="14995396271014165">类型</a></li><li class="secondary_catalog"><span>2.1 </span><a href="javascript:" data-id="14995396271014165"><b>单目型和立体型</b></a></li><li class="secondary_catalog"><span>2.2 </span><a href="javascript:" data-id="14995396271014165"><b>特征点法和直接法</b></a></li><li class="secondary_catalog"><span>2.3 </span><a href="javascript:" data-id="14995396271014165">视觉惯性里程计</a></li></ul><ul class="catalog_list col2"><li><span class="order">3</span><a href="javascript:" data-level="1" data-id="14995396287791363">算法</a></li><li><span class="order">4</span><a href="javascript:" data-level="1" data-id="14995396287791364">自运动</a></li><li class="secondary_catalog"><span>4.1 </span><a href="javascript:" data-id="14995396287791364">概览</a></li><li><span class="order">5</span><a href="javascript:" data-level="1" data-id="references">参考文献</a></li></ul></div></div><div id="paragraphs"><div><div id="par_14995396271014164"><h2 class="title">1 概览<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>在导航中，里程计利用来自执行器的运动数据（例如旋转编码器测量的车轮速度）估计位置随时间的变化。传统的里程计技术虽然对轮式或履带式车辆有用，但不能应用于以非标准方式移动的机器人，如足式机器人。此外，里程计普遍存在精度问题，因为车轮在地板上存在滑动，相较于车轮旋转，其产生了不均匀的行驶距离。当车辆在非光滑表面上行驶时，误差将会更大。随着时间的推移里程计读数越来越不可靠，因为误差会不断积累复合。</p><p>视觉里程计使用连续的相机图像来估计行进距离，从而确定等效里程计信息。视觉里程计允许机器人或车辆在任何表面上使用任何类型的运动来提高导航精度。</p></div></div><div id="par_14995396271014165"><h2 class="title">2 类型<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>视觉里程计有各种类型。</p><p></p><h3>2.1 <span>单目型和立体型</span></h3><p></p><p>根据摄像机设置，摄像机可分为单目摄像机(单摄像机)、立体摄像机(立体设置中的两个摄像机)。</p><p></p><div class="text_img ed_imgfloat_embedmiddle">
            <a class="ed_image_link" data-src="https://img04.sogoucdn.com/app/a/200698/sogou_science_12996" data-bigsrc="" title="点击查看大图" href="javascript:">
                <img alt="" class="ed_imgfloat_embedmiddle" img_height="300" img_width="300" titlename="VIO广泛应用于商用四轴飞行器，在GPS缺失的情况下提供定位。" data-align="embedmiddle" width="300" data-src="https://img04.sogoucdn.com/app/a/200698/sogou_science_12996" src="./539.视觉里程计 - 搜狗科学百科_files/sogou_science_12996" data-observer="true">
            </a>
            <div class="text_img_title" style="text-align: center;">VIO广泛应用于商用四轴飞行器，在GPS缺失的情况下提供定位。</div>
        </div> <p></p><p></p><h3>2.2 <span>特征点法和直接法</span></h3><p></p><p>传统视觉里程计的视觉信息是通过特征点法获得的，该方法提取图像特征点，并在图像序列中跟踪它们。最新的视觉里程计研究提供了一种替代方法，称为直接法，它直接使用图像序列中的像素强度作为视觉输入。还存在有混合方法。</p><p></p><h3>2.3 <span>视觉惯性里程计</span></h3><p></p><p>如果视觉里程计系统采用了惯性测量单元(IMU)，其通常被称为视觉惯性里程计(VIO)。</p></div></div><div id="par_14995396287791363"><h2 class="title">3 算法<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>大多数现有的视觉里程计方法基于以下的步骤。</p><p>
 </p><ol>
  <li>获取输入图像:使用单个相机<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_2" class="kx_ref">[2]</a></sup><sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_3" class="kx_ref">[3]</a></sup> ，立体相机，<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_3" class="kx_ref">[3]</a></sup><sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_4" class="kx_ref">[4]</a></sup> 或全向相机。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_5" class="kx_ref">[5]</a></sup><sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_6" class="kx_ref">[6]</a></sup></li>
  <li>图像校正:应用图像处理技术消除镜头失真等。</li>
  <li>特征检测:定义兴趣点算子，跨帧匹配特征，构建光流场。
   <ol>
    <li>使用相关性建立两幅图像的对应关系，但不进行长期特征跟踪。</li>
    <li>特征提取与特征关联。</li>
    <li>构造光流场(卢卡斯-卡纳德方法)。</li>
   </ol></li>
  <li>检查流场向量是否有潜在的跟踪误差，并移除异常值。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_7" class="kx_ref">[7]</a></sup></li>
  <li>用光流估算相机运动。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_8" class="kx_ref">[8]</a></sup><sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_9" class="kx_ref">[9]</a></sup><sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_10" class="kx_ref">[10]</a></sup><sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_11" class="kx_ref">[11]</a></sup> &nbsp;
   <ol>
    <li>选择1:用于状态估计分布维护的卡尔曼滤波器。</li>
    <li>选择2:基于两个相邻图像之间的重投影误差，找到最小化损失函数的几何3D属性。这可以通过数值优化或随机抽样来实现。</li>
   </ol></li>
  <li>定期重新填充跟踪点，以保持图像的覆盖范围。</li>
 </ol><p></p><p>特征点法的一种替代方法是“直接”或基于外观的视觉里程计技术，该技术直接最小化传感器空间中的误差，并避免了特征匹配和提取。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_4" class="kx_ref">[4]</a></sup><sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_12" class="kx_ref">[12]</a></sup><sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_13" class="kx_ref">[13]</a></sup> </p><p>另一种被称为“视觉测量法”的方法使用相位相关而不是特征提取来估计图像之间的平面旋转平移。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_14" class="kx_ref">[14]</a></sup><sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_15" class="kx_ref">[15]</a></sup> </p></div></div><div id="par_14995396287791364"><h2 class="title">4 自运动<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p><b>自运动</b>定义为相机在环境中的三维运动。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_16" class="kx_ref">[16]</a></sup> 在计算机视觉领域，自运动指的是相机相对于刚性场景的运动估计。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_17" class="kx_ref">[17]</a></sup> 自运动估计的一个例子是估计汽车相对于车道线或车辆所观测路标的相对运动。自运动估计在自主机器人导航应用中非常重要<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_18" class="kx_ref">[18]</a></sup> </p><p></p><h3>4.1 <span>概览</span></h3><p></p><p>相机自运动估计的目标是使用相机拍摄的图像序列来确定相机在环境中的3D运动。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_19" class="kx_ref">[19]</a></sup> 在环境中估计相机运动的过程涉及对移动相机捕获的一系列图像使用视觉里程计技术。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_20" class="kx_ref">[20]</a></sup> 通常通过特征检测，以从单目相机或立体相机生成图像序列中的<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_16" class="kx_ref">[16]</a></sup> 两个图像帧构建光流。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_20" class="kx_ref">[20]</a></sup> 使用立体图像帧有利于减少误差，并可以提供额外的深度和比例信息。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_21" class="kx_ref">[21]</a></sup><sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_22" class="kx_ref">[22]</a></sup> </p><p>特征在第一帧中检测，在第二帧中匹配。然后，该信息用于为这两幅图像中检测到的特征制作光流场。光流场说明了特征是如何从一个点(扩展点)偏离的。扩展点可以从光流场中进行检测，用以指示相机的运动方向，从而提供对相机运动的估计。</p><p>还有其他一些从图像中提取自运动信息的方法，包括不用特征检测与光流场而是直接使用图像强度的方法。<sup><a href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/d10539.htm#quote_16" class="kx_ref">[16]</a></sup> </p></div></div></div></div><div id="references"><h2 class="title" id="par_references">参考文献</h2><ul class="references"><li id="quote_1"><span class="references-num">[1]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Maimone, M.; Cheng, Y.; Matthies, L. (2007). "Two years of Visual Odometry on the Mars Exploration Rovers" (PDF). Journal of Field Robotics. 24 (3): 169–186. doi:10.1002/rob.20184. Retrieved 2008-07-10..</span></p></li><li id="quote_2"><span class="references-num">[2]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Chhaniyara, Savan; KASPAR ALTHOEFER; LAKMAL D. SENEVIRATNE (2008). "Visual Odometry Technique Using Circular Marker Identification For Motion Parameter Estimation". Advances in Mobile Robotics: Proceedings of the Eleventh International Conference on Climbing and Walking Robots and the Support Technologies for Mobile Machines, Coimbra, Portugal. The Eleventh International Conference on Climbing and Walking Robots and the Support Technologies for Mobile Machines. 11. World Scientific, 2008..</span></p></li><li id="quote_3"><span class="references-num">[3]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Nister, D; Naroditsky, O.; Bergen, J (Jan 2004). Visual Odometry. Computer Vision and Pattern Recognition, 2004. CVPR 2004. 1. pp. I–652 – I–659 Vol.1. doi:10.1109/CVPR.2004.1315094..</span></p></li><li id="quote_4"><span class="references-num">[4]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Comport, A.I.; Malis, E.; Rives, P. (2010). F. Chaumette; P. Corke; P. Newman, eds. "Real-time Quadrifocal Visual Odometry". International Journal of Robotics Research, Special Issue on Robot Vision. 29 (2–3): 245–266. CiteSeerX 10.1.1.720.3113. doi:10.1177/0278364909356601..</span></p></li><li id="quote_5"><span class="references-num">[5]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Scaramuzza, D.; Siegwart, R. (October 2008). "Appearance-Guided Monocular Omnidirectional Visual Odometry for Outdoor Ground Vehicles". IEEE Transactions on Robotics: 1–12. Retrieved 2008-10-20..</span></p></li><li id="quote_6"><span class="references-num">[6]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Corke, P.; Strelow, D.; Singh, S. "Omnidirectional visual odometry for a planetary rover". Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on. 4..</span></p></li><li id="quote_7"><span class="references-num">[7]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Campbell, J.; Sukthankar, R.; Nourbakhsh, I.; Pittsburgh, I.R. "Techniques for evaluating optical flow for visual odometry in extreme terrain". Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on. 4..</span></p></li><li id="quote_8"><span class="references-num">[8]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Sunderhauf, N.; Konolige, K.; Lacroix, S.; Protzel, P. (2005). "Visual odometry using sparse bundle adjustment on an autonomous outdoor vehicle" (PDF). Levi, Schanz, Lafrenz, and Avrutin, Editors, Tagungsband Autonome Mobile Systeme: 157–163. Retrieved 2008-07-10..</span></p></li><li id="quote_9"><span class="references-num">[9]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Konolige, K.; Agrawal, M.; Bolles, R.C.; Cowan, C.; Fischler, M.; Gerkey, B.P. (2006). "Outdoor mapping and navigation using stereo vision" (PDF). Proc. Of the Intl. Symp. On Experimental Robotics (ISER). Retrieved 2008-07-10..</span></p></li><li id="quote_10"><span class="references-num">[10]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Olson, C.F.; Matthies, L.; Schoppers, M.; Maimone, M.W. (2002). "Rover navigation using stereo ego-motion" (PDF). Robotics and Autonomous Systems. 43 (4): 215–229. doi:10.1016/s0921-8890(03)00004-6. Retrieved 2010-06-06..</span></p></li><li id="quote_11"><span class="references-num">[11]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Cheng, Y.; Maimone, M.W.; Matthies, L. (2006). "Visual Odometry on the Mars Exploration Rovers" (PDF). IEEE Robotics and Automation Magazine. 13 (2): 54–62. CiteSeerX 10.1.1.297.4693. doi:10.1109/MRA.2006.1638016. Retrieved 2008-07-10..</span></p></li><li id="quote_12"><span class="references-num">[12]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Engel, Jakob; Schöps, Thomas; Cremers, Daniel (2014). "European Conference on Computer Vision (ECCV) 2014" (PDF). |contribution= ignored (help).</span></p></li><li id="quote_13"><span class="references-num">[13]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Engel, Jakob; Sturm, Jürgen; Cremers, Daniel (2013). "IEEE International Conference on Computer Vision (ICCV)" (PDF). |contribution= ignored (help).</span></p></li><li id="quote_14"><span class="references-num">[14]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Zaman, M. (2007). "High Precision Relative Localization Using a Single Camera". Robotics and Automation, 2007.(ICRA 2007). Proceedings. 2007 IEEE International Conference on..</span></p></li><li id="quote_15"><span class="references-num">[15]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Zaman, M. (2007). "High resolution relative localisation using two cameras". Journal of Robotics and Autonomous Systems (JRAS). 55 (9): 685–692. doi:10.1016/j.robot.2007.05.008..</span></p></li><li id="quote_16"><span class="references-num">[16]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Irani, M.; Rousso, B.; Peleg S. (June 1994). "Recovery of Ego-Motion Using Image Stabilization" (PDF). IEEE Computer Society Conference on Computer Vision and Pattern Recognition: 21–23. Retrieved 7 June 2010..</span></p></li><li id="quote_17"><span class="references-num">[17]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Burger, W.; Bhanu, B. (Nov 1990). "Estimating 3D egomotion from perspective image sequence". IEEE Transactions on Pattern Analysis and Machine Intelligence. 12 (11): 1040–1058. doi:10.1109/34.61704..</span></p></li><li id="quote_18"><span class="references-num">[18]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Shakernia, O.; Vidal, R.; Shankar, S. (2003). "Omnidirectional Egomotion Estimation From Back-projection Flow" (PDF). Conference on Computer Vision and Pattern Recognition Workshop. 7: 82. CiteSeerX 10.1.1.5.8127. doi:10.1109/CVPRW.2003.10074. Retrieved 7 June 2010..</span></p></li><li id="quote_19"><span class="references-num">[19]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Tian, T.; Tomasi, C.; Heeger, D. (1996). "Comparison of Approaches to Egomotion Computation" (PDF). IEEE Computer Society Conference on Computer Vision and Pattern Recognition: 315. Archived from the original (PDF) on August 8, 2008. Retrieved 7 June 2010..</span></p></li><li id="quote_20"><span class="references-num">[20]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Milella, A.; Siegwart, R. (January 2006). "Stereo-Based Ego-Motion Estimation Using Pixel Tracking and Iterative Closest Point" (PDF). IEEE International Conference on Computer Vision Systems: 21. Retrieved 7 June 2010..</span></p></li><li id="quote_21"><span class="references-num">[21]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Olson, C. F.; Matthies, L.; Schoppers, M.; Maimoneb M. W. (June 2003). "Rover navigation using stereo ego-motion" (PDF). Robotics and Autonomous Systems. 43 (9): 215–229. doi:10.1016/s0921-8890(03)00004-6. Retrieved 7 June 2010..</span></p></li><li id="quote_22"><span class="references-num">[22]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Sudin Dinesh, Koteswara Rao, K. ; Unnikrishnan, M. ; Brinda, V. ; Lalithambika, V.R. ; Dhekane, M.V. "Improvements in Visual Odometry Algorithm for Planetary Exploration Rovers". IEEE International Conference on Emerging Trends in Communication, Control, Signal Processing &amp; Computing Applications (C2SPCA), 2013.</span></p></li></ul></div><div class="read-num">阅读 <!-- -->46</div></div><div class="right-side" id="rightSide"><div class="side" id="lemma-side"><div class="side-title">版本记录</div><ul class="side-lst"><li><p class="side-lst-txt">暂无</p></li></ul><div class="user-card userCard"></div></div><div class="side"><div class="side-event"></div></div></div></div><div class="footer-box"><div id="footer"><div class="footer-logo-wrap"><div class="footer-logo"></div><div class="footer-logo-text">知识·传播·科普</div></div><div class="footer-info">本网站内容采用<a target="_blank" href="https://web.archive.org/web/20221025114450/https://creativecommons.org/licenses/by-sa/3.0/deed.zh?tdsourcetag=s_pctim_aiomsg">CC-BY-SA 3.0</a>授权</div><div class="footer-btn-wrap"><a target="_blank" href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/help/#user_protocol">用户协议</a><a target="_blank" href="https://web.archive.org/web/20221025114450/http://www.sogou.com/docs/terms.htm?v=1">免责声明</a><a target="_blank" href="https://web.archive.org/web/20221025114450/http://corp.sogou.com/private.html">隐私政策</a><a target="_blank" href="https://web.archive.org/web/20221025114450/https://baike.sogou.com/kexue/intro.htm">关于我们</a></div></div></div><script>window.lemmaInfo ={"lemmaId":"10539","versionId":"60030653407756035","title":"视觉里程计","subtitle":"","abstracts":{"paragraphId":"60030653424533250","title":"摘要","versionId":"60030653407756036","lemmaId":10539,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":78021691,"name":"没时间到白头","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1603077057,"comment":null,"dependVersionId":0,"contentType":2,"content":"<p>在机器人学和计算机视觉中，<b>视觉里程计</b>是通过分析相关的相机图像以确定机器人位置和方向的过程。视觉里程计已被广泛应用于机器人领域，例如火星探测车。<sup><a href=\"#quote_1\" class=\"kx_ref\">[1]</a></sup> </p>","pics":[{"originalUrl":"https://web.archive.org/web/20221025114450/https://img02.sogoucdn.com/app/a/200698/sogou_science_12995?width=231&height=174&titlename=%E8%A7%86%E9%A2%91%E5%BA%8F%E5%88%97%E4%B8%AD%E8%BF%90%E5%8A%A8%E7%89%A9%E4%BD%93%E7%9A%84%E5%85%89%E6%B5%81%E7%9F%A2%E9%87%8F&w=300&h=139","url":"https://web.archive.org/web/20221025114450/https://img02.sogoucdn.com/app/a/200698/sogou_science_12995","rw":300,"rh":139,"title":"视频序列中运动物体的光流矢量","alt":null,"width":231,"height":174}],"card":null,"references":[],"versionCount":0},"card":{"paragraphId":"0","title":"","versionId":"0","lemmaId":0,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":0,"comment":null,"dependVersionId":0,"contentType":0,"content":"","pics":null,"card":null,"references":null,"versionCount":0},"categories":[{"id":6,"name":"机械工程","parents":[]}],"creator":{"uid":31221062,"name":"张志煌","pic":"https://web.archive.org/web/20221025114450/https://cache.soso.com/qlogo/g?b=oidb&k=yI5YsiaWJ2vMgkUriabwpoPQ&s=100&t=1555539303","introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":"","jobBrief":"","role":0,"roleName":null,"title":"","professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":151,"partnerIdCreateTime":1595844883,"partnerIdPoped":false},"createTime":1568626256,"editor":{"uid":31221062,"name":"张志煌","pic":"https://web.archive.org/web/20221025114450/https://cache.soso.com/qlogo/g?b=oidb&k=yI5YsiaWJ2vMgkUriabwpoPQ&s=100&t=1555539303","introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":"","jobBrief":"","role":0,"roleName":null,"title":"","professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":151,"partnerIdCreateTime":1595844883,"partnerIdPoped":false},"editTime":1576233952,"state":1,"versionCount":1,"upNum":0,"downNum":0,"pics":[{"originalUrl":"https://web.archive.org/web/20221025114450/https://img02.sogoucdn.com/app/a/200698/sogou_science_12995?width=231&height=174&titlename=%E8%A7%86%E9%A2%91%E5%BA%8F%E5%88%97%E4%B8%AD%E8%BF%90%E5%8A%A8%E7%89%A9%E4%BD%93%E7%9A%84%E5%85%89%E6%B5%81%E7%9F%A2%E9%87%8F&w=300&h=139","url":"https://web.archive.org/web/20221025114450/https://img02.sogoucdn.com/app/a/200698/sogou_science_12995","rw":300,"rh":139,"title":"视频序列中运动物体的光流矢量","alt":null,"width":231,"height":174},{"originalUrl":"https://web.archive.org/web/20221025114450/https://img04.sogoucdn.com/app/a/200698/sogou_science_12996?w=300&h=300&titlename=VIO%E5%B9%BF%E6%B3%9B%E5%BA%94%E7%94%A8%E4%BA%8E%E5%95%86%E7%94%A8%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8%EF%BC%8C%E5%9C%A8GPS%E7%BC%BA%E5%A4%B1%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E6%8F%90%E4%BE%9B%E5%AE%9A%E4%BD%8D%E3%80%82","url":"https://web.archive.org/web/20221025114450/https://img04.sogoucdn.com/app/a/200698/sogou_science_12996","rw":300,"rh":300,"title":"VIO广泛应用于商用四轴飞行器，在GPS缺失的情况下提供定位。","alt":null,"width":0,"height":0}],"catalogs":[{"level":1,"title":"概览","paragraphId":"14995396271014164","subCatalogs":null},{"level":1,"title":"类型","paragraphId":"14995396271014165","subCatalogs":[{"level":2,"title":"<b>单目型和立体型</b>","paragraphId":"14995396271014165","subCatalogs":null},{"level":2,"title":"<b>特征点法和直接法</b>","paragraphId":"14995396271014165","subCatalogs":null},{"level":2,"title":"视觉惯性里程计","paragraphId":"14995396271014165","subCatalogs":null}]},{"level":1,"title":"算法","paragraphId":"14995396287791363","subCatalogs":null},{"level":1,"title":"自运动","paragraphId":"14995396287791364","subCatalogs":[{"level":2,"title":"概览","paragraphId":"14995396287791364","subCatalogs":null}]},{"level":1,"title":"参考文献","paragraphId":"-1","subCatalogs":null}],"paragraphs":[{"paragraphId":"14995396271014164","title":"概览","versionId":"60030653407756037","lemmaId":10539,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":78021691,"name":"没时间到白头","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1603077057,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>在导航中，里程计利用来自执行器的运动数据（例如旋转编码器测量的车轮速度）估计位置随时间的变化。传统的里程计技术虽然对轮式或履带式车辆有用，但不能应用于以非标准方式移动的机器人，如足式机器人。此外，里程计普遍存在精度问题，因为车轮在地板上存在滑动，相较于车轮旋转，其产生了不均匀的行驶距离。当车辆在非光滑表面上行驶时，误差将会更大。随着时间的推移里程计读数越来越不可靠，因为误差会不断积累复合。</p><p>视觉里程计使用连续的相机图像来估计行进距离，从而确定等效里程计信息。视觉里程计允许机器人或车辆在任何表面上使用任何类型的运动来提高导航精度。</p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14995396271014165","title":"类型","versionId":"60030653407756038","lemmaId":10539,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":78021691,"name":"没时间到白头","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1603077057,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>视觉里程计有各种类型。</p><p></p><h3><b>单目型和立体型</b></h3><p></p><p>根据摄像机设置，摄像机可分为单目摄像机(单摄像机)、立体摄像机(立体设置中的两个摄像机)。</p><p><img alt=\"\" class=\"ed_imgfloat_embedmiddle\" img_height=\"300\" img_width=\"300\" titlename=\"VIO广泛应用于商用四轴飞行器，在GPS缺失的情况下提供定位。\" data-align=\"embedmiddle\" width=\"300\" data-src=\"https://img04.sogoucdn.com/app/a/200698/sogou_science_12996\"> </p><p></p><h3><b>特征点法和直接法</b></h3><p></p><p>传统视觉里程计的视觉信息是通过特征点法获得的，该方法提取图像特征点，并在图像序列中跟踪它们。最新的视觉里程计研究提供了一种替代方法，称为直接法，它直接使用图像序列中的像素强度作为视觉输入。还存在有混合方法。</p><p></p><h3>视觉惯性里程计</h3><p></p><p>如果视觉里程计系统采用了惯性测量单元(IMU)，其通常被称为视觉惯性里程计(VIO)。</p>","pics":[{"originalUrl":"https://web.archive.org/web/20221025114450/https://img04.sogoucdn.com/app/a/200698/sogou_science_12996?w=300&h=300&titlename=VIO%E5%B9%BF%E6%B3%9B%E5%BA%94%E7%94%A8%E4%BA%8E%E5%95%86%E7%94%A8%E5%9B%9B%E8%BD%B4%E9%A3%9E%E8%A1%8C%E5%99%A8%EF%BC%8C%E5%9C%A8GPS%E7%BC%BA%E5%A4%B1%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%E6%8F%90%E4%BE%9B%E5%AE%9A%E4%BD%8D%E3%80%82","url":"https://web.archive.org/web/20221025114450/https://img04.sogoucdn.com/app/a/200698/sogou_science_12996","rw":300,"rh":300,"title":"VIO广泛应用于商用四轴飞行器，在GPS缺失的情况下提供定位。","alt":null,"width":0,"height":0}],"card":null,"references":[],"versionCount":0},{"paragraphId":"14995396287791363","title":"算法","versionId":"60030653407756039","lemmaId":10539,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":78021691,"name":"没时间到白头","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1603077057,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>大多数现有的视觉里程计方法基于以下的步骤。</p><p>\n <ol>\n  <li>获取输入图像:使用单个相机<sup><a href=\"#quote_2\" class=\"kx_ref\">[2]</a></sup><sup><a href=\"#quote_3\" class=\"kx_ref\">[3]</a></sup> ，立体相机，<sup><a href=\"#quote_3\" class=\"kx_ref\">[3]</a></sup><sup><a href=\"#quote_4\" class=\"kx_ref\">[4]</a></sup> 或全向相机。<sup><a href=\"#quote_5\" class=\"kx_ref\">[5]</a></sup><sup><a href=\"#quote_6\" class=\"kx_ref\">[6]</a></sup></li>\n  <li>图像校正:应用图像处理技术消除镜头失真等。</li>\n  <li>特征检测:定义兴趣点算子，跨帧匹配特征，构建光流场。\n   <ol>\n    <li>使用相关性建立两幅图像的对应关系，但不进行长期特征跟踪。</li>\n    <li>特征提取与特征关联。</li>\n    <li>构造光流场(卢卡斯-卡纳德方法)。</li>\n   </ol></li>\n  <li>检查流场向量是否有潜在的跟踪误差，并移除异常值。<sup><a href=\"#quote_7\" class=\"kx_ref\">[7]</a></sup></li>\n  <li>用光流估算相机运动。<sup><a href=\"#quote_8\" class=\"kx_ref\">[8]</a></sup><sup><a href=\"#quote_9\" class=\"kx_ref\">[9]</a></sup><sup><a href=\"#quote_10\" class=\"kx_ref\">[10]</a></sup><sup><a href=\"#quote_11\" class=\"kx_ref\">[11]</a></sup> &nbsp;\n   <ol>\n    <li>选择1:用于状态估计分布维护的卡尔曼滤波器。</li>\n    <li>选择2:基于两个相邻图像之间的重投影误差，找到最小化损失函数的几何3D属性。这可以通过数值优化或随机抽样来实现。</li>\n   </ol></li>\n  <li>定期重新填充跟踪点，以保持图像的覆盖范围。</li>\n </ol></p><p>特征点法的一种替代方法是“直接”或基于外观的视觉里程计技术，该技术直接最小化传感器空间中的误差，并避免了特征匹配和提取。<sup><a href=\"#quote_4\" class=\"kx_ref\">[4]</a></sup><sup><a href=\"#quote_12\" class=\"kx_ref\">[12]</a></sup><sup><a href=\"#quote_13\" class=\"kx_ref\">[13]</a></sup> </p><p>另一种被称为“视觉测量法”的方法使用相位相关而不是特征提取来估计图像之间的平面旋转平移。<sup><a href=\"#quote_14\" class=\"kx_ref\">[14]</a></sup><sup><a href=\"#quote_15\" class=\"kx_ref\">[15]</a></sup> </p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14995396287791364","title":"自运动","versionId":"60030653407756040","lemmaId":10539,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":78021691,"name":"没时间到白头","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1603077057,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p><b>自运动</b>定义为相机在环境中的三维运动。<sup><a href=\"#quote_16\" class=\"kx_ref\">[16]</a></sup> 在计算机视觉领域，自运动指的是相机相对于刚性场景的运动估计。<sup><a href=\"#quote_17\" class=\"kx_ref\">[17]</a></sup> 自运动估计的一个例子是估计汽车相对于车道线或车辆所观测路标的相对运动。自运动估计在自主机器人导航应用中非常重要<sup><a href=\"#quote_18\" class=\"kx_ref\">[18]</a></sup> </p><p><h3>概览</h3></p><p>相机自运动估计的目标是使用相机拍摄的图像序列来确定相机在环境中的3D运动。<sup><a href=\"#quote_19\" class=\"kx_ref\">[19]</a></sup> 在环境中估计相机运动的过程涉及对移动相机捕获的一系列图像使用视觉里程计技术。<sup><a href=\"#quote_20\" class=\"kx_ref\">[20]</a></sup> 通常通过特征检测，以从单目相机或立体相机生成图像序列中的<sup><a href=\"#quote_16\" class=\"kx_ref\">[16]</a></sup> 两个图像帧构建光流。<sup><a href=\"#quote_20\" class=\"kx_ref\">[20]</a></sup> 使用立体图像帧有利于减少误差，并可以提供额外的深度和比例信息。<sup><a href=\"#quote_21\" class=\"kx_ref\">[21]</a></sup><sup><a href=\"#quote_22\" class=\"kx_ref\">[22]</a></sup> </p><p>特征在第一帧中检测，在第二帧中匹配。然后，该信息用于为这两幅图像中检测到的特征制作光流场。光流场说明了特征是如何从一个点(扩展点)偏离的。扩展点可以从光流场中进行检测，用以指示相机的运动方向，从而提供对相机运动的估计。</p><p>还有其他一些从图像中提取自运动信息的方法，包括不用特征检测与光流场而是直接使用图像强度的方法。<sup><a href=\"#quote_16\" class=\"kx_ref\">[16]</a></sup> </p>","pics":null,"card":null,"references":[],"versionCount":0}],"references":[{"id":1,"type":"book","title":"Maimone, M.; Cheng, Y.; Matthies, L. (2007). \"Two years of Visual Odometry on the Mars Exploration Rovers\" (PDF). Journal of Field Robotics. 24 (3): 169–186. doi:10.1002/rob.20184. Retrieved 2008-07-10.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":2,"type":"book","title":"Chhaniyara, Savan; KASPAR ALTHOEFER; LAKMAL D. SENEVIRATNE (2008). \"Visual Odometry Technique Using Circular Marker Identification For Motion Parameter Estimation\". Advances in Mobile Robotics: Proceedings of the Eleventh International Conference on Climbing and Walking Robots and the Support Technologies for Mobile Machines, Coimbra, Portugal. The Eleventh International Conference on Climbing and Walking Robots and the Support Technologies for Mobile Machines. 11. World Scientific, 2008.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":3,"type":"book","title":"Nister, D; Naroditsky, O.; Bergen, J (Jan 2004). Visual Odometry. Computer Vision and Pattern Recognition, 2004. CVPR 2004. 1. pp. I–652 – I–659 Vol.1. doi:10.1109/CVPR.2004.1315094.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":4,"type":"book","title":"Comport, A.I.; Malis, E.; Rives, P. (2010). F. Chaumette; P. Corke; P. Newman, eds. \"Real-time Quadrifocal Visual Odometry\". International Journal of Robotics Research, Special Issue on Robot Vision. 29 (2–3): 245–266. CiteSeerX 10.1.1.720.3113. doi:10.1177/0278364909356601.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":5,"type":"book","title":"Scaramuzza, D.; Siegwart, R. (October 2008). \"Appearance-Guided Monocular Omnidirectional Visual Odometry for Outdoor Ground Vehicles\". IEEE Transactions on Robotics: 1–12. Retrieved 2008-10-20.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":6,"type":"book","title":"Corke, P.; Strelow, D.; Singh, S. \"Omnidirectional visual odometry for a planetary rover\". Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on. 4.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":7,"type":"book","title":"Campbell, J.; Sukthankar, R.; Nourbakhsh, I.; Pittsburgh, I.R. \"Techniques for evaluating optical flow for visual odometry in extreme terrain\". Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on. 4.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":8,"type":"book","title":"Sunderhauf, N.; Konolige, K.; Lacroix, S.; Protzel, P. (2005). \"Visual odometry using sparse bundle adjustment on an autonomous outdoor vehicle\" (PDF). Levi, Schanz, Lafrenz, and Avrutin, Editors, Tagungsband Autonome Mobile Systeme: 157–163. Retrieved 2008-07-10.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":9,"type":"book","title":"Konolige, K.; Agrawal, M.; Bolles, R.C.; Cowan, C.; Fischler, M.; Gerkey, B.P. (2006). \"Outdoor mapping and navigation using stereo vision\" (PDF). Proc. Of the Intl. Symp. On Experimental Robotics (ISER). Retrieved 2008-07-10.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":10,"type":"book","title":"Olson, C.F.; Matthies, L.; Schoppers, M.; Maimone, M.W. (2002). \"Rover navigation using stereo ego-motion\" (PDF). Robotics and Autonomous Systems. 43 (4): 215–229. doi:10.1016/s0921-8890(03)00004-6. Retrieved 2010-06-06.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":11,"type":"book","title":"Cheng, Y.; Maimone, M.W.; Matthies, L. (2006). \"Visual Odometry on the Mars Exploration Rovers\" (PDF). IEEE Robotics and Automation Magazine. 13 (2): 54–62. CiteSeerX 10.1.1.297.4693. doi:10.1109/MRA.2006.1638016. Retrieved 2008-07-10.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":12,"type":"book","title":"Engel, Jakob; Schöps, Thomas; Cremers, Daniel (2014). \"European Conference on Computer Vision (ECCV) 2014\" (PDF). |contribution= ignored (help)","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":13,"type":"book","title":"Engel, Jakob; Sturm, Jürgen; Cremers, Daniel (2013). \"IEEE International Conference on Computer Vision (ICCV)\" (PDF). |contribution= ignored (help)","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":14,"type":"book","title":"Zaman, M. (2007). \"High Precision Relative Localization Using a Single Camera\". Robotics and Automation, 2007.(ICRA 2007). Proceedings. 2007 IEEE International Conference on.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":15,"type":"book","title":"Zaman, M. (2007). \"High resolution relative localisation using two cameras\". Journal of Robotics and Autonomous Systems (JRAS). 55 (9): 685–692. doi:10.1016/j.robot.2007.05.008.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":16,"type":"book","title":"Irani, M.; Rousso, B.; Peleg S. (June 1994). \"Recovery of Ego-Motion Using Image Stabilization\" (PDF). IEEE Computer Society Conference on Computer Vision and Pattern Recognition: 21–23. Retrieved 7 June 2010.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":17,"type":"book","title":"Burger, W.; Bhanu, B. (Nov 1990). \"Estimating 3D egomotion from perspective image sequence\". IEEE Transactions on Pattern Analysis and Machine Intelligence. 12 (11): 1040–1058. doi:10.1109/34.61704.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":18,"type":"book","title":"Shakernia, O.; Vidal, R.; Shankar, S. (2003). \"Omnidirectional Egomotion Estimation From Back-projection Flow\" (PDF). Conference on Computer Vision and Pattern Recognition Workshop. 7: 82. CiteSeerX 10.1.1.5.8127. doi:10.1109/CVPRW.2003.10074. Retrieved 7 June 2010.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":19,"type":"book","title":"Tian, T.; Tomasi, C.; Heeger, D. (1996). \"Comparison of Approaches to Egomotion Computation\" (PDF). IEEE Computer Society Conference on Computer Vision and Pattern Recognition: 315. Archived from the original (PDF) on August 8, 2008. Retrieved 7 June 2010.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":20,"type":"book","title":"Milella, A.; Siegwart, R. (January 2006). \"Stereo-Based Ego-Motion Estimation Using Pixel Tracking and Iterative Closest Point\" (PDF). IEEE International Conference on Computer Vision Systems: 21. Retrieved 7 June 2010.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":21,"type":"book","title":"Olson, C. F.; Matthies, L.; Schoppers, M.; Maimoneb M. W. (June 2003). \"Rover navigation using stereo ego-motion\" (PDF). Robotics and Autonomous Systems. 43 (9): 215–229. doi:10.1016/s0921-8890(03)00004-6. Retrieved 7 June 2010.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":22,"type":"book","title":"Sudin Dinesh, Koteswara Rao, K. ; Unnikrishnan, M. ; Brinda, V. ; Lalithambika, V.R. ; Dhekane, M.V. \"Improvements in Visual Odometry Algorithm for Planetary Exploration Rovers\". IEEE International Conference on Emerging Trends in Communication, Control, Signal Processing & Computing Applications (C2SPCA), 2013","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false}],"recommendReferences":null,"auditState":2,"lemmaLevel":1,"origin":0,"originEnTitle":null,"originZhTitle":null,"pv":46,"auditType":0,"synonyms":null,"showEditTime":"2019.12.13 18:45","auditors":[{"uid":0,"name":"Ki.κe","pic":"https://web.archive.org/web/20221025114450/https://wx.qlogo.cn/mmopen/vi_32/y67kfr32Doib4wg71Jiau7jVWvharic3nRKgdRRQSl6koeQJCo0GQs2Krw0vwdFRsOWnHIQOwAZsSg5lIkIrFCOcQ/132","introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":"","jobBrief":"","role":0,"roleName":null,"title":"","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false}],"hasZhishiNav":false,"auditInfos":{},"isHistory":false};</script><script crossorigin="anonymous" src="./539.视觉里程计 - 搜狗科学百科_files/aegis.min.js.download"></script><script crossorigin="anonymous" src="./539.视觉里程计 - 搜狗科学百科_files/main_2020092401.js.download"></script><script crossorigin="anonymous" src="./539.视觉里程计 - 搜狗科学百科_files/react.production.min.js.download"></script><script crossorigin="anonymous" src="./539.视觉里程计 - 搜狗科学百科_files/react-dom.production.min.js.download"></script><script crossorigin="anonymous" src="./539.视觉里程计 - 搜狗科学百科_files/jquery-1.11.1.min.js.download"></script><script crossorigin="anonymous" src="./539.视觉里程计 - 搜狗科学百科_files/main_2022062701.js.download"></script><script crossorigin="anonymous" src="./539.视觉里程计 - 搜狗科学百科_files/main_66bbe21.js.download"></script><script crossorigin="anonymous" src="./539.视觉里程计 - 搜狗科学百科_files/react.production.min.js.download"></script><script crossorigin="anonymous" src="./539.视觉里程计 - 搜狗科学百科_files/react-dom.production.min.js.download"></script><script crossorigin="anonymous" src="./539.视觉里程计 - 搜狗科学百科_files/main_edf0f08.js.download"></script>
</body></html>