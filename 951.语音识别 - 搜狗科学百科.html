<!DOCTYPE html>
<!-- saved from url=(0083)https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm -->
<html class="" data-reactroot=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./951.语音识别 - 搜狗科学百科_files/analytics.js.download" type="text/javascript"></script>
<script type="text/javascript">window.addEventListener('DOMContentLoaded',function(){var v=archive_analytics.values;v.service='wb';v.server_name='wwwb-app204.us.archive.org';v.server_ms=309;archive_analytics.send_pageview({});});</script>
<script type="text/javascript" src="./951.语音识别 - 搜狗科学百科_files/bundle-playback.js.download" charset="utf-8"></script>
<script type="text/javascript" src="./951.语音识别 - 搜狗科学百科_files/wombat.js.download" charset="utf-8"></script>
<script type="text/javascript">
  __wm.init("https://web.archive.org/web");
  __wm.wombat("https://baike.sogou.com/kexue/d10951.htm","20221028212226","https://web.archive.org/","web","/_static/",
	      "1666992146");
</script>
<link rel="stylesheet" type="text/css" href="./951.语音识别 - 搜狗科学百科_files/banner-styles.css">
<link rel="stylesheet" type="text/css" href="./951.语音识别 - 搜狗科学百科_files/iconochive.css">
<!-- End Wayback Rewrite JS Include -->
<meta name="save" content="history"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="baidu-site-verification" content="VWGb6TyYx8"><meta content="语音识别 - 搜狗科学百科" name="keywords"><meta content="搜狗科学百科是一部有着平等、协作、分享、自由理念的网络科学全书，为每一个互联网用户创造一个涵盖所有领域知识、服务的中文知识性平台。" name="description"><meta http-equiv="x-dns-prefetch-control" content="on"><meta name="server" baike="235" ip="210" env="online"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://cache.soso.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://hhy.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://pic.baike.soso.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://ugc.qpic.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://xui.ptlogin2.qq.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://q1.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://q2.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://q3.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://q4.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://q.qlogo.cn/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://img01.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://img02.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://img03.sogoucdn.com/"><link rel="dns-prefetch" href="https://web.archive.org/web/20221028212226/https://img04.sogoucdn.com/"><link rel="Shortcut Icon" href="https://web.archive.org/web/20221028212226im_/https://www.sogou.com/images/logo/new/favicon.ico?v=4"><link rel="Bookmark" href="https://www.sogou.com/images/logo/new/favicon.ico?v=4"><link href="./951.语音识别 - 搜狗科学百科_files/base_b849887.css" rel="stylesheet"><link href="./951.语音识别 - 搜狗科学百科_files/detail_378aed5.css" rel="stylesheet"><link href="./951.语音识别 - 搜狗科学百科_files/inviteAudit_7894507.css" rel="stylesheet"><link rel="stylesheet" href="./951.语音识别 - 搜狗科学百科_files/highlight.min.css"><title>语音识别 - 搜狗科学百科</title><style>.onekey-close {
	position: absolute;
	top: 16px;
	right: 16px;
	width: 24px;
	height: 24px;
	background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	text-indent: -999em;
	background-size: 84px;
	background-position: -63px 0;
}

.onekey-login {
	position: absolute;
	top: 16.4%;
	left: 0;
	right: 0;
	width: 100%;
}

/* .onekey-login-img {
    width: 75px;
    height: 75px;
    background: url("https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/images/sprite_wap_baike.png") no-repeat;
    background-size: 100px 91px;
    background-position: 0 0;
    background-repeat: no-repeat;
    margin: 0 auto;
} */

.onekey-login-title {
	text-align: center;
	padding-bottom: 3px;
	font-size: 21px;
	font-weight: bold;
	line-height: 30px;
	color: #000;
}

.onekey-login-txt {
	text-align: center;
	font-family: PingFangSC;
	font-size: 14px;
	line-height: 20px;
	color: #8f8f8f;
}

.onekey-login-qq,
.onekey-login-wx,
.onekey-login-phone {
	display: block;
	width: 245px;
	height: 54px;
	border-radius: 45px;
	text-align: center;

	margin: 0 auto;
	font-size: 17px;
	line-height: 24px;
	color: #000;
	/* padding: 16px 77px; */
	border-radius: 12px;
	border: solid 1px #e0e0e0;
}
.onekey-qq-content,
.onekey-vx-content,
.onekey-phone-content {
	display: inline-block;
	margin-top: 16px;
}
.onekey-qq-content {
	padding: 0 5px;
}

.onekey-login-qq {
	margin-top: 48px;
	margin-bottom: 24px;
}

.onekey-login-qq:before {
	display: inline-block;
	content: "";
	width: 20px;
	height: 20px;
	background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 80px;
	background-position: -20px 0;
	vertical-align: top;
	margin: 17px 8px 0 0;
}

.onekey-login-wx {
	margin-bottom: 24px;
}

.onekey-login-wx:before {
	display: inline-block;
	content: "";
	width: 21px;
	height: 21px;
	background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 84px;
	background-position: 0 0;
	vertical-align: top;
	margin: 17px 10px 0 0;
}

.onekey-login-phone {
}

.onekey-login-phone:before {
	display: inline-block;
	content: "";
	width: 21px;
	height: 21px;
	background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/sprite_wap_baike_37443f3.png) no-repeat;
	background-size: 84px;
	background-position: -42px 0;
	vertical-align: top;
	margin: 17px 10px 0 0;
}

.onekey-fixed {
	z-index: 100;
	position: fixed;
	top: 0;
	bottom: 0;
	left: 0;
	right: 0;
	background: #fff;
	width: 100%;
	height: 100%;
}

.onekey-fixed.forbid {
	z-index: 100;
	position: fixed;
	top: auto;
	bottom: 68px;
	left: 9%;
	right: 0;
	background: rgba(0, 0, 0, 0.7);
	width: 82%;
	height: 43px;
	border-radius: 25px;
	color: #ffffff;
}
.onekey-login-title.forbid {
	text-align: center;
	padding-bottom: 3px;
	font-size: 14px;
	font-weight: normal;
	line-height: 30px;
	color: white;
}
</style><style>#login_mask {
  background: #000;
  opacity: 0.5;
  filter: alpha(opacity=50);
  position: fixed;
  /*fixed好像在哪个IE上有BUG，先用用*/
  left: 0;
  top: 0;
  z-index: 999;
  height: 100%;
}

#login_iframe_container {
  position: fixed;
  width: 550px;
  height: 360px;
  z-index: 1020;
  background-color: #ffffff;
}

@media screen and (max-width: 828px) {
  #login_iframe_container {
    top: 50% !important;
    left: 50% !important;
    transform: translate(-50%, -50%);
  }
}

#login_iframe_container.new-login {
  width: 550px;
  height: 360px;
  background-image: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/background_2a4a8a6.png);
}

#login_iframe_container.new-login.no-bg {
  background: #fff;
}

#login_iframe_container.new-login .login-title {
  width: 100%;
  height: 42px;
  line-height: 42px;
  text-align: center;
  font-size: 30px;
  letter-spacing: 0.19px;
  color: #ffffff;
  margin-top: 62px;
}
#login_iframe_container.new-login .forbid-title {
  width: 100%;
  height: 42px;
  line-height: 42px;
  text-align: center;
  font-size: 24px;
  letter-spacing: 0.19px;
  color: #333333;
  margin-top: 150px;
}

#login_iframe_container.new-login.no-bg .login-title {
  color: #333333;
}

#login_iframe_container.new-login .login-subtitle {
  width: 100%;
  height: 18px;
  line-height: 18px;
  font-size: 13px;
  letter-spacing: 0.08px;
  color: #ffffff;
  text-align: center;
  margin-top: 9px;
  margin-bottom: 43px;
}

#login_iframe_container.new-login.no-bg .login-subtitle {
  color: #999999;
}

#login_iframe_container.new-login .login-subtitle::before {
  content: '';
  display: inline-block;
  width: 10px;
  height: 1px;
  background-color: #ffffff;
  position: relative;
  top: -4px;
  left: -5px;
}

#login_iframe_container.new-login .login-subtitle::after {
  content: '';
  display: inline-block;
  width: 10px;
  height: 1px;
  background-color: #ffffff;
  position: relative;
  top: -4px;
  left: 5px;
}

#login_iframe_container.new-login.no-bg .login-subtitle::before {
  background-color: #999999;
}

#login_iframe_container.new-login.no-bg .login-subtitle::after {
  background-color: #999999;
}

#login_iframe_container.new-login .close-btn {
  position: absolute;
  top: 20px;
  right: 20px;
  width: 12px;
  height: 12px;
  background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -59px -10px;
  background-size: 81px 91px;
  cursor: pointer;
}

#login_iframe_container.new-login .login-btn {
  width: 220px;
  height: 47px;
  border-radius: 24px;
  border: solid 1px #dddddd;
  background-color: #ffffff;
  margin: 0 auto;
  margin-top: 28px;
  position: relative;
  display: block;
}

#login_iframe_container.new-login .login-btn .login-icon {
  position: absolute;
}

#login_iframe_container.new-login .login-btn .login-text {
  width: 61px;
  height: 47px;
  line-height: 47px;
  vertical-align: middle;
  font-size: 15px;
  letter-spacing: 0.1px;
  color: #666666;
  position: absolute;
  right: 62px;
}

#login_iframe_container.new-login .login-btn.qq-btn .login-icon {
  width: 22px;
  height: 27px;
  top: 10px;
  left: 67px;
  background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -10px -54px;
  background-size: 81px 91px;
}

#login_iframe_container.new-login .login-btn.qq-btn .login-text {
  right: 59px;
}

#login_iframe_container.new-login .login-btn.wechat-btn .login-icon {
  width: 29px;
  height: 24px;
  top: 12px;
  left: 62px;
  background: url(//web.archive.org/web/20221028205211/https://hhy.sogoucdn.com/js/common/hhy/login-sprites_e3853e5.png) -10px -10px;
  background-size: 81px 91px;
}</style><style>/* -- container -- */
.rodal,
.rodal-mask {
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 100;
}

.rodal {
    position: fixed;
}

/* -- mask -- */
.rodal-mask {
    position: fixed;
    background: rgba(0, 0, 0, .5);
}

/* -- dialog -- */
.rodal-dialog {
    position: absolute;
    z-index: 101;
    background: #fff;
    border-radius: 3px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, .2);
}

.rodal-center {
    top: 50%;
    transform: translateY(-50%);
    left: 0;
    right: 0;
    margin: 0 auto;
}

.rodal-bottom {
    left: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

.rodal-top {
    left: 0;
    right: 0;
    top: 0;
    margin: auto;
}

.rodal-left {
    top: 0;
    left: 0;
    bottom: 0;
    margin: auto;
}

.rodal-right {
    top: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

/* -- close button -- */
.rodal-close {
    position: absolute;
    cursor: pointer;
    top: 16px;
    right: 16px;
    width: 16px;
    height: 16px;
}

.rodal-close:before,
.rodal-close:after {
    position: absolute;
    content: '';
    height: 2px;
    width: 100%;
    top: 50%;
    left: 0;
    margin-top: -1px;
    background: #999;
    border-radius: 100%;
    -webkit-transition: background .2s;
    transition: background .2s;
}

.rodal-close:before {
    -webkit-transform: rotate(45deg);
    transform: rotate(45deg);
}

.rodal-close:after {
    -webkit-transform: rotate(-45deg);
    transform: rotate(-45deg);
}

.rodal-close:hover:before,
.rodal-close:hover:after {
    background: #333;
}

/* -- fade -- */
/* @-webkit-keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

@keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

.rodal-fade-enter {
    -webkit-animation: rodal-fade-enter both ease-in;
    animation: rodal-fade-enter both ease-in;
} */

@-webkit-keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

@keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

.rodal-fade-leave {
    -webkit-animation: rodal-fade-leave both ease-out;
    animation: rodal-fade-leave both ease-out;
}

/* -- zoom -- */
@-webkit-keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-enter {
    -webkit-animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-leave {
    -webkit-animation: rodal-zoom-leave both;
    animation: rodal-zoom-leave both;
}

/* -- slideDown -- */
@-webkit-keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-enter {
    -webkit-animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-leave {
    -webkit-animation: rodal-slideDown-leave both;
    animation: rodal-slideDown-leave both;
}

/* -- slideLeft -- */
@-webkit-keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-enter {
    -webkit-animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-leave {
    -webkit-animation: rodal-slideLeft-leave both;
    animation: rodal-slideLeft-leave both;
}

/* -- slideRight -- */
@-webkit-keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-enter {
    -webkit-animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-leave {
    -webkit-animation: rodal-slideRight-leave both;
    animation: rodal-slideRight-leave both;
}

/* -- slideUp -- */
@-webkit-keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-enter {
    -webkit-animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
    animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
}

@-webkit-keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-leave {
    -webkit-animation: rodal-slideUp-leave both;
    animation: rodal-slideUp-leave both;
}

/* -- flip -- */
@-webkit-keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

@keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

.rodal-flip-enter {
    -webkit-animation: rodal-flip-enter both ease-in;
    animation: rodal-flip-enter both ease-in;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

@-webkit-keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

@keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

.rodal-flip-leave {
    -webkit-animation: rodal-flip-leave both;
    animation: rodal-flip-leave both;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

/* -- rotate -- */
@-webkit-keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-enter {
    -webkit-animation: rodal-rotate-enter both;
    animation: rodal-rotate-enter both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

@-webkit-keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-leave {
    -webkit-animation: rodal-rotate-leave both;
    animation: rodal-rotate-leave both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

/* -- door -- */
@-webkit-keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

@keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

.rodal-door-enter {
    -webkit-animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

@keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

.rodal-door-leave {
    -webkit-animation: rodal-door-leave both;
    animation: rodal-door-leave both;
}</style><style>/* -- container -- */
.rodal,
.rodal-mask {
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: 100;
}

.rodal {
    position: fixed;
}

/* -- mask -- */
.rodal-mask {
    position: fixed;
    background: rgba(0, 0, 0, .5);
}

/* -- dialog -- */
.rodal-dialog {
    position: absolute;
    z-index: 101;
    background: #fff;
    border-radius: 3px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, .2);
}

.rodal-center {
    top: 50%;
    transform: translateY(-50%);
    left: 0;
    right: 0;
    margin: 0 auto;
}

.rodal-bottom {
    left: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

.rodal-top {
    left: 0;
    right: 0;
    top: 0;
    margin: auto;
}

.rodal-left {
    top: 0;
    left: 0;
    bottom: 0;
    margin: auto;
}

.rodal-right {
    top: 0;
    right: 0;
    bottom: 0;
    margin: auto;
}

/* -- close button -- */
.rodal-close {
    position: absolute;
    cursor: pointer;
    top: 16px;
    right: 16px;
    width: 16px;
    height: 16px;
}

.rodal-close:before,
.rodal-close:after {
    position: absolute;
    content: '';
    height: 2px;
    width: 100%;
    top: 50%;
    left: 0;
    margin-top: -1px;
    background: #999;
    border-radius: 100%;
    -webkit-transition: background .2s;
    transition: background .2s;
}

.rodal-close:before {
    -webkit-transform: rotate(45deg);
    transform: rotate(45deg);
}

.rodal-close:after {
    -webkit-transform: rotate(-45deg);
    transform: rotate(-45deg);
}

.rodal-close:hover:before,
.rodal-close:hover:after {
    background: #333;
}

/* -- fade -- */
/* @-webkit-keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

@keyframes rodal-fade-enter {
    from {
        opacity: 0;
    }
}

.rodal-fade-enter {
    -webkit-animation: rodal-fade-enter both ease-in;
    animation: rodal-fade-enter both ease-in;
} */

@-webkit-keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

@keyframes rodal-fade-leave {
    to {
        opacity: 0
    }
}

.rodal-fade-leave {
    -webkit-animation: rodal-fade-leave both ease-out;
    animation: rodal-fade-leave both ease-out;
}

/* -- zoom -- */
@-webkit-keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-enter {
    from {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-enter {
    -webkit-animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-zoom-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

@keyframes rodal-zoom-leave {
    to {
        -webkit-transform: scale3d(.3, .3, .3);
        transform: scale3d(.3, .3, .3);
    }
}

.rodal-zoom-leave {
    -webkit-animation: rodal-zoom-leave both;
    animation: rodal-zoom-leave both;
}

/* -- slideDown -- */
@-webkit-keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-enter {
    from {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-enter {
    -webkit-animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideDown-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

@keyframes rodal-slideDown-leave {
    to {
        -webkit-transform: translate3d(0, -100px, 0);
        transform: translate3d(0, -100px, 0);
    }
}

.rodal-slideDown-leave {
    -webkit-animation: rodal-slideDown-leave both;
    animation: rodal-slideDown-leave both;
}

/* -- slideLeft -- */
@-webkit-keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-enter {
    from {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-enter {
    -webkit-animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideLeft-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

@keyframes rodal-slideLeft-leave {
    to {
        -webkit-transform: translate3d(-150px, 0, 0);
        transform: translate3d(-150px, 0, 0);
    }
}

.rodal-slideLeft-leave {
    -webkit-animation: rodal-slideLeft-leave both;
    animation: rodal-slideLeft-leave both;
}

/* -- slideRight -- */
@-webkit-keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-enter {
    from {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-enter {
    -webkit-animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-slideRight-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

@keyframes rodal-slideRight-leave {
    to {
        -webkit-transform: translate3d(150px, 0, 0);
        transform: translate3d(150px, 0, 0);
    }
}

.rodal-slideRight-leave {
    -webkit-animation: rodal-slideRight-leave both;
    animation: rodal-slideRight-leave both;
}

/* -- slideUp -- */
@-webkit-keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-enter {
    from {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-enter {
    -webkit-animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
    animation: rodal-slideUp-enter both cubic-bezier(0.23, 1, 0.32, 1);
}

@-webkit-keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

@keyframes rodal-slideUp-leave {
    to {
        -webkit-transform: translate3d(0, 100px, 0);
        transform: translate3d(0, 100px, 0);
    }
}

.rodal-slideUp-leave {
    -webkit-animation: rodal-slideUp-leave both;
    animation: rodal-slideUp-leave both;
}

/* -- flip -- */
@-webkit-keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

@keyframes rodal-flip-enter {
    from {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 60deg);
    }

    70% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }
}

.rodal-flip-enter {
    -webkit-animation: rodal-flip-enter both ease-in;
    animation: rodal-flip-enter both ease-in;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

@-webkit-keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

@keyframes rodal-flip-leave {
    from {
        -webkit-transform: perspective(400px);
        transform: perspective(400px);
    }

    30% {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
        transform: perspective(400px) rotate3d(1, 0, 0, -15deg);
    }

    to {
        -webkit-transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
        transform: perspective(400px) rotate3d(1, 0, 0, 45deg);
    }
}

.rodal-flip-leave {
    -webkit-animation: rodal-flip-leave both;
    animation: rodal-flip-leave both;
    -webkit-backface-visibility: visible !important;
    backface-visibility: visible !important;
}

/* -- rotate -- */
@-webkit-keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-enter {
    from {
        -webkit-transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, -180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-enter {
    -webkit-animation: rodal-rotate-enter both;
    animation: rodal-rotate-enter both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

@-webkit-keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

@keyframes rodal-rotate-leave {
    to {
        -webkit-transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
        transform: rotate3d(0, 0, 1, 180deg) scale3d(.3, .3, .3);
    }
}

.rodal-rotate-leave {
    -webkit-animation: rodal-rotate-leave both;
    animation: rodal-rotate-leave both;
    -webkit-transform-origin: center;
    transform-origin: center;
}

/* -- door -- */
@-webkit-keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

@keyframes rodal-door-enter {
    from {
        -webkit-transform: scale3d(0, 1, 1);
        transform: scale3d(0, 1, 1);
    }
}

.rodal-door-enter {
    -webkit-animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
    animation: rodal-door-enter both cubic-bezier(0.4, 0, 0, 1.5);
}

@-webkit-keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

@keyframes rodal-door-leave {
    60% {
        -webkit-transform: scale3d(.01, 1, 1);
        transform: scale3d(.01, 1, 1);
    }

    to {
        -webkit-transform: scale3d(0, 1, .1);
        transform: scale3d(0, 1, .1);
    }
}

.rodal-door-leave {
    -webkit-animation: rodal-door-leave both;
    animation: rodal-door-leave both;
}</style></head><body class=""><!-- BEGIN WAYBACK TOOLBAR INSERT -->
<style type="text/css">
body {
  margin-top:0 !important;
  padding-top:0 !important;
  /*min-width:800px !important;*/
}
</style>
<script>__wm.rw(0);</script>
<div id="wm-ipp-base" lang="en" style="display: block; direction: ltr;">
</div><div id="wm-ipp-print">The Wayback Machine - https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm</div>
<script type="text/javascript">//<![CDATA[
__wm.bt(675,27,25,2,"web","https://baike.sogou.com/kexue/d10951.htm","20221028212226",1996,"/_static/",["/_static/css/banner-styles.css?v=S1zqJCYt","/_static/css/iconochive.css?v=qtvMKcIJ"], false);
  __wm.rw(1);
//]]></script>
<!-- END WAYBACK TOOLBAR INSERT --><script>window._gtag=window._gtag||{};window._gtag.shouldGrayed = false;if ('09ad3bbe4d594e3093aee0af22370952') window._gtag.traceId = '09ad3bbe4d594e3093aee0af22370952';if ({"illegality":true}) window.userInfo = {"illegality":true};</script><div class="topnavbox"><ul class="topnav"><li><a href="https://web.archive.org/web/20221028212226/https://www.sogou.com/web?query=">网页</a></li><li><a href="https://web.archive.org/web/20221028212226/https://weixin.sogou.com/weixin?p=75351201">微信</a></li><li><a href="https://web.archive.org/web/20221028212226/https://zhihu.sogou.com/zhihu?p=75351218">知乎</a></li><li><a href="https://web.archive.org/web/20221028212226/https://pic.sogou.com/pics?query=">图片</a></li><li><a href="https://web.archive.org/web/20221028212226/https://v.sogou.com/v?query=">视频</a></li><li><a href="https://web.archive.org/web/20221028212226/https://mingyi.sogou.com/">医疗</a></li><li class="cur"><strong>科学</strong></li><li><a href="https://web.archive.org/web/20221028212226/https://hanyu.sogou.com/">汉语</a></li><li><a href="https://web.archive.org/web/20221028212226/https://wenwen.sogou.com/">问问</a></li><li><a href="https://web.archive.org/web/20221028212226/https://www.sogou.com/docs/more.htm">更多<span class="topraquo">»</span></a></li></ul></div><div id="header"><div class="header-wrap"><a class="header-logo" href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue"></a><div class="header-search"><div class="querybox" id="suggBox"><form><input id="searchInput" class="query" type="text" placeholder="搜科学领域专业百科词条" name="query" autocomplete="off" value=""><a href="javascript:;" class="query-search"></a></form></div></div><div class="header-rgt"><span class="btn-header-rgt btn-edit" id="editLemma">创建</span><div class="header-user no-login"></div></div></div></div><div class="fixed-placeholder" style="visibility:none"></div><div id="container" class=""><div class="content lemma-level1"><div class="detail-title" id="abstract-title"><h1>语音识别</h1><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#!" class="detail-edit">编辑</a></div><div class="section_content" data-id="54416935730864642"><div class="text_img ed_imgfloat_right"><a class="ed_image_link" data-src="https://img02.sogoucdn.com/app/a/200698/800_600_20200910174330-1366981798.gif" data-bigsrc="https://img02.sogoucdn.com/app/a/200698/800_600_20200910174330-1366981798.gif?width=227&amp;height=174&amp;titlename=&amp;w=800&amp;h=600" title="点击查看大图" data-w="800" data-h="600" style="background-image:url(https://web.archive.org/web/20221028212226im_/https://img02.sogoucdn.com/app/a/200698/800_600_20200910174330-1366981798.gif)" href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#!"></a></div><div><p><b>语音识别</b>是计算语言学的跨学科子领域，利用其开发方法和技术，能够通过计算机识别和翻译口语。也被称为<b>自动语音识别技术</b>（<b>ASR</b>)，<b>计算机语音识别</b>或<b>语音到文本</b>（<b>STT</b>)技术。它融合了语言学、计算机科学和电气工程领域的知识和研究。</p>
<p>一些语音识别系统需要“训练”(也称为“注册”)，其中个体说话者将文本或孤立的词汇读入系统。该系统分析该人的特定声音，并使用它来微调对该人语音的识别，从而提高准确性。不使用训练的系统被称为“说话者无关”<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_1" class="kx_ref">[1]</a></sup>系统。使用训练的系统被称为“说话者相关”。</p>
<p>语音识别应用包括语音用户界面，例如语音拨号(例如“呼叫总部”)、呼叫路由(例如“我想打对方付费电话”)、多用户设备控制、搜索(例如找到说出特定单词的播客)、简单的数据输入(例如输入信用卡号码)、结构化文档的准备(例如放射学报告)、确定说话者特征，<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_2" class="kx_ref">[2]</a></sup>语音到文本处理(例如文字处理器或电子邮件)和飞机(通常称为直接语音输入)。</p>
<p>术语 voice recognition<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_3" class="kx_ref">[3]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_4" class="kx_ref">[4]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_5" class="kx_ref">[5]</a></sup>或者speaker identification<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_6" class="kx_ref">[6]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_7" class="kx_ref">[7]</a></sup>指的是识别说话者，而不是他们在说什么。识别说话人可以简化为在已经对特定人语音训练的系统中翻译语音的任务，或者作为安全过程的一部分来验证说话人的身份。</p>
<p>从技术角度来看，语音识别有着悠久的历史，并且经历了几次重大创新浪潮。近年来，该领域受益于深度学习和大数据技术的进步。这些进步不仅体现在该领域发表的学术论文激增上，更重要的是体现在世界范围内的各行各业在设计和部署语音识别系统时均采用了各种深度学习方法。</p></div></div><div id="catalog"><h2 class="title2">目录<a href="javascript:" class="detail-edit">编辑</a></h2><div class="catalog_wrap" style=""><ul class="catalog_list col3"><li><span class="order">1</span><a href="javascript:" data-level="1" data-id="14996178911363597">历史</a></li><li class="secondary_catalog"><span>1.1 </span><a href="javascript:" data-id="14996178911363597">1970年前</a></li><li class="secondary_catalog"><span>1.2 </span><a href="javascript:" data-id="14996178911363597">1970-1990年</a></li><li class="secondary_catalog"><span>1.3 </span><a href="javascript:" data-id="14996178911363597">实用语音识别</a></li><li><span class="order">2</span><a href="javascript:" data-level="1" data-id="14996178911363598">模型、方法和算法</a></li><li class="secondary_catalog"><span>2.1 </span><a href="javascript:" data-id="14996178911363598">隐马尔可夫模型</a></li><li class="secondary_catalog"><span>2.2 </span><a href="javascript:" data-id="14996178911363598">基于动态时间规整(DTW)的语音识别</a></li><li class="secondary_catalog"><span>2.3 </span><a href="javascript:" data-id="14996178911363598">神经网络</a></li></ul><ul class="catalog_list col3"><li class="secondary_catalog"><span>2.4 </span><a href="javascript:" data-id="14996178911363598">端到端自动语音识别</a></li><li><span class="order">3</span><a href="javascript:" data-level="1" data-id="14996178928140812">应用程序</a></li><li class="secondary_catalog"><span>3.1 </span><a href="javascript:" data-id="14996178928140812">车载系统</a></li><li class="secondary_catalog"><span>3.2 </span><a href="javascript:" data-id="14996178928140812">卫生保健</a></li><li class="secondary_catalog"><span>3.3 </span><a href="javascript:" data-id="14996178928140812">军队</a></li><li class="secondary_catalog"><span>3.4 </span><a href="javascript:" data-id="14996178928140812">电话和其他领域</a></li><li class="secondary_catalog"><span>3.5 </span><a href="javascript:" data-id="14996178928140812">在教育和日常生活中的使用</a></li><li class="secondary_catalog"><span>3.6 </span><a href="javascript:" data-id="14996178928140812">残疾人</a></li><li class="secondary_catalog"><span>3.7 </span><a href="javascript:" data-id="14996178928140812">进一步的应用</a></li></ul><ul class="catalog_list col3"><li><span class="order">4</span><a href="javascript:" data-level="1" data-id="14996178928140813">表演</a></li><li class="secondary_catalog"><span>4.1 </span><a href="javascript:" data-id="14996178928140813">准确</a></li><li class="secondary_catalog"><span>4.2 </span><a href="javascript:" data-id="14996178928140813">安全问题</a></li><li><span class="order">5</span><a href="javascript:" data-level="1" data-id="14996178928140814">更多信息</a></li><li class="secondary_catalog"><span>5.1 </span><a href="javascript:" data-id="14996178928140814">会议和期刊</a></li><li class="secondary_catalog"><span>5.2 </span><a href="javascript:" data-id="14996178928140814">书籍</a></li><li class="secondary_catalog"><span>5.3 </span><a href="javascript:" data-id="14996178928140814">软件</a></li><li><span class="order">6</span><a href="javascript:" data-level="1" data-id="references">参考文献</a></li></ul></div></div><div id="paragraphs"><div><div id="par_14996178911363597"><h2 class="title">1 历史<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>增长的关键领域是：词汇量、说话人独立性和处理速度。</p><p></p><h3>1.1 <span>1970年前</span></h3><p></p><p>
 </p><ul>
  <li><b>1952年–</b>三位贝尔实验室的研究人员，斯蒂芬·巴拉克，<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_8" class="kx_ref">[8]</a></sup>R.比德勒夫和戴维斯建立了一个名为“Audrey”的系统<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_9" class="kx_ref">[9]</a></sup>，用于单人语音数字识别。们的系统将共振峰定位在每个话语的功率谱中。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_10" class="kx_ref">[10]</a></sup></li>
  <li><b>1960</b>–贡纳·范特开发并出版了语音产生的源过滤模型。</li>
  <li><b>1962</b>–IBM在1962年的世界博览会上展示了其16字的“Shoebox”机器的语音识别能力。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_11" class="kx_ref">[11]</a></sup></li>
  <li><b>1969</b>–贝尔实验室的资金枯竭了好几年，1969年，颇具影响力的约翰·皮尔斯写了一封公开信，批评并平息了语音识别研究。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_12" class="kx_ref">[12]</a></sup>撤资一直持续到皮尔斯退休，詹姆斯·弗拉纳根的接任。</li>
 </ul><p></p><p>20世纪60年代末，罗杰·瑞迪是第一个以斯坦福大学研究生身份持续进行语音识别研究的人。以前的系统要求用户在讲出每个单词后稍作等待。瑞迪的系统发出了用于玩国际象棋的口头命令。</p><p>大约在这个时候，苏联研究人员发明了“动态时间规整（DTW）”算法，并使用它创建了一个能够对200个单词进行操作的识别器。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_13" class="kx_ref">[13]</a></sup>DTW通过将语音分成短帧，例如10ms的片段，并将每个帧作为一个单元来处理。尽管DTW被后来的算法取代，但这项技术仍在继续使用。在这个时期内，实现说话人的独立性仍未得到解决。</p><p></p><h3>1.2 <span>1970-1990年</span></h3><p></p><p>
 </p><ul>
  <li><b>1971</b>–美国国防部高级研究计划局（DARPA）资助了五年的<i>语音识别研究</i>，语音识别研究寻求最小词汇量为1000个单词。他们认为言语理解是在语音识别方面取得进展的关键；后来证明这是不真实的。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_14" class="kx_ref">[14]</a></sup> BBN 、IBM 、卡耐基梅隆大学和斯坦福研究所都参加了该项目。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_15" class="kx_ref">[15]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_16" class="kx_ref">[16]</a></sup>这篇复兴的语音识别研究发表在约翰·皮尔斯的信中。</li>
 </ul><p></p><p>
 </p><ul>
  <li><b>1972–</b>IEEE声学、语音和信号处理小组在牛顿市(马萨诸塞州)举行了一次会议。</li>
  <li><b>1976–</b>第一届ICASSP在费城举行，此后该市一直是发表语音识别研究成果的主要场所。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_17" class="kx_ref">[17]</a></sup></li>
 </ul><p></p><p>在20世纪60年代末伦纳德·鲍姆在国防分析研究所开发了马尔可夫链的数学计算方法。十年后，在CMU，Raj Reddy的学生James Baker和Janet M. Baker开始使用隐马尔可夫模型（HMM）进行语音识别。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_18" class="kx_ref">[18]</a></sup>James Baker在本科教育期间从国防分析研究所的暑期工作中了解到HMM。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_19" class="kx_ref">[19]</a></sup>HMM的使用允许研究人员将不同的知识来源，如声学、语言和语法，结合在一个统一的概率模型中。</p><p>
 </p><ul>
  <li>到了<b>20世纪80年代中期</b>IBM的弗雷德·杰利内克团队创造了一种叫做Tangora的声控打字机，可以处理20000个词汇<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_20" class="kx_ref">[20]</a></sup>，杰利内克的统计方法不太重视模拟人脑处理和理解语音的方式，而是倾向于使用像隐马尔科夫模型这样的统计建模技术。(杰利内克的团队独立发现了隐马尔科夫模型在语音中的应用。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_19" class="kx_ref">[19]</a></sup>)这引起了语言学家的争议，因为隐马尔科夫模型过于简单，无法解释人类语言的许多特征。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_21" class="kx_ref">[21]</a></sup>然而，隐马尔可夫模型被证明是一种非常有用的语音建模方法，并在20世纪80年代取代了DTW成为主流的语音识别算法。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_22" class="kx_ref">[22]</a></sup></li>
 </ul><p></p><p>
 </p><ul>
  <li><b>1982年</b>–由詹姆斯和珍妮特·贝克创建的Dragon Systems<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_23" class="kx_ref">[23]</a></sup>是IBM的少数竞争对手之一。</li>
 </ul><p></p><p></p><h3>1.3 <span>实用语音识别</span></h3><p></p><p>20世纪80年代还出现了n-gram 语言模型。</p><p>
 </p><ul>
  <li><b>1987年</b>– 退避模型允许语言模型使用多个长度的n-gram，CSELT使用隐马尔可夫模型来识别语言。</li>
 </ul><p></p><p>该领域的大部分进展归功于计算机能力的迅速提高。在1976年美国国防部高级研究计划局(DARPA)的项目结束时，研究人员可以使用的最好的计算机是内存为4 MB的PDP-10 。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_21" class="kx_ref">[21]</a></sup> 其解码30秒的语音可能需要100分钟。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_24" class="kx_ref">[24]</a></sup> </p><p>两种实用产品是:</p><p>
 </p><ul>
  <li><b>1987年</b>– Kurzweil Applied Intelligence公司的识别器</li>
  <li><b>1990年</b>–Dragon Dictate，1990年发布的消费品<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_25" class="kx_ref">[25]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_26" class="kx_ref">[26]</a></sup> AT &amp; amp；在1992年部署了语音识别呼叫处理服务来进行路由电话呼叫，而不需要使用人工操作员。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_27" class="kx_ref">[27]</a></sup>这项技术是由劳伦斯·拉比纳和贝尔实验室的其他人开发的。</li>
 </ul><p></p><p>至此，典型的商业语音识别系统的词汇量已经超过了人类的平均词汇量。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_21" class="kx_ref">[21]</a></sup>Raj Reddy以前的学生黄学东，在CMU开发了Sphinx-II系统。Sphinx-II系统是第一个独立于说话人、词汇量大、连续语音识别的系统，在1992年美国国防部高级研究计划局的评估中表现最佳。处理大词汇量的连续语音是语音识别历史上的一个重要里程碑。1993年，黄学东接着找到了微软语音识别小组，Raj Reddy的学生李开复加入苹果公司，帮助开发了一款名为Casper的苹果电脑语音接口原型。</p><p>总部位于比利时的语音识别公司Lernout＆Hauspie收购了其他几家公司，包括1997年的Kurzweil Applied Intelligence公司和2000年的Dragon Systems公司。Windows XP 操作系统中使用了L&amp;H语音技术。L&amp;H一直是行业领导者，直到2001年一桩会计丑闻结束了公司的好运。L&amp;H的语音技术被ScanSoft收购，该公司于2005年成为 Nuance 。苹果最初从Nuance获得授权，为其数字助理Siri 提供语音识别功能。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_28" class="kx_ref">[28]</a></sup> </p><p><strong>2000年代</strong></p><p>在2000年代，美国国防部高级研究计划局赞助了两个语音识别项目：2002年的有效、可负担的可重复使用的语音到文本转换(EARS)和全球自主语言开发(GALE)。四个小组参加了EARS项目：IBM，由BBN和LIMSI以及匹兹堡大学，剑桥大学团队，以及一个由ICSI，斯里兰卡和华盛顿大学组成的团队。EARS资助收集了交换台电话语音语料库，其中包含来自500多名发言者的260小时录音会话。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_29" class="kx_ref">[29]</a></sup>GALE计划的重点是阿拉伯语和普通话广播新闻演讲。谷歌2007年，Nuance聘请了一些研究人员进行语音识别。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_30" class="kx_ref">[30]</a></sup>第一个产品是GOOG-411 ，一种基于电话的目录服务。GOOG-411的录音产生了有价值的数据，帮助谷歌改进了他们的识别系统。 Google语音搜索现在支持30多种语言。</p><p>在美国国家安全局至少从2006年就开始利用一种语音识别技术对关键词进行识别。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_31" class="kx_ref">[31]</a></sup>这项技术允许分析师搜索大量记录的对话，并隔离关键词。可以对记录进行索引，分析师可以在数据库上查找感兴趣的对话。一些政府研究计划侧重于语音识别的智能应用，例如DARPA的EARS计划和IARPA 的 Babel 计划。</p><p>在21世纪初，语音识别仍然由传统的方法主导，如隐马尔可夫模型结合前馈人工神经网络。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_32" class="kx_ref">[32]</a></sup>然而，今天，语音识别的许多方面已经被一种叫做长短期记忆 (LSTM)的深度学习方法所取代，这是一种由Sepp Hochreiter 和Jürgen Schmidhuber 于1997年出版的递归神经网络。LSTM神经网络避免了梯度消失问题，可以进行“深度学习”任务<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_33" class="kx_ref">[33]</a></sup>，这需要对发生在数千个离散时间步骤前的事件进行记忆，这对语音识别很重要。大约在2007年，LSTM接受了联结主义时间分类(CTC)的训练<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_34" class="kx_ref">[34]</a></sup>，在某些应用中开始超越传统的语音识别。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_35" class="kx_ref">[35]</a></sup>据报道，2015年，经过CTC培训的LSTM让谷歌的语音识别性能大幅提升了49%，现在所有智能手机用户都可以通过谷歌语音获得这一服务。 </p><p>声学模型使用的深度前馈(非递归)网络是由Geoffrey Hinton和他在多伦多大学的学生Li Deng以及微软研究院的同事，在2009年下半年提出的<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_36" class="kx_ref">[36]</a></sup>，最初是在微软和多伦多大学的合作中，后来扩展到包括IBM和谷歌(因为在他们2012年的综述论文中有“四个研究小组的共同观点”副标题)。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_37" class="kx_ref">[37]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_38" class="kx_ref">[38]</a></sup>微软的一名研究主管称这一创新是“自1979年以来准确性方面最戏剧性的变化”。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_39" class="kx_ref">[39]</a></sup>与过去几十年稳步递增的改进相比，深度学习的应用将单词错误率降低了30%。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_39" class="kx_ref">[39]</a></sup>这项创新很快被整个领域所采用。研究人员也开始将深度学习技术用于语言建模。</p><p>在语音识别的漫长历史中，人工神经网络的浅层和深层(例如递归网络)。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_40" class="kx_ref">[40]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_41" class="kx_ref">[41]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_42" class="kx_ref">[42]</a></sup>但是这些方法从来没有赢过基于区分性训练的语音生成的高斯混合模型/隐马尔可夫模型(GMM-HMM)技术。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_43" class="kx_ref">[43]</a></sup>1990年，对一些关键的困难进行了方法分析，包括梯度递减<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_44" class="kx_ref">[44]</a></sup>和弱时间相关结构。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_45" class="kx_ref">[45]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_46" class="kx_ref">[46]</a></sup>所有这些困难都是对早期缺乏大训练数据和大计算能力的补充。大多数理解这些障碍的语音识别研究人员因此离开神经网络，转而寻求生成模型方法，直到从2009-2010年左右，由于深度学习克服了所有这些困难，因此神经网络开始复苏。Hinton和Deng等人回顾了这段历史的一部分，讲述了他们如何相互协作，然后与四个群体(多伦多大学、微软、谷歌和IBM)的同事协作，引发了深度前馈神经网络在语音识别中应用的复兴。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_38" class="kx_ref">[38]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_47" class="kx_ref">[47]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_48" class="kx_ref">[48]</a></sup> </p><p><strong>2010年代</strong></p><p>到2010年代初语言识别，也称为语音识别<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_49" class="kx_ref">[49]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_50" class="kx_ref">[50]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_51" class="kx_ref">[51]</a></sup>，明显区别于说话人识别，而说话者独立被认为是一个重大突破。在那之前，系统需要一个“训练”期。一个1987年的洋娃娃广告上写着这样一句话:“终于，那个能理解你的洋娃娃。”它被描述为“哪些孩子可以接受训练以响应他们的声音”。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_11" class="kx_ref">[11]</a></sup> </p></div></div><div id="par_14996178911363598"><h2 class="title">2 模型、方法和算法<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>声学模型和语言模型都是当今基于统计的语音识别算法的重要组成部分。隐马尔可夫模型(HMM)在许多系统中被广泛使用。语言建模也用于许多其他自然语言处理应用，如文档分类或统计机器翻译。</p><p></p><h3>2.1 <span>隐马尔可夫模型</span></h3><p></p><p>现代通用语音识别系统普遍基于隐马尔可夫模型，是输出一系列符号或数量序列的统计模型。隐马尔科夫模型被用于语音识别，因为语音信号可以被视为分段平稳信号或短时平稳信号。在短时间尺度(例如10毫秒)内，语音可以近似为平稳过程。语音可以被认为是一个随机的马尔可夫模型。</p><p>隐马尔科夫模型流行的另一个原因是，它们可以自动训练，使用起来简单且计算可行。在语音识别中，隐马尔可夫模型将输出一系列n维实值向量(<i>n是</i>整数，例如10)，每10毫秒输出一个。向量由倒谱系数组成，这些系数是通过对短时间语音窗口进行傅立叶变换并使用余弦变换对频谱进行去相关，然后取其中第一个(最重要的)系数而获得的。隐马尔可夫模型在每个状态下均倾向于具有对角协方差高斯混合的统计分布，这将给出每个观测向量的可能性。每个单词，或者(对于更通用的语音识别系统)每个音素，将具有不同的输出分布；一个单词或音素序列的隐马尔可夫模型是通过将单独训练的单词和音素的隐马尔可夫模型串联而成的。</p><p>上述是最常见的基于隐马尔可夫模型的语音识别方法的核心要素。现代语音识别系统使用多种标准技术的各种组合，以便与上述基本方法进行对比改善。典型的大词汇系统需要音素的上下文依赖（因此具有不同左右上下文的音素具有与HMM状态不同的实现）；它会使用倒谱归一化来规范不同的扬声器和录音条件；为了进一步使说话人归一化，可以使用声道长度归一化(VTLN)进行男女归一化，此外，最大似然线性回归(MLLR)用于更一般的扬声器改编。这些特征会有所谓的δ和δ-δ系数以捕获语音动态，此外还可以使用异方差线性判别分析(HLDA)；或者可以跳过δ和δ-δ系数并使用插接和基于LDA的投影，然后通过异方差线性判别分析或全局半连接协方差进行变换（也称为最大似然线性变换，或MLLT）(也称为最大似然线性变换，MLLT)。许多系统使用所谓的判别训练技术，这种技术省去了HMM参数估计的纯统计方法，而是优化了训练数据的一些分类相关的测量。例子有：最大互信息（MMI），最小分类错误（MCE）和最小电话错误（MPE）。</p><p>语音的解码(当系统呈现新的话语并且必须计算最可能的源句子时的术语)可能使用维特比算法寻找最佳路径，这里有两种选择，一种是动态创建包含声学和语言模型信息的组合隐马尔可夫模型，另一种是预先静态组合的隐马尔可夫模型(有限状态传感器或FST方法)。</p><p>解码的一个可能的改进是保留一组好的候选项，而不仅仅是保留最好的候选项，并且使用更好的评分函数(重新评分)对这些优秀的候选语句进行评分，这样我们就可以根据这个精确的分数选出最好的候选语句。候选集可以保存为列表(即最佳列表方法)或作为模型的子集(格子)。重新评分通常是通过尝试最小化贝叶斯风险<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_52" class="kx_ref">[52]</a></sup>(或其近似)实现的：我们不是以最大概率选取源句子，而是试图选取相对于所有可能的转录，选择最小化给定损失函数的句子(即，我们选取以其估计概率加权的，并与其他句子平均距离最小化的候选者)。损失函数通常是Levenshtein距离，对于特定的任务它的数值是不同的；当然，可以删减一组可能的转录语句以保持易处理性。已经设计了有效的算法来重新划分表示为加权有限状态换能器的格子，其中编辑距离表示为验证某些假设的有限状态换能器。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_53" class="kx_ref">[53]</a></sup> </p><p></p><h3>2.2 <span>基于动态时间规整(DTW)的语音识别</span></h3><p></p><p>动态时间规整是一种历史上用于语音识别的方法，但现在已经在很大程度上被更成功的基于隐马尔可夫模型的方法所取代。</p><p>动态时间规整是一种用于测量可能随时间或速度变化的两个序列之间相似性的算法。例如，即使在一个视频中的人走得慢，而在另一个视频中走得快，或者即使在一次观察过程中有加速和减速，也可以检测到行走模式的相似性。DTW已经应用于视频、音频和图形中–事实上，任何可以转化为线性表示的数据都可以用DTW进行分析。</p><p>一个众所周知的应用是自动语音识别，以应对不同的说话速度。一般来说，这是一种允许计算机在具有特定限制的两个给定序列(例如时间序列)之间找到最佳匹配的方法。也就是说，序列被非线性地“规整”以相互匹配。这种序列比对方法经常在隐马尔可夫模型中使用。</p><p></p><h3>2.3 <span>神经网络</span></h3><p></p><p>神经网络是20世纪80年代后期在ASR中出现的一种有吸引力的声学建模方法。从那时起，神经网络已经逐渐用于语音识别的许多方面，例如音素分类，<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_54" class="kx_ref">[54]</a></sup>孤立单词识别，<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_55" class="kx_ref">[55]</a></sup>视听语音识别、视听说话者识别和说话者适应。</p><p>与隐马尔科夫模型相比，神经网络对特征统计特性的明确假设较少，并且具有多种特性，使得它们成为语音识别领域中具有吸引力的识别模型。当用于估计语音特征片段的概率时，神经网络允许以自然和有效的方式进行判别训练。然而，尽管它们在分类短时间单位（如个体音素和孤立单词）方面有效，<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_56" class="kx_ref">[56]</a></sup> 但早期的神经网络很少成功地完成连续识别任务，这是由于它们对时间相关性建模的能力有限。</p><p>这种限制的一种解决方法是在基于HMM的识别之前使用神经网络对语音数据进行预处理，特征变换或降维<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_57" class="kx_ref">[57]</a></sup>。近年来，LSTM和相关的递归神经网络<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_58" class="kx_ref">[58]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_58" class="kx_ref">[58]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_58" class="kx_ref">[58]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_59" class="kx_ref">[59]</a></sup>和时延神经网络(TDNN)<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_60" class="kx_ref">[60]</a></sup>在这一领域的表现已经被证明是有效的。</p><p><strong>深度前馈和递归神经网络</strong></p><p>深度神经网络与自动去噪编码器<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_61" class="kx_ref">[61]</a></sup> 也正处于研究中。深度前馈神经网络(DNN)是一种在输入和输出层之间具有多个隐藏单元层的人工神经网络。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_62" class="kx_ref">[62]</a></sup>与浅层神经网络相似，DNN可以模拟复杂的非线性关系。DNN架构生成合成模型，其中额外的层允许从较低层合成特征，提供了巨大的学习能力，因此具有建模复杂语音数据模式的潜力。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_62" class="kx_ref">[62]</a></sup> </p><p>2010年，工业研究人员与学术研究人员合作，在大词汇量语音识别中成功实现了DNN，其中采用了基于决策树构造的上下文相关HMM状态的DNN大输出层。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_63" class="kx_ref">[63]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_64" class="kx_ref">[64]</a></sup> <sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_65" class="kx_ref">[65]</a></sup>请参见微软研究院最近出版的《Springer》著作中对截至2014年10月的发展以及最新技术的讨论。另请参见自动语音识别的相关背景和各种机器学习范例的影响，特别是包括深度学习的技术。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_66" class="kx_ref">[66]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_67" class="kx_ref">[67]</a></sup> </p><p>深度学习的一个基本原则是摒弃手工制作的特征工程而使用原始特征进行学习。这一原理首先在深度自动编码器的架构中成功地探索到“原始”频谱图或线性滤波器特征，<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_68" class="kx_ref">[68]</a></sup>显示出优于Mel-倒谱图的特征，后者包含从光谱图固定转换的几个阶段。语音波形的真正“原始”特征最近被证明可以生成出色的大规模语音识别结果。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_69" class="kx_ref">[69]</a></sup> </p><p></p><h3>2.4 <span>端到端自动语音识别</span></h3><p></p><p>自2014年以来，人们对“端到端”ASR有了很大的研究兴趣。传统的基于语音的方法(即基于 HMM 的模型)需要单独的组件和对语音、声学和语言模型的训练。端到端模型共同学习语音识别器的所有组件。这一点很重要，因为它简化了培训过程和部署过程。例如，所有基于隐马尔可夫模型的系统都需要一个 n-gram语言模型，典型的n-gram语言模型通常需要几千兆字节的内存，使得它们无法部署在移动设备上。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_70" class="kx_ref">[70]</a></sup>因此，谷歌和苹果的现代商用ASR系统(截至2017年)部署在云系统中，需要网络连接才可以正常使用。</p><p>端到端ASR的第一次尝试是使用基于Connectionist Temporal class ification(CTC)的系统，该系统由Google DeepMind的Alex Graves和多伦多大学的Navdeep Jaitly于2014年引入。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_71" class="kx_ref">[71]</a></sup>该模型由递归神经网络和CTC层组成。RNN-CTC模型共同学习发音和声学模型，但是由于类似于HMM的条件独立假设，它无法学习语言。因此，CTC模型具有直接学习将语音声学映射到英语字符的能力，但这些模型会犯许多常见的拼写错误，必须依赖单独的语言模型来清理转录本。后来，百度利用非常大的数据集扩展了这项工作，并用中文和英文展示了一些商业上成功的案例。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_72" class="kx_ref">[72]</a></sup>2016年，牛津大学推出了LipNet，<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_73" class="kx_ref">[73]</a></sup>第一个端到端语句级唇读模型，使用时空卷积结合RNN-CTC架构，在受限语法数据集中超越了人类的水平。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_74" class="kx_ref">[74]</a></sup>谷歌DeepMind 在2018年推出了一个大型CNN-RNN-CTC架构，其性能是人类专家的6倍。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_75" class="kx_ref">[75]</a></sup> </p><p>基于CTC模型的另一种方法是基于attention-based的模型。卡内基梅隆大学和Google Brain的Chan等人和蒙特利尔大学的Bahdanau等人于2016年同时引入了基于attention-based 的ASR模型。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_76" class="kx_ref">[76]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_77" class="kx_ref">[77]</a></sup>名为“听、听、拼”(Listen, Attend and Spell，LAS)，字面上是“听”声音信号，注意信号的不同部分，一次“拼”出一个字符的抄本。与基于CTC的模型不同，基于attention-based的模型没有条件独立假设，可以直接学习语音识别器的所有组件，包括语音、声学和语言模型。这意味着，在部署过程中，不需要携带语言模型，这使得它在部署到仅具有特定内存存储器的应用程序上非常实用。截至2016年底，基于attention-based的模型取得了相当大的成功，包括超越CTC模型(有或没有外部语言模型)。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_78" class="kx_ref">[78]</a></sup>自最初的LAS模型以来，已经提出了各种扩展。潜在序列分解（LSD）是由卡耐基梅隆大学、麻省理工学院和谷歌大脑提出的，可以直接发出比英文字符更自然的子词单元；<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_79" class="kx_ref">[79]</a></sup> 牛津大学和谷歌DeepMind 将LAS扩展为“观看，收听，参加和拼写“（Watch, Listen, Attend and Spell，WLAS）处理唇读超过人类表现。。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_80" class="kx_ref">[80]</a></sup> </p></div></div><div id="par_14996178928140812"><h2 class="title">3 应用程序<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p></p><h3>3.1 <span>车载系统</span></h3><p></p><p>通常，手动控制输入，例如通过方向盘上的手指控制，启动语音识别系统，并通过音频提示向驾驶员发出信号。在音频提示后，系统有一个“监听窗口”，在此期间它可以接受语音输入以进行识别。</p><p>简单的语音命令可用于发起电话呼叫、选择无线电台或从兼容的智能手机、MP3播放器或音乐加载闪存驱动器中播放音乐。声音识别能力因车型而异。一些最新的汽车模型提供自然语言语音识别代替固定的一组命令，允许驾驶员使用完整的句子和常用短语。因此，使用这样的系统，用户不需要记住一组固定的命令字。</p><p></p><h3>3.2 <span>卫生保健</span></h3><p></p><p><strong>医疗文件</strong></p><p>在医疗保健领域，语音识别可以在医疗记录过程的前端或后端实现。前端语音识别是指命令者向语音识别引擎发出指令，识别出的单词在说话时显示出来，命令者负责编辑和签署文档。后端或延迟语音识别是指命令者将语音输入到数字听写系统中，语音通过语音识别机器传送，识别出的草稿文档与原始语音文件一起传送到编辑器，在编辑器中编辑草稿并最终完成报告。延迟语音识别目前在行业中广泛使用。</p><p>在医疗保健中使用语音识别的一个主要问题是，2009年美国复苏与再投资法案 ( ARRA )根据“有价值的使用”标准为使用EMR的医生提供了大量的经济利益。这些标准要求EMR必须维护大量数据(现在更普遍地称为电子健康记录或EHR)。语音识别的使用更自然地适合于叙事文本的生成，作为放射学/病理学解释、治疗进度注释或出院总结的一部分：对于视力正常并能操作键盘和鼠标的人来说，使用语音识别输入结构化离散数据(例如，列表或词汇表中的数值或代码)的人机工程收益相对较小。</p><p>一个更重要的问题是，大多数EHR没有被明确规定必须利用语音识别能力。临床医生与EHR的大部分互动涉及使用菜单和选项卡/按钮点击导航用户界面，并且严重依赖于键盘和鼠标：基于语音的导航仅提供适度的人机工程益处。相比之下，许多高度定制的放射学或病理学听写系统实现了语音“宏”，其中某些短语(例如，“正常”)的使用将自动填写大量默认值或生成样板文件，样板文件将随检查类型而变化，例如，例如胸部X射线与放射系统的胃肠对比系列。</p><p>作为手动导航的替代方案，语音识别和信息提取的级联使用已经被研究<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_81" class="kx_ref">[81]</a></sup>作为填写临床验证和签核交接表的一种方式。结果令人鼓舞，论文还将数据、相关性能基准和一些处理软件向研究和社区开放，用于研究临床文档和语言处理。</p><p><strong>治疗用途</strong></p><p>长期使用语音识别软件与文字处理器相结合，已经展现出接受切除术治疗的脑血管瘤患者的短期记忆再强化的好处。需要进行进一步的研究来确定使用放射技术治疗脑血管瘤(AVM)的益处。</p><p></p><h3>3.3 <span>军队</span></h3><p></p><p><strong>高性能战斗机</strong></p><p>在过去的十年中，人们在测试和评估战斗机中的语音识别方面做了大量的工作。值得注意的是美国的高级战斗机技术集成(AFTI) / F-16 飞机(F-16 VISTA )语音识别计划，法国的幻影飞机计划，以及英国各种飞机平台计划。在这些程序中，语音识别器已经在战斗机上成功运行，应用包括:设置无线电频率、命令自动驾驶系统、设置转向点坐标和武器释放参数以及控制飞行显示。</p><p>与在JAS-39 Gripen驾驶舱内飞行的瑞典飞行员一起工作的Englund（2004）发现随着g负荷的增加，识别能力也在下降。该报告还得出结论，适应性极大地改善了所有病例的结果，并且显示呼吸模型的引入显著提高了识别分数。与预期相反，并没有发现说话者的蹩脚英语的影响。正如人们所期望的那样，自发语音会给识别器带来问题。因此，有限的词汇，最重要的是正确的语法，会大大提高识别精度。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_82" class="kx_ref">[82]</a></sup> </p><p>目前在英国皇家空军服役的台风战斗机采用了一个与扬声器相关的系统，要求每个飞行员创建一个模板。该系统不用于任何安全或武器任务，如武器释放或起落架放下，但用于此外的其他驾驶舱功能。语音命令由视觉或听觉反馈确认。该系统被视为减少飞行员工作量的主要设计,<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_83" class="kx_ref">[83]</a></sup>甚至允许飞行员用两个简单的语音指令给飞机指定目标，或者只用五个指令给任何一个僚机指定目标。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_84" class="kx_ref">[84]</a></sup> </p><p>独立于扬声器的系统也正在开发中，并正在 F35闪电二号 (JSF)和M-346教练机中进行测试。这些系统已经产生了超过98%的单词准确性率。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_85" class="kx_ref">[85]</a></sup> </p><p><strong>直升机</strong></p><p>在压力和噪声下获得高识别精度的问题与直升机和喷气式战斗机飞行环境密切相关。声学噪声问题在直升机环境中实际上更为严重，不仅是因为高噪声水平，而且因为直升机飞行员通常不佩戴能够降低麦克风中声学噪声的面罩。在过去的十年中，直升机语音识别系统的应用已经进行了大量的测试和评估，尤其是由美国陆军航空电子研究发展中心(AVRADA)和英国皇家航空航天机构( RAE )的成就比较明显。法国的Puma直升机的语音识别、加拿大的也进行了不少工作。成果令人鼓舞，语音应用包括:通信无线电控制、导航系统设置和自动目标切换控制。</p><p>与战斗机应用一样，直升机中语音识别的首要问题是对飞行员效率的影响。据报告，AVRADA试验取得了令人鼓舞的结果，尽管这些结果只是试验环境中的可行性演示。因此，在语音识别和整体语音技术方面还有许多工作要做，以便在操作环境中持续实现性能改进。</p><p><strong>训练空中交通管制员</strong></p><p>空中交通管制员的培训是语音识别系统的一个很好的应用。许多空中交通管制训练系统目前需要一个人充当“伪飞行员”，与训练员控制器进行语音对话，模拟控制器在真实空中交通管制情况下与飞行员进行的对话。语音识别和合成技术有可能消除人们充当伪飞行员的需要，从而减少培训和地面支持人员。理论上，空中控制器语音任务具有高度结构化的语音输出特征，因此一定程度上降低了语音识别任务的难度。实际上，情况很少如此。美国联邦航空局文件7110.65详细说明了空中交通管制员应该使用的短语。虽然本文给出的这种短语的例子不到150个，但是由模拟供应商的语音识别系统支持的短语数量超过了500000个。</p><p>美国空军、美国海军陆战队、美国陆军、美国海军和联邦航空局以及许多国际空中交通管制培训组织，如澳大利亚皇家空军和意大利、巴西和加拿大的民航局，目前正在使用带有来自许多不同供应商的具有语音识别功能的空中交通管制模拟器。</p><p></p><h3>3.4 <span>电话和其他领域</span></h3><p></p><p>ASR如今在电话领域很常见，并且在计算机游戏和模拟领域越来越广泛。在电话系统中，ASR现在主要通过与交互式语音应答IVR 系统集成来使用。尽管广泛应用于一般个人计算与字处理的高度集成，但在文档制作领域，ASR的使用没有达到预期的效果。</p><p>移动处理器速度的提高使语音识别在智能手机中变得切实可行。语音主要用作用户界面的一部分，用于创建预定义或自定义的语音命令。</p><p></p><h3>3.5 <span>在教育和日常生活中的使用</span></h3><p></p><p>对于语言学习，语音识别对于学习第二语言很有用。它能教人正确的发音，还能帮助一个人训练流利的口语。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_86" class="kx_ref">[86]</a></sup> </p><p>失明或视力低下的学生可以受益于使用这种技术来传达单词，然后听到计算机对它们进行朗读，还可以通过用声音发出指令来使用计算机，而不必看屏幕和键盘。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_87" class="kx_ref">[87]</a></sup> </p><p>身体残疾或患有重复性劳损或上肢损伤的学生可以通过使用语音到文本的程序，而不必担心手写、打字或与抄写员一起完成学校作业。他们还可以利用语音识别技术来自由享受互联网或单独使用计算机的乐趣，而不必亲自操作鼠标和键盘。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_87" class="kx_ref">[87]</a></sup> </p><p>语音识别可以让有学习障碍的学生成为更好的作家。通过大声说出单词，他们可以增加写作的流动性，减轻对拼写、标点符号和其它写作技巧的担忧。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_88" class="kx_ref">[88]</a></sup> </p><p>使用语音识别软件，配合数字录音机和运行文字处理软件的个人计算机，已被证明对恢复中风和开颅患者受损的短期记忆能力是有益的。</p><p></p><h3>3.6 <span>残疾人</span></h3><p></p><p>残疾人可以从语音识别项目中受益。对于聋人或听力困难的人，语音识别软件用于自动生成对话的隐藏字幕，例如会议室讨论、课堂讲座或宗教服务。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_89" class="kx_ref">[89]</a></sup> </p><p>语音识别对使用手有困难的人也非常有用，从轻微的重复性应激损伤到涉及使用传统计算机输入设备的残疾。事实上，经常使用键盘并发展出 RSI 的人成为了语音识别的一个迫切的早期市场。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_90" class="kx_ref">[90]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_91" class="kx_ref">[91]</a></sup>语音识别用于聋人电话，例如语音邮件到文本、中继服务和字幕电话。有学习障碍的人在思想到纸张的交流中有问题(本质上他们想到了一个想法，但处理不准确会导致表达错误)可能会从软件中受益，但该技术不是防错的。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_92" class="kx_ref">[92]</a></sup>此外，对智力残疾人来说，与文本对话的整个想法可能很难实现，因为很少有人试图学习这种技术来教育残疾人。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_93" class="kx_ref">[93]</a></sup> </p><p>这种技术可以帮助有阅读障碍的人，但其他残疾仍然存在问题。产品的有效性是阻碍其有效性的关键。虽然一个孩子可能会说一个词，这取决于他们说得有多清楚，但系统可能会认为他们说的是另一个词而输入错误的信息。给他们更多的工作去修正，导致他们不得不花更多的时间去修正错误的单词。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_94" class="kx_ref">[94]</a></sup> </p><p></p><h3>3.7 <span>进一步的应用</span></h3><p></p><p>
 </p><ul>
  <li>航空航天(如空间探索、航天器等)，美国宇航局的火星极地着陆者号在着陆器上的麦克风中使用了传感公司的语音识别技术<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_95" class="kx_ref">[95]</a></sup></li>
  <li>带语音识别的自动字幕</li>
  <li>自动情感识别<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_96" class="kx_ref">[96]</a></sup></li>
  <li>自动翻译</li>
  <li>法庭报告(实时演讲写作)</li>
  <li>电子发现(法律发现)</li>
  <li>免提计算：语音识别计算机用户界面</li>
  <li>智能家居</li>
  <li>交互式语音响应</li>
  <li>移动电话，包括移动电子邮件</li>
  <li>多模式交互</li>
  <li>计算机辅助语言学习应用中的发音评估</li>
  <li>实时字幕</li>
  <li>机器人</li>
  <li>语音到文本(将语音转录为文本、实时视频字幕、法庭报告)</li>
  <li>远程信息处理系统(例如车辆导航系统)</li>
  <li>转录(数字语音到文本)</li>
  <li>电子游戏（汤姆克兰西的EndWar和Lifeline）</li>
  <li>虚拟助手(例如苹果Siri )</li>
 </ul><p></p></div></div><div id="par_14996178928140813"><h2 class="title">4 表演<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p>语音识别系统的性能通常根据准确性和速度来评估。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_97" class="kx_ref">[97]</a></sup><sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_98" class="kx_ref">[98]</a></sup>准确度通常用单词错误率来评定(WER)，而速度用实时性来测量。其它衡量准确性的指标包括单字错误率 (SWER)和命令成功率 (CSR)。</p><p>然而，机器语音识别是一个非常复杂的问题。发声在口音、发音、清晰度、粗糙度、鼻音、音高、音量和速度方面各不相同。语音被背景噪声和回声、电特性所扭曲。语音识别的准确性可能因以下因素而异：<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_99" class="kx_ref">[99]</a></sup> </p><p>
 </p><ul>
  <li>词汇量和易混淆性</li>
  <li>词汇量大小和混淆</li>
  <li>说话者依赖与独立</li>
  <li>孤立、不连续或连续的语音</li>
  <li>任务和语言限制</li>
  <li>阅读与自发言论</li>
  <li>不利条件</li>
 </ul><p></p><p></p><h3>4.1 <span>准确</span></h3><p></p><p>如本文前面所述，语音识别的准确性可能因以下因素而异:</p><p>
 </p><ul>
  <li>错误率随着词汇量的增加而增加:</li>
 </ul><p></p><p>
 </p><dl>
  <dd>
   <dl>
    <dd>
     例如，从“0”到“9”的10位数字基本上可以被完美地识别，但是200、5000或100000的词汇量可能分别具有3%、7%或45%的错误率。
    </dd>
   </dl>
  </dd>
 </dl><p></p><p>
 </p><ul>
  <li>词汇如果包含容易混淆的单词，就很难识别:</li>
 </ul><p></p><p>
 </p><dl>
  <dd>
   <dl>
    <dd>
     例如，英语字母表中的26个字母很难区分，因为它们是容易混淆的单词(最明显的是E-set:“B，C，D，E，G，P，T，V，Z”)；8%的错误率被认为是这几个字母造成的。
    </dd>
   </dl>
  </dd>
 </dl><p></p><p>
 </p><ul>
  <li>说话者依赖性与独立性:</li>
 </ul><p></p><p>
 </p><dl>
  <dd>
   <dl>
    <dd>
     与扬声器相关的系统旨在由单个扬声器使用。
    </dd>
    <dd>
     独立于扬声器的系统适用于任何扬声器(难度更大)。
    </dd>
   </dl>
  </dd>
 </dl><p></p><p>
 </p><ul>
  <li>孤立、不连续或连续的语音</li>
 </ul><p></p><p>
 </p><dl>
  <dd>
   <dl>
    <dd>
     对于孤立的语音，使用单个单词，因此更容易识别。
    </dd>
   </dl>
  </dd>
 </dl><p></p><p>对于不连续的语音，使用由静音分隔的完整句子，因此与孤立的语音一样，识别语音变得更容易。<br>连续语音使用连续的自然口语句子，因此与孤立和不连续语音不同，语音识别变得更加困难。</p><p>
 </p><ul>
  <li>任务和语言限制
   <ul>
    <li>例如，查询应用程序可能会驳回“苹果是红色的”假设。</li>
    <li>例如，约束可以是语义化的；拒绝“苹果生气了”</li>
    <li>例如，句法；拒绝“红色就是苹果”</li>
   </ul></li>
 </ul><p></p><p>约束通常由语法表示。</p><p>
 </p><ul>
  <li>阅读与自发言语——当一个人阅读时，通常是在事先准备好的背景下进行的，但是当一个人使用自发言语时，由于不流畅(如“呃”和“嗯”、错误的开头、不完整的句子、口吃、咳嗽和笑声)以及词汇有限，就很难识别出该言语。</li>
  <li>不利条件——环境噪声(例如汽车或工厂中的噪声)。声学失真(例如回声、室内声学)</li>
 </ul><p></p><p>语音识别是一项多层次的模式识别任务。</p><p>
 </p><ul>
  <li>声学信号被构造成单元的层次结构，例如音素、单词、短语和句子；</li>
  <li>每个级别都提供了额外的约束；</li>
 </ul><p></p><p>例如已知单词发音或法律单词序列，其可以补偿较低水平的错误或不确定性；</p><p>
 </p><ul>
  <li>利用这种约束层次。机器语音识别是一个分成几个阶段的过程，通过在所有较低层次上的组合概率决策，而只在最高层次上做出更确定性的决策。在计算上，这是一个声音模式必须被识别或分类到对人类来说有意义问题类别。每个声音信号都可以分解成更小、更基本的子信号。当更复杂的声音信号被分解成更小的子声音时，会产生不同的声音级别，在最高级别，我们会产生复杂的声音，这些声音由较低级别、更简单的声音组成，而在更低的级别，我们会产生更基本、更短、更简单的声音。最低级别，声音是最基本的，机器将检查声音应代表的简单和更概率的规则。一旦这些声音在上层组合成更复杂的声音，一组新的更确定的规则会预测新的复杂声音所代表的内容。确定性规则的最高层应该理解复杂表达式的含义。为了扩展我们关于语音识别的知识，就需要考虑神经网络。神经网络方法有四个步骤:</li>
  <li>数字化我们想要识别的语音</li>
 </ul><p></p><p>对于电话语音，采样率为每秒8000个样本；</p><p>
 </p><ul>
  <li>计算语音的谱域特征(傅里叶变换)；</li>
 </ul><p></p><p>每10毫秒计算一次，每个10毫秒的部分称为一帧；</p><p>对四步神经网络方法的分析可以通过进一步的信息来解释。声音是由空气(或其他介质)振动产生的，我们用耳朵记录，但机器用接收器记录。基本声音产生的波有两种描述：振幅(大小)和频率(每秒振动的次数)。</p><p></p><h3>4.2 <span>安全问题</span></h3><p></p><p>语音识别可以成为攻击、盗窃或意外操作的手段。例如，音频或视频广播中所说的“Alexa”之类的激活词可能会导致家庭和办公室中的设备开始不恰当地收听外部音频输入，或者可能采取不必要的行动。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_100" class="kx_ref">[100]</a></sup>如果可以在室内听到，语音控制设备也可以被大楼的访客使用，甚至是大楼外的访客。攻击者可能会访问个人信息，如日历、地址簿内容、私人消息和文档。他们也可以模拟用户发送消息或在线购物。</p><p>已经证明存在两种使用人工声音的攻击。人们发射超声波，试图在附近的人没有注意的情况下发送命令。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_101" class="kx_ref">[101]</a></sup>另一种是为其他语音或音乐添加小的不可见的失真，这些语音或音乐被定做以混淆特定的语音识别系统将音乐识别为语音，或者使听起来像人类对系统发出的一个命令的声音。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_102" class="kx_ref">[102]</a></sup> </p></div></div><div id="par_14996178928140814"><h2 class="title">5 更多信息<a href="javascript:" class="detail-edit">编辑</a></h2><div class="section_content"><p></p><h3>5.1 <span>会议和期刊</span></h3><p></p><p>每年或每两年举行的流行语音识别会议包括SpeechTEK和SpeechTEK Europe，ICASSP，Interspeech / Eurospeech和IEEE ASRU。自然语言处理领域的会议，如ACL，NAACL，EMNLP和HLT，包括有关语音处理的论文。重要期刊包括 IEEE Transactions on Speech and Audio Processing(后来改名IEEE Transactions on Audio, Speech and Language Processing，自2014年9月起更名为IEEE/ACM Transactions on Audio, Speech and Language Processing—随后，与ACM出版物合并)、计算机语音和语言（Computer Speech and Language）以及语音通信（Speech Communication）。</p><p></p><h3>5.2 <span>书籍</span></h3><p></p><p>劳伦斯·拉比纳的《语音识别基础》有助于学习一些语音识别的基础知识，但可能不是完全最新的(1993)。此外还有Frederick Jelinek 的《语音识别统计方法》和黄东学的《口语处理(2001)》等。最新的是Manfred R. Schroeder 的《计算机语音》（2004年，第二版），李登和Doug O'Shaughnessey于2003年出版的《语音处理：动态和优化导向的方法》。最近更新的《<i>语音和语言处理</i>(2008)》由Jurafsky和Martin编写，主要介绍了ASR的基础知识和最新技术。说话人识别也使用与语音识别相同的功能、大多数相同的前端处理和分类技术。一本最新的综合性教科书《说话人识别基础》是关于理论和实践最新细节的讨论。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_103" class="kx_ref">[103]</a></sup>通过关注政府资助的评估，例如由DARPA组织的评估，可以获得对最佳现代系统中使用的语音识别技术进行深入的了解（截至2007年，最大的语音识别相关项目是GALE项目，其涉及语音识别和翻译组件）。</p><p>罗伯托·皮拉西奇（Roberto Pieraccini，2012）的《机器中的声音》提供了语音识别技术及一个很好且容易理解的介绍。</p><p>最近一本关于语音识别的书是《<i>自动语音识别:深度学习方法》</i>(Publisher: Springer)由D. Yu和L. Deng撰写，于2014年底出版，在基于dnn和相关深度学习方法的现代语音识别系统中推导和实现上具有高度数学化的技术细节，可以深入了解学习方法的深度。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_104" class="kx_ref">[104]</a></sup>D. Yu和L. Deng在2014年初出版的一本相关书籍《深度学习:方法和应用》，提供了2009-2014年期间基于DNN的语音识别的技术含量较低但更注重方法的概述，该书被放在深度学习应用的一般框架中，不仅包括语音识别，还包括图像识别、自然语言处理、信息检索、多模式处理和多任务学习。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_104" class="kx_ref">[104]</a></sup> </p><p></p><h3>5.3 <span>软件</span></h3><p></p><p>就免费资源而言，卡内基梅隆大学的Sphinx工具包是开始学习语音识别和开展实验的一个工具。另一个资源(免费但受版权保护)是 HTK 书籍(以及附带的HTK工具包)。如果需要更新、最先进的技术，可以使用 Kaldi 工具包。</p><p>Cobalt的网页上有在线语音识别器的演示。<sup><a href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/d10951.htm#quote_104" class="kx_ref">[104]</a></sup> </p></div></div></div></div><div id="references"><h2 class="title" id="par_references">参考文献</h2><ul class="references"><li id="quote_1"><span class="references-num">[1]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Speaker Independent Connected Speech Recognition- Fifth Generation Computer Corporation". Fifthgen.com. Archived from the original on 11 November 2013. Retrieved 15 June 2013..</span></p></li><li id="quote_2"><span class="references-num">[2]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">P. Nguyen (2010). "Automatic classification of speaker characteristics"..</span></p></li><li id="quote_3"><span class="references-num">[3]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"British English definition of voice recognition". Macmillan Publishers Limited. Archived from the original on 16 September 2011. Retrieved 21 February 2012..</span></p></li><li id="quote_4"><span class="references-num">[4]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"voice recognition, definition of". WebFinance, Inc. Archived from the original on 3 December 2011. Retrieved 21 February 2012..</span></p></li><li id="quote_5"><span class="references-num">[5]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"The Mailbag LG #114". Linuxgazette.net. Archived from the original on 19 February 2013. Retrieved 15 June 2013..</span></p></li><li id="quote_6"><span class="references-num">[6]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Reynolds, Douglas; Rose, Richard (January 1995). "Robust text-independent speaker identification using Gaussian mixture speaker models" (PDF). IEEE Transactions on Speech and Audio Processing. 3 (1): 72–83. doi:10.1109/89.365379. ISSN 1063-6676. OCLC 26108901. Archived (PDF) from the original on 8 March 2014. Retrieved 21 February 2014..</span></p></li><li id="quote_7"><span class="references-num">[7]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Speaker Identification (WhisperID)". Microsoft Research. Microsoft. Archived from the original on 25 February 2014. Retrieved 21 February 2014. When you speak to someone, they don't just recognize what you say: they recognize who you are. WhisperID will let computers do that, too, figuring out who you are by the way you sound..</span></p></li><li id="quote_8"><span class="references-num">[8]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Obituaries: Stephen Balashek". The Star-Ledger. 22 July 2012..</span></p></li><li id="quote_9"><span class="references-num">[9]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"IBM-Shoebox-front.jpg". androidauthority.net. Retrieved 4 April 2019..</span></p></li><li id="quote_10"><span class="references-num">[10]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Juang, B. H.; Rabiner, Lawrence R. "Automatic speech recognition–a brief history of the technology development" (PDF): 6. Archived (PDF) from the original on 17 August 2004. Retrieved 17 January 2015..</span></p></li><li id="quote_11"><span class="references-num">[11]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Melanie Pinola (2 November 2011). "Speech Recognition Through the Decades: How We Ended Up With Siri". PC World. Retrieved 22 October 2018..</span></p></li><li id="quote_12"><span class="references-num">[12]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">John R. Pierce (1969). "Whither speech recognition?". Journal of the Acoustical Society of America. 46 (48): 1049–1051. Bibcode:1969ASAJ...46.1049P. doi:10.1121/1.1911801..</span></p></li><li id="quote_13"><span class="references-num">[13]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Benesty, Jacob; Sondhi, M. M.; Huang, Yiteng (2008). Springer Handbook of Speech Processing. Springer Science &amp; Business Media. ISBN 978-3540491255..</span></p></li><li id="quote_14"><span class="references-num">[14]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">John Makhoul. "ISCA Medalist: For leadership and extensive contributions to speech and language processing". Archived from the original on 24 January 2018. Retrieved 23 January 2018..</span></p></li><li id="quote_15"><span class="references-num">[15]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Blechman, R. O.; Blechman, Nicholas (23 June 2008). "Hello, Hal". The New Yorker. Archived from the original on 20 January 2015. Retrieved 17 January 2015..</span></p></li><li id="quote_16"><span class="references-num">[16]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Klatt, Dennis H. (1977). "Review of the ARPA speech understanding project". The Journal of the Acoustical Society of America. 62 (6): 1345–1366. Bibcode:1977ASAJ...62.1345K. doi:10.1121/1.381666..</span></p></li><li id="quote_17"><span class="references-num">[17]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Rabiner (1984). "The Acoustics, Speech, and Signal Processing Society. A Historical Perspective" (PDF). Archived (PDF) from the original on 9 August 2017. Retrieved 23 January 2018..</span></p></li><li id="quote_18"><span class="references-num">[18]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"First-Hand:The Hidden Markov Model – Engineering and Technology History Wiki". ethw.org. Archived from the original on 3 April 2018. Retrieved 1 May 2018..</span></p></li><li id="quote_19"><span class="references-num">[19]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"James Baker interview". Archived from the original on 28 August 2017. Retrieved 9 February 2017..</span></p></li><li id="quote_20"><span class="references-num">[20]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Pioneering Speech Recognition". 7 March 2012. Archived from the original on 19 February 2015. Retrieved 18 January 2015..</span></p></li><li id="quote_21"><span class="references-num">[21]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Xuedong Huang; James Baker; Raj Reddy. "A Historical Perspective of Speech Recognition". Communications of the ACM. Archived from the original on 20 January 2015. Retrieved 20 January 2015..</span></p></li><li id="quote_22"><span class="references-num">[22]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Juang, B. H.; Rabiner, Lawrence R. "Automatic speech recognition–a brief history of the technology development" (PDF): 10. Archived (PDF) from the original on 17 August 2014. Retrieved 17 January 2015..</span></p></li><li id="quote_23"><span class="references-num">[23]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"History of Speech Recognition". Dragon Medical Transcription. Archived from the original on 13 August 2015. Retrieved 17 January 2015..</span></p></li><li id="quote_24"><span class="references-num">[24]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Kevin McKean (8 April 1980). "When Cole talks, computers listen". Sarasota Journal. AP. Retrieved 23 November 2015..</span></p></li><li id="quote_25"><span class="references-num">[25]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Melanie Pinola (2 November 2011). "Speech Recognition Through the Decades: How We Ended Up With Siri". PC World. Archived from the original on 13 January 2017. Retrieved 28 July 2017..</span></p></li><li id="quote_26"><span class="references-num">[26]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Ray Kurzweil biography". KurzweilAINetwork. Archived from the original on 5 February 2014. Retrieved 25 September 2014..</span></p></li><li id="quote_27"><span class="references-num">[27]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Juang, B.H.; Rabiner, Lawrence. "Automatic Speech Recognition – A Brief History of the Technology Development" (PDF). Archived (PDF) from the original on 9 August 2017. Retrieved 28 July 2017..</span></p></li><li id="quote_28"><span class="references-num">[28]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Nuance Exec on iPhone 4S, Siri, and the Future of Speech". Tech.pinions. 10 October 2011. Archived from the original on 19 November 2011. Retrieved 23 November 2011..</span></p></li><li id="quote_29"><span class="references-num">[29]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Switchboard-1 Release 2". Archived from the original on 11 July 2017. Retrieved 26 July 2017..</span></p></li><li id="quote_30"><span class="references-num">[30]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Jason Kincaid. "The Power of Voice: A Conversation With The Head Of Google's Speech Technology". Tech Crunch. Archived from the original on 21 July 2015. Retrieved 21 July 2015..</span></p></li><li id="quote_31"><span class="references-num">[31]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Froomkin, Dan (5 May 2015). "THE COMPUTERS ARE LISTENING". The Intercept. Archived from the original on 27 June 2015. Retrieved 20 June 2015..</span></p></li><li id="quote_32"><span class="references-num">[32]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">赫尔夫·布尔拉德和尼尔森·摩根，Connectionist语音识别:混合方法，Kluwer国际工程和计算机科学系列；247，波士顿:克鲁瓦学术出版社，1994年。.</span></p></li><li id="quote_33"><span class="references-num">[33]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Schmidhuber, Jürgen (2015). "Deep learning in neural networks: An overview". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637..</span></p></li><li id="quote_34"><span class="references-num">[34]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Alex Graves，Santiago Fernandez，Faustino Gomez和 jürgen schmid Huber (2006)。联结主义时间分类:用递归神经网络标记未分段的序列数据。ICML会议录' 06，第369-376页。.</span></p></li><li id="quote_35"><span class="references-num">[35]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">圣地亚哥·费尔南德斯、亚历克斯·格雷夫斯和尤尔根·施密休伯(2007年)。递归神经网络在判别关键词识别中的应用。ICANN会议录(2)，第220-229页。.</span></p></li><li id="quote_36"><span class="references-num">[36]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Li Deng". Li Deng Site..</span></p></li><li id="quote_37"><span class="references-num">[37]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">NIPS研讨会:语音识别和相关应用的深度学习，加拿大不列颠哥伦比亚省惠斯勒，2009年12月(组织者:邓梨、杰夫·辛顿、俞敏洪)。.</span></p></li><li id="quote_38"><span class="references-num">[38]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Deng, L.; Hinton, G.; Kingsbury, B. (2013). "New types of deep neural network learning for speech recognition and related applications: An overview". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: New types of deep neural network learning for speech recognition and related applications: An overview. p. 8599. doi:10.1109/ICASSP.2013.6639344. ISBN 978-1-4799-0356-6..</span></p></li><li id="quote_39"><span class="references-num">[39]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Markoff, John (23 November 2012). "Scientists See Promise in Deep-Learning Programs". New York Times. Archived from the original on 30 November 2012. Retrieved 20 January 2015..</span></p></li><li id="quote_40"><span class="references-num">[40]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Morgan，Bourlard，Renals，Cohen，Franco (1993)"用于连续语音识别的混合神经网络/隐马尔可夫模型系统。ICASSP/IJPRAI ".</span></p></li><li id="quote_41"><span class="references-num">[41]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">罗宾逊。(1992) 一种实时循环错误传播网络单词识别系统 Archived 3 9月 2017 at the Wayback Machine，ICASSP。.</span></p></li><li id="quote_42"><span class="references-num">[42]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">外贝尔，哈那扎瓦，辛顿，志卡诺，朗。(1989)"使用延时神经网络的音素识别。IEEE声学、语音和信号处理汇刊.</span></p></li><li id="quote_43"><span class="references-num">[43]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Baker, J.; Li Deng; Glass, J.; Khudanpur, S.; Chin-Hui Lee; Morgan, N.; O'Shaughnessy, D. (2009). "Developments and Directions in Speech Recognition and Understanding, Part 1". IEEE Signal Processing Magazine. 26 (3): 75–80. Bibcode:2009ISPM...26...75B. doi:10.1109/MSP.2009.932166..</span></p></li><li id="quote_44"><span class="references-num">[44]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Sepp Hochreiter(1991年)，动态神经网络 Archived 6 3月 2015 at the Wayback Machine，毕业论文。慕尼黑工业大学信息研究所。顾问:施密休伯。.</span></p></li><li id="quote_45"><span class="references-num">[45]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Bengio, Y. (1991). Artificial Neural Networks and their Application to Speech/Sequence Recognition (Ph.D.). McGill University..</span></p></li><li id="quote_46"><span class="references-num">[46]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Deng, L.; Hassanein, K.; Elmasry, M. (1994). "Analysis of the correlation structure for a neural predictive model with application to speech recognition". Neural Networks. 7 (2): 331–339. doi:10.1016/0893-6080(94)90027-2..</span></p></li><li id="quote_47"><span class="references-num">[47]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">主旨演讲:深度神经网络的最新发展。ICASSP，2013(作者:Geoff Hinton)。.</span></p></li><li id="quote_48"><span class="references-num">[48]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">主旨演讲:“深度学习的成就和挑战:从语音分析和识别到语言和多模式处理”，Interspeech，2014年9月(作者:邓梨)。.</span></p></li><li id="quote_49"><span class="references-num">[49]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Improvements in voice recognition software increase". TechRepublic.com. 27 August 2002. Maners said IBM has worked on advancing speech recognition ... or on the floor of a noisy trade show..</span></p></li><li id="quote_50"><span class="references-num">[50]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Voice Recognition To Ease Travel Bookings: Business Travel News". BusinessTravelNews.com. 3 March 1997. The earliest applications of speech recognition software were dictation ... Four months ago, IBM introduced a 'continual dictation product' designed to ... debuted at the National Business Travel Association trade show in 1994..</span></p></li><li id="quote_51"><span class="references-num">[51]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Ellis Booker (14 March 1994). "Voice recognition enters the mainstream". Computerworld. p. 45. Just a few years ago, speech recognition was limited to ....</span></p></li><li id="quote_52"><span class="references-num">[52]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Goel, Vaibhava; Byrne, William J. (2000). "Minimum Bayes-risk automatic speech recognition". Computer Speech &amp; Language. 14 (2): 115–135. doi:10.1006/csla.2000.0138. Archived from the original on 25 July 2011. Retrieved 28 March 2011..</span></p></li><li id="quote_53"><span class="references-num">[53]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Mohri, M. (2002). "Edit-Distance of Weighted Automata: General Definitions and Algorithms" (PDF). International Journal of Foundations of Computer Science. 14 (6): 957–982. doi:10.1142/S0129054103002114. Archived (PDF) from the original on 18 March 2012. Retrieved 28 March 2011..</span></p></li><li id="quote_54"><span class="references-num">[54]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Waibel, A.; Hanazawa, T.; Hinton, G.; Shikano, K.; Lang, K. J. (1989). "Phoneme recognition using time-delay neural networks". IEEE Transactions on Acoustics, Speech, and Signal Processing. 37 (3): 328–339. doi:10.1109/29.21701..</span></p></li><li id="quote_55"><span class="references-num">[55]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Wu, J.; Chan, C. (1993). "Isolated Word Recognition by Neural Network Models with Cross-Correlation Coefficients for Speech Dynamics". IEEE Transactions on Pattern Analysis and Machine Intelligence. 15 (11): 1174–1185. doi:10.1109/34.244678..</span></p></li><li id="quote_56"><span class="references-num">[56]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">S.A. Zahorian，A. M. Zimmer和F. Meng，(2002)“基于计算机视觉反馈的听力受损者语音训练元音分类”，ICSLP 2002.</span></p></li><li id="quote_57"><span class="references-num">[57]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Hu, Hongbing; Zahorian, Stephen A. (2010). "Dimensionality Reduction Methods for HMM Phonetic Recognition" (PDF). ICASSP 2010. Archived (PDF) from the original on 6 July 2012..</span></p></li><li id="quote_58"><span class="references-num">[58]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Sepp Hochreiter; J. Schmidhuber (1997). "Long Short-Term Memory". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. PMID 9377276..</span></p></li><li id="quote_59"><span class="references-num">[59]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Graves, Alex; Mohamed, Abdel-rahman; Hinton, Geoffrey (2013). "Speech recognition with deep recurrent neural networks". arXiv:1303.5778 [cs.NE].ICASSP 2013。.</span></p></li><li id="quote_60"><span class="references-num">[60]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Waibel, Alex (1989). "Modular Construction of Time-Delay Neural Networks for Speech Recognition" (PDF). Neural Computation. 1 (1): 39–46. doi:10.1162/neco.1989.1.1.39. Archived (PDF) from the original on 29 June 2016..</span></p></li><li id="quote_61"><span class="references-num">[61]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Maas, Andrew L.; Le, Quoc V.; O'Neil, Tyler M.; Vinyals, Oriol; Nguyen, Patrick; Ng, Andrew Y. (2012). "Recurrent Neural Networks for Noise Reduction in Robust ASR". Proceedings of Interspeech 2012..</span></p></li><li id="quote_62"><span class="references-num">[62]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Hinton, Geoffrey; Deng, Li; Yu, Dong; Dahl, George; Mohamed, Abdel-Rahman; Jaitly, Navdeep; Senior, Andrew; Vanhoucke, Vincent; Nguyen, Patrick; Sainath, Tara; Kingsbury, Brian (2012). "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The shared views of four research groups". IEEE Signal Processing Magazine. 29 (6): 82–97. Bibcode:2012ISPM...29...82H. doi:10.1109/MSP.2012.2205597..</span></p></li><li id="quote_63"><span class="references-num">[63]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Yu, D.; Deng, L.; Dahl, G. (2010). "Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition". NIPS Workshop on Deep Learning and Unsupervised Feature Learning..</span></p></li><li id="quote_64"><span class="references-num">[64]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Dahl, George E.; Yu, Dong; Deng, Li; Acero, Alex (2012). "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition". IEEE Transactions on Audio, Speech, and Signal Processing. 20 (1): 30–42. doi:10.1109/TASL.2011.2134090..</span></p></li><li id="quote_65"><span class="references-num">[65]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">邓l，李军，黄军，姚军，k，俞，d，塞德，f .等。微软语音研究深度学习的最新进展。会计师协会，2013年。.</span></p></li><li id="quote_66"><span class="references-num">[66]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Deng, L.; Li, Xiao (2013). "Machine Learning Paradigms for Speech Recognition: An Overview". IEEE Transactions on Audio, Speech, and Language Processing..</span></p></li><li id="quote_67"><span class="references-num">[67]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Schmidhuber, Jürgen (2015). "Deep Learning". Scholarpedia. 10 (11): 32832. Bibcode:2015SchpJ..1032832S. doi:10.4249/scholarpedia.32832..</span></p></li><li id="quote_68"><span class="references-num">[68]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">长度Deng，M. Seltzer，D. Yu，A. Acero，A. Mohamed和G. Hinton (2010)使用深度自编码对语音频谱图进行二进制编码。散布技术。.</span></p></li><li id="quote_69"><span class="references-num">[69]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Tüske, Zoltán; Golik, Pavel; Schlüter, Ralf; Ney, Hermann (2014). "Acoustic Modeling with Deep Neural Networks Using Raw Time Signal for LVCSR" (PDF). Interspeech 2014. Archived (PDF) from the original on 21 December 2016..</span></p></li><li id="quote_70"><span class="references-num">[70]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Jurafsky, Daniel (2016). Speech and Language Processing..</span></p></li><li id="quote_71"><span class="references-num">[71]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Graves, Alex (2014). "Towards End-to-End Speech Recognition with Recurrent Neural Networks". ICML..</span></p></li><li id="quote_72"><span class="references-num">[72]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Amodei, Dario (2016). "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin". arXiv:1512.02595 [cs.CL]..</span></p></li><li id="quote_73"><span class="references-num">[73]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"LipNet: How easy do you think lipreading is?". YouTube. Archived from the original on 27 April 2017. Retrieved 5 May 2017..</span></p></li><li id="quote_74"><span class="references-num">[74]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Assael, Yannis; Shillingford, Brendan; Whiteson, Shimon; de Freitas, Nando (5 November 2016). "LipNet: End-to-End Sentence-level Lipreading". arXiv:1611.01599 [cs.CV]..</span></p></li><li id="quote_75"><span class="references-num">[75]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Shillingford, Brendan; Assael, Yannis; Hoffman, Matthew W.; Paine, Thomas; Hughes, Cían; Prabhu, Utsav; Liao, Hank; Sak, Hasim; Rao, Kanishka (2018-07-13). "Large-Scale Visual Speech Recognition". arXiv:1807.05162..</span></p></li><li id="quote_76"><span class="references-num">[76]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Chan, William; Jaitly, Navdeep; Le, Quoc; Vinyals, Oriol (2016). "Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition". ICASSP..</span></p></li><li id="quote_77"><span class="references-num">[77]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Bahdanau, Dzmitry (2016). "End-to-End Attention-based Large Vocabulary Speech Recognition". arXiv:1508.04395 [cs.CL]..</span></p></li><li id="quote_78"><span class="references-num">[78]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Chorowski, Jan; Jaitly, Navdeep (8 December 2016). "Towards better decoding and language model integration in sequence to sequence models". arXiv:1612.02695 [cs.NE]..</span></p></li><li id="quote_79"><span class="references-num">[79]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Chan, William; Zhang, Yu; Le, Quoc; Jaitly, Navdeep (10 October 2016). "Latent Sequence Decompositions". arXiv:1610.03035 [stat.ML]..</span></p></li><li id="quote_80"><span class="references-num">[80]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Chung, Joon Son; Senior, Andrew; Vinyals, Oriol; Zisserman, Andrew (16 November 2016). "Lip Reading Sentences in the Wild". arXiv:1611.05358 [cs.CV]..</span></p></li><li id="quote_81"><span class="references-num">[81]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Suominen, Hanna; Zhou, Liyuan; Hanlen, Leif; Ferraro, Gabriela (2015). "Benchmarking Clinical Speech Recognition and Information Extraction: New Data, Methods, and Evaluations". JMIR Medical Informatics. 3 (2): e19. doi:10.2196/medinform.4321. PMC 4427705. PMID 25917752..</span></p></li><li id="quote_82"><span class="references-num">[82]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Englund, Christine (2004). Speech recognition in the JAS 39 Gripen aircraft: Adaptation to speech at different G-loads (PDF) (Masters thesis). Stockholm Royal Institute of Technology. Archived (PDF) from the original on 2 October 2008..</span></p></li><li id="quote_83"><span class="references-num">[83]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"The Cockpit". Eurofighter Typhoon. Archived from the original on 1 March 2017..</span></p></li><li id="quote_84"><span class="references-num">[84]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Eurofighter Typhoon – The world's most advanced fighter aircraft". www.eurofighter.com. Archived from the original on 11 May 2013. Retrieved 1 May 2018..</span></p></li><li id="quote_85"><span class="references-num">[85]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Schutte, John (15 October 2007). "Researchers fine-tune F-35 pilot-aircraft speech system". United States Air Force. Archived from the original on 20 October 2007..</span></p></li><li id="quote_86"><span class="references-num">[86]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Cerf, Vinton; Wrubel, Rob; Sherwood, Susan. "Can speech-recognition software break down educational language barriers?". Curiosity.com. Discovery Communications. Archived from the original on 7 April 2014. Retrieved 26 March 2014..</span></p></li><li id="quote_87"><span class="references-num">[87]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Speech Recognition for Learning". National Center for Technology Innovation. 2010. Archived from the original on 13 April 2014. Retrieved 26 March 2014..</span></p></li><li id="quote_88"><span class="references-num">[88]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Follensbee, Bob; McCloskey-Dale, Susan (2000). "Speech recognition in schools: An update from the field". Technology And Persons With Disabilities Conference 2000. Archived from the original on 21 August 2006. Retrieved 26 March 2014..</span></p></li><li id="quote_89"><span class="references-num">[89]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Overcoming Communication Barriers in the Classroom". MassMATCH. 18 March 2010. Archived from the original on 25 July 2013. Retrieved 15 June 2013..</span></p></li><li id="quote_90"><span class="references-num">[90]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Speech recognition for disabled people". Archived from the original on 4 April 2008..</span></p></li><li id="quote_91"><span class="references-num">[91]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">朋友国际支持小组.</span></p></li><li id="quote_92"><span class="references-num">[92]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Garrett, Jennifer Tumlin; et al. (2011). "Using Speech Recognition Software to Increase Writing Fluency for Individuals with Physical Disabilities". Journal of Special Education Technology. 26 (1): 25–41. doi:10.1177/016264341102600104..</span></p></li><li id="quote_93"><span class="references-num">[93]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">辅助技术:增强残疾学生的能力信息交换所75.3(2002):122–6。网络。.</span></p></li><li id="quote_94"><span class="references-num">[94]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Tang, K. W.; Kamoua, Ridha; Sutan, Victor (2004). "Speech Recognition Technology for Disabilities Education". Journal of Educational Technology Systems. 33 (2): 173–84. CiteSeerX 10.1.1.631.3736. doi:10.2190/K6K8-78K2-59Y7-R9R2..</span></p></li><li id="quote_95"><span class="references-num">[95]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Projects: Planetary Microphones". The Planetary Society. Archived from the original on 27 January 2012..</span></p></li><li id="quote_96"><span class="references-num">[96]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Caridakis, George; Castellano, Ginevra; Kessous, Loic; Raouzaiou, Amaryllis; Malatesta, Lori; Asteriadis, Stelios; Karpouzis, Kostas (19 September 2007). Multimodal emotion recognition from expressive faces, body gestures and speech. IFIP the International Federation for Information Processing (in 英语). 247. Springer US. pp. 375–388. doi:10.1007/978-0-387-74161-1_41. ISBN 978-0-387-74160-4..</span></p></li><li id="quote_97"><span class="references-num">[97]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">再见，阿尔伯托。"性能评估报告的原型."日晷工作包8000 (1993)。.</span></p></li><li id="quote_98"><span class="references-num">[98]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Gerbino，e .，Baggia，p .，Ciaramella，a . &amp; Rullent，C. (1993年，4月)。口语对话系统的测试与评估。声学、语音和信号处理，1993年。ICASSP-93。，1993年IEEE国际会议(第2卷，第135-138页)。IEEE。.</span></p></li><li id="quote_99"><span class="references-num">[99]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">国家标准和技术研究所。“NIST自动语音识别历史评价 Archived 8 10月 2013 at the Wayback Machine“。.</span></p></li><li id="quote_100"><span class="references-num">[100]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Listen Up: Your AI Assistant Goes Crazy For NPR Too". NPR. 6 March 2016. Archived from the original on 23 July 2017..</span></p></li><li id="quote_101"><span class="references-num">[101]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Claburn, Thomas (25 August 2017). "Is it possible to control Amazon Alexa, Google Now using inaudible commands? Absolutely". The Register. Archived from the original on 2 September 2017..</span></p></li><li id="quote_102"><span class="references-num">[102]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">"Attack Targets Automatic Speech Recognition Systems". vice.com. 31 January 2018. Archived from the original on 3 March 2018. Retrieved 1 May 2018..</span></p></li><li id="quote_103"><span class="references-num">[103]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Beigi, Homayoon (2011). Fundamentals of Speaker Recognition. New York: Springer. ISBN 978-0-387-77591-3. Archived from the original on 31 January 2018..</span></p></li><li id="quote_104"><span class="references-num">[104]</span><p><a class="ref-back-btn">^</a><span data-url="" class="">Deng, Li; Yu, Dong (2014). "Deep Learning: Methods and Applications" (PDF). Foundations and Trends in Signal Processing. 7 (3–4): 197–387. CiteSeerX 10.1.1.691.3679. doi:10.1561/2000000039. Archived (PDF) from the original on 22 October 2014..</span></p></li></ul></div><div class="read-num">阅读 <!-- -->5866</div></div><div class="right-side" id="rightSide"><div class="side" id="lemma-side"><div class="side-title">版本记录</div><ul class="side-lst"><li><p class="side-lst-txt">暂无</p></li></ul><div class="user-card userCard"></div></div><div class="side"><div class="side-event"></div></div></div></div><div class="footer-box"><div id="footer"><div class="footer-logo-wrap"><div class="footer-logo"></div><div class="footer-logo-text">知识·传播·科普</div></div><div class="footer-info">本网站内容采用<a target="_blank" href="https://web.archive.org/web/20221028212226/https://creativecommons.org/licenses/by-sa/3.0/deed.zh?tdsourcetag=s_pctim_aiomsg">CC-BY-SA 3.0</a>授权</div><div class="footer-btn-wrap"><a target="_blank" href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/help/#user_protocol">用户协议</a><a target="_blank" href="https://web.archive.org/web/20221028212226/http://www.sogou.com/docs/terms.htm?v=1">免责声明</a><a target="_blank" href="https://web.archive.org/web/20221028212226/http://corp.sogou.com/private.html">隐私政策</a><a target="_blank" href="https://web.archive.org/web/20221028212226/https://baike.sogou.com/kexue/intro.htm">关于我们</a></div></div></div><script>window.lemmaInfo ={"lemmaId":"10951","versionId":"54416935697310214","title":"语音识别","subtitle":"","abstracts":{"paragraphId":"54416935730864642","title":"摘要","versionId":"54416935697310215","lemmaId":10951,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82812762,"name":"雷克斯班纳","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599731021,"comment":null,"dependVersionId":0,"contentType":2,"content":"<p><b>语音识别</b>是计算语言学的跨学科子领域，利用其开发方法和技术，能够通过计算机识别和翻译口语。也被称为<b>自动语音识别技术</b>（<b>ASR</b>)，<b>计算机语音识别</b>或<b>语音到文本</b>（<b>STT</b>)技术。它融合了语言学、计算机科学和电气工程领域的知识和研究。</p>\n<p>一些语音识别系统需要“训练”(也称为“注册”)，其中个体说话者将文本或孤立的词汇读入系统。该系统分析该人的特定声音，并使用它来微调对该人语音的识别，从而提高准确性。不使用训练的系统被称为“说话者无关”<sup><a href=\"#quote_1\" class=\"kx_ref\">[1]</a></sup>系统。使用训练的系统被称为“说话者相关”。</p>\n<p>语音识别应用包括语音用户界面，例如语音拨号(例如“呼叫总部”)、呼叫路由(例如“我想打对方付费电话”)、多用户设备控制、搜索(例如找到说出特定单词的播客)、简单的数据输入(例如输入信用卡号码)、结构化文档的准备(例如放射学报告)、确定说话者特征，<sup><a href=\"#quote_2\" class=\"kx_ref\">[2]</a></sup>语音到文本处理(例如文字处理器或电子邮件)和飞机(通常称为直接语音输入)。</p>\n<p>术语 voice recognition<sup><a href=\"#quote_3\" class=\"kx_ref\">[3]</a></sup><sup><a href=\"#quote_4\" class=\"kx_ref\">[4]</a></sup><sup><a href=\"#quote_5\" class=\"kx_ref\">[5]</a></sup>或者speaker identification<sup><a href=\"#quote_6\" class=\"kx_ref\">[6]</a></sup><sup><a href=\"#quote_7\" class=\"kx_ref\">[7]</a></sup>指的是识别说话者，而不是他们在说什么。识别说话人可以简化为在已经对特定人语音训练的系统中翻译语音的任务，或者作为安全过程的一部分来验证说话人的身份。</p>\n<p>从技术角度来看，语音识别有着悠久的历史，并且经历了几次重大创新浪潮。近年来，该领域受益于深度学习和大数据技术的进步。这些进步不仅体现在该领域发表的学术论文激增上，更重要的是体现在世界范围内的各行各业在设计和部署语音识别系统时均采用了各种深度学习方法。</p>","pics":[{"originalUrl":"https://web.archive.org/web/20221028212226/https://img02.sogoucdn.com/app/a/200698/800_600_20200910174330-1366981798.gif?width=227&height=174&titlename=&w=800&h=600","url":"https://web.archive.org/web/20221028212226/https://img02.sogoucdn.com/app/a/200698/800_600_20200910174330-1366981798.gif","rw":800,"rh":600,"title":"","alt":null,"width":227,"height":174}],"card":null,"references":[],"versionCount":0},"card":{"paragraphId":"0","title":"","versionId":"0","lemmaId":0,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":0,"comment":null,"dependVersionId":0,"contentType":0,"content":"","pics":null,"card":null,"references":null,"versionCount":0},"categories":[{"id":14,"name":"电子与通信技术","parents":[]}],"creator":{"uid":10145103,"name":"柚子otto","pic":"https://web.archive.org/web/20221028212226/https://img02.sogoucdn.com/app/a/200698/1152_1152_1864088_20200427232849-1518843971.png","introduction":"","educations":[{"schoolName":"中国地质大学（北京）","major":"","degree":"本科","universityId":22,"universityLogo":"https://web.archive.org/web/20221028212226/https://img01.sogoucdn.com/app/a/200943/d3465c1c-6011-11e9-b353-fc4dd4f70029","majorLevel1":"理学","majorLevel2":"地理学","majorLevel3":"地图学与地理信息系统","majorLevel1Id":1,"majorLevel2Id":103,"majorLevel3Id":109,"state":"毕业","lab":"","researchField":""}],"jobs":[{"company":"搜狗","title":"产品经理"}],"works":null,"educationBrief":"中国地质大学（北京）","jobBrief":"产品经理","role":0,"roleName":null,"title":"中国地质大学（北京） · 地理学本科","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":139,"partnerIdCreateTime":1595844881,"partnerIdPoped":true},"createTime":1569478612,"editor":{"uid":10145103,"name":"柚子otto","pic":"https://web.archive.org/web/20221028212226/https://img02.sogoucdn.com/app/a/200698/1152_1152_1864088_20200427232849-1518843971.png","introduction":"","educations":[{"schoolName":"中国地质大学（北京）","major":"","degree":"本科","universityId":22,"universityLogo":"https://web.archive.org/web/20221028212226/https://img01.sogoucdn.com/app/a/200943/d3465c1c-6011-11e9-b353-fc4dd4f70029","majorLevel1":"理学","majorLevel2":"地理学","majorLevel3":"地图学与地理信息系统","majorLevel1Id":1,"majorLevel2Id":103,"majorLevel3Id":109,"state":"毕业","lab":"","researchField":""}],"jobs":[{"company":"搜狗","title":"产品经理"}],"works":null,"educationBrief":"中国地质大学（北京）","jobBrief":"产品经理","role":0,"roleName":null,"title":"中国地质大学（北京） · 地理学本科","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":139,"partnerIdCreateTime":1595844881,"partnerIdPoped":true},"editTime":1576234418,"state":1,"versionCount":1,"upNum":2,"downNum":0,"pics":[{"originalUrl":"https://web.archive.org/web/20221028212226/https://img02.sogoucdn.com/app/a/200698/800_600_20200910174330-1366981798.gif?width=227&height=174&titlename=&w=800&h=600","url":"https://web.archive.org/web/20221028212226/https://img02.sogoucdn.com/app/a/200698/800_600_20200910174330-1366981798.gif","rw":800,"rh":600,"title":"","alt":null,"width":227,"height":174}],"catalogs":[{"level":1,"title":"历史","paragraphId":"14996178911363597","subCatalogs":[{"level":2,"title":"1970年前","paragraphId":"14996178911363597","subCatalogs":null},{"level":2,"title":"1970-1990年","paragraphId":"14996178911363597","subCatalogs":null},{"level":2,"title":"实用语音识别","paragraphId":"14996178911363597","subCatalogs":null}]},{"level":1,"title":"模型、方法和算法","paragraphId":"14996178911363598","subCatalogs":[{"level":2,"title":"隐马尔可夫模型","paragraphId":"14996178911363598","subCatalogs":null},{"level":2,"title":"基于动态时间规整(DTW)的语音识别","paragraphId":"14996178911363598","subCatalogs":null},{"level":2,"title":"神经网络","paragraphId":"14996178911363598","subCatalogs":null},{"level":2,"title":"端到端自动语音识别","paragraphId":"14996178911363598","subCatalogs":null}]},{"level":1,"title":"应用程序","paragraphId":"14996178928140812","subCatalogs":[{"level":2,"title":"车载系统","paragraphId":"14996178928140812","subCatalogs":null},{"level":2,"title":"卫生保健","paragraphId":"14996178928140812","subCatalogs":null},{"level":2,"title":"军队","paragraphId":"14996178928140812","subCatalogs":null},{"level":2,"title":"电话和其他领域","paragraphId":"14996178928140812","subCatalogs":null},{"level":2,"title":"在教育和日常生活中的使用","paragraphId":"14996178928140812","subCatalogs":null},{"level":2,"title":"残疾人","paragraphId":"14996178928140812","subCatalogs":null},{"level":2,"title":"进一步的应用","paragraphId":"14996178928140812","subCatalogs":null}]},{"level":1,"title":"表演","paragraphId":"14996178928140813","subCatalogs":[{"level":2,"title":"准确","paragraphId":"14996178928140813","subCatalogs":null},{"level":2,"title":"安全问题","paragraphId":"14996178928140813","subCatalogs":null}]},{"level":1,"title":"更多信息","paragraphId":"14996178928140814","subCatalogs":[{"level":2,"title":"会议和期刊","paragraphId":"14996178928140814","subCatalogs":null},{"level":2,"title":"书籍","paragraphId":"14996178928140814","subCatalogs":null},{"level":2,"title":"软件","paragraphId":"14996178928140814","subCatalogs":null}]},{"level":1,"title":"参考文献","paragraphId":"-1","subCatalogs":null}],"paragraphs":[{"paragraphId":"14996178911363597","title":"历史","versionId":"54416935697310216","lemmaId":10951,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82812762,"name":"雷克斯班纳","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599731021,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>增长的关键领域是：词汇量、说话人独立性和处理速度。</p><p><h3>1970年前</h3></p><p>\n <ul>\n  <li><b>1952年–</b>三位贝尔实验室的研究人员，斯蒂芬·巴拉克，<sup><a href=\"#quote_8\" class=\"kx_ref\">[8]</a></sup>R.比德勒夫和戴维斯建立了一个名为“Audrey”的系统<sup><a href=\"#quote_9\" class=\"kx_ref\">[9]</a></sup>，用于单人语音数字识别。们的系统将共振峰定位在每个话语的功率谱中。<sup><a href=\"#quote_10\" class=\"kx_ref\">[10]</a></sup></li>\n  <li><b>1960</b>–贡纳·范特开发并出版了语音产生的源过滤模型。</li>\n  <li><b>1962</b>–IBM在1962年的世界博览会上展示了其16字的“Shoebox”机器的语音识别能力。<sup><a href=\"#quote_11\" class=\"kx_ref\">[11]</a></sup></li>\n  <li><b>1969</b>–贝尔实验室的资金枯竭了好几年，1969年，颇具影响力的约翰·皮尔斯写了一封公开信，批评并平息了语音识别研究。<sup><a href=\"#quote_12\" class=\"kx_ref\">[12]</a></sup>撤资一直持续到皮尔斯退休，詹姆斯·弗拉纳根的接任。</li>\n </ul></p><p>20世纪60年代末，罗杰·瑞迪是第一个以斯坦福大学研究生身份持续进行语音识别研究的人。以前的系统要求用户在讲出每个单词后稍作等待。瑞迪的系统发出了用于玩国际象棋的口头命令。</p><p>大约在这个时候，苏联研究人员发明了“动态时间规整（DTW）”算法，并使用它创建了一个能够对200个单词进行操作的识别器。<sup><a href=\"#quote_13\" class=\"kx_ref\">[13]</a></sup>DTW通过将语音分成短帧，例如10ms的片段，并将每个帧作为一个单元来处理。尽管DTW被后来的算法取代，但这项技术仍在继续使用。在这个时期内，实现说话人的独立性仍未得到解决。</p><p><h3>1970-1990年</h3></p><p>\n <ul>\n  <li><b>1971</b>–美国国防部高级研究计划局（DARPA）资助了五年的<i>语音识别研究</i>，语音识别研究寻求最小词汇量为1000个单词。他们认为言语理解是在语音识别方面取得进展的关键；后来证明这是不真实的。<sup><a href=\"#quote_14\" class=\"kx_ref\">[14]</a></sup> BBN 、IBM 、卡耐基梅隆大学和斯坦福研究所都参加了该项目。<sup><a href=\"#quote_15\" class=\"kx_ref\">[15]</a></sup><sup><a href=\"#quote_16\" class=\"kx_ref\">[16]</a></sup>这篇复兴的语音识别研究发表在约翰·皮尔斯的信中。</li>\n </ul></p><p>\n <ul>\n  <li><b>1972–</b>IEEE声学、语音和信号处理小组在牛顿市(马萨诸塞州)举行了一次会议。</li>\n  <li><b>1976–</b>第一届ICASSP在费城举行，此后该市一直是发表语音识别研究成果的主要场所。<sup><a href=\"#quote_17\" class=\"kx_ref\">[17]</a></sup></li>\n </ul></p><p>在20世纪60年代末伦纳德·鲍姆在国防分析研究所开发了马尔可夫链的数学计算方法。十年后，在CMU，Raj Reddy的学生James Baker和Janet M. Baker开始使用隐马尔可夫模型（HMM）进行语音识别。<sup><a href=\"#quote_18\" class=\"kx_ref\">[18]</a></sup>James Baker在本科教育期间从国防分析研究所的暑期工作中了解到HMM。<sup><a href=\"#quote_19\" class=\"kx_ref\">[19]</a></sup>HMM的使用允许研究人员将不同的知识来源，如声学、语言和语法，结合在一个统一的概率模型中。</p><p>\n <ul>\n  <li>到了<b>20世纪80年代中期</b>IBM的弗雷德·杰利内克团队创造了一种叫做Tangora的声控打字机，可以处理20000个词汇<sup><a href=\"#quote_20\" class=\"kx_ref\">[20]</a></sup>，杰利内克的统计方法不太重视模拟人脑处理和理解语音的方式，而是倾向于使用像隐马尔科夫模型这样的统计建模技术。(杰利内克的团队独立发现了隐马尔科夫模型在语音中的应用。<sup><a href=\"#quote_19\" class=\"kx_ref\">[19]</a></sup>)这引起了语言学家的争议，因为隐马尔科夫模型过于简单，无法解释人类语言的许多特征。<sup><a href=\"#quote_21\" class=\"kx_ref\">[21]</a></sup>然而，隐马尔可夫模型被证明是一种非常有用的语音建模方法，并在20世纪80年代取代了DTW成为主流的语音识别算法。<sup><a href=\"#quote_22\" class=\"kx_ref\">[22]</a></sup></li>\n </ul></p><p>\n <ul>\n  <li><b>1982年</b>–由詹姆斯和珍妮特·贝克创建的Dragon Systems<sup><a href=\"#quote_23\" class=\"kx_ref\">[23]</a></sup>是IBM的少数竞争对手之一。</li>\n </ul></p><p><h3>实用语音识别</h3></p><p>20世纪80年代还出现了n-gram 语言模型。</p><p>\n <ul>\n  <li><b>1987年</b>– 退避模型允许语言模型使用多个长度的n-gram，CSELT使用隐马尔可夫模型来识别语言。</li>\n </ul></p><p>该领域的大部分进展归功于计算机能力的迅速提高。在1976年美国国防部高级研究计划局(DARPA)的项目结束时，研究人员可以使用的最好的计算机是内存为4 MB的PDP-10 。<sup><a href=\"#quote_21\" class=\"kx_ref\">[21]</a></sup> 其解码30秒的语音可能需要100分钟。<sup><a href=\"#quote_24\" class=\"kx_ref\">[24]</a></sup> </p><p>两种实用产品是:</p><p>\n <ul>\n  <li><b>1987年</b>– Kurzweil Applied Intelligence公司的识别器</li>\n  <li><b>1990年</b>–Dragon Dictate，1990年发布的消费品<sup><a href=\"#quote_25\" class=\"kx_ref\">[25]</a></sup><sup><a href=\"#quote_26\" class=\"kx_ref\">[26]</a></sup> AT &amp; amp；在1992年部署了语音识别呼叫处理服务来进行路由电话呼叫，而不需要使用人工操作员。<sup><a href=\"#quote_27\" class=\"kx_ref\">[27]</a></sup>这项技术是由劳伦斯·拉比纳和贝尔实验室的其他人开发的。</li>\n </ul></p><p>至此，典型的商业语音识别系统的词汇量已经超过了人类的平均词汇量。<sup><a href=\"#quote_21\" class=\"kx_ref\">[21]</a></sup>Raj Reddy以前的学生黄学东，在CMU开发了Sphinx-II系统。Sphinx-II系统是第一个独立于说话人、词汇量大、连续语音识别的系统，在1992年美国国防部高级研究计划局的评估中表现最佳。处理大词汇量的连续语音是语音识别历史上的一个重要里程碑。1993年，黄学东接着找到了微软语音识别小组，Raj Reddy的学生李开复加入苹果公司，帮助开发了一款名为Casper的苹果电脑语音接口原型。</p><p>总部位于比利时的语音识别公司Lernout＆Hauspie收购了其他几家公司，包括1997年的Kurzweil Applied Intelligence公司和2000年的Dragon Systems公司。Windows XP 操作系统中使用了L&amp;H语音技术。L&amp;H一直是行业领导者，直到2001年一桩会计丑闻结束了公司的好运。L&amp;H的语音技术被ScanSoft收购，该公司于2005年成为 Nuance 。苹果最初从Nuance获得授权，为其数字助理Siri 提供语音识别功能。<sup><a href=\"#quote_28\" class=\"kx_ref\">[28]</a></sup> </p><p><strong>2000年代</strong></p><p>在2000年代，美国国防部高级研究计划局赞助了两个语音识别项目：2002年的有效、可负担的可重复使用的语音到文本转换(EARS)和全球自主语言开发(GALE)。四个小组参加了EARS项目：IBM，由BBN和LIMSI以及匹兹堡大学，剑桥大学团队，以及一个由ICSI，斯里兰卡和华盛顿大学组成的团队。EARS资助收集了交换台电话语音语料库，其中包含来自500多名发言者的260小时录音会话。<sup><a href=\"#quote_29\" class=\"kx_ref\">[29]</a></sup>GALE计划的重点是阿拉伯语和普通话广播新闻演讲。谷歌2007年，Nuance聘请了一些研究人员进行语音识别。<sup><a href=\"#quote_30\" class=\"kx_ref\">[30]</a></sup>第一个产品是GOOG-411 ，一种基于电话的目录服务。GOOG-411的录音产生了有价值的数据，帮助谷歌改进了他们的识别系统。 Google语音搜索现在支持30多种语言。</p><p>在美国国家安全局至少从2006年就开始利用一种语音识别技术对关键词进行识别。<sup><a href=\"#quote_31\" class=\"kx_ref\">[31]</a></sup>这项技术允许分析师搜索大量记录的对话，并隔离关键词。可以对记录进行索引，分析师可以在数据库上查找感兴趣的对话。一些政府研究计划侧重于语音识别的智能应用，例如DARPA的EARS计划和IARPA 的 Babel 计划。</p><p>在21世纪初，语音识别仍然由传统的方法主导，如隐马尔可夫模型结合前馈人工神经网络。<sup><a href=\"#quote_32\" class=\"kx_ref\">[32]</a></sup>然而，今天，语音识别的许多方面已经被一种叫做长短期记忆 (LSTM)的深度学习方法所取代，这是一种由Sepp Hochreiter 和Jürgen Schmidhuber 于1997年出版的递归神经网络。LSTM神经网络避免了梯度消失问题，可以进行“深度学习”任务<sup><a href=\"#quote_33\" class=\"kx_ref\">[33]</a></sup>，这需要对发生在数千个离散时间步骤前的事件进行记忆，这对语音识别很重要。大约在2007年，LSTM接受了联结主义时间分类(CTC)的训练<sup><a href=\"#quote_34\" class=\"kx_ref\">[34]</a></sup>，在某些应用中开始超越传统的语音识别。<sup><a href=\"#quote_35\" class=\"kx_ref\">[35]</a></sup>据报道，2015年，经过CTC培训的LSTM让谷歌的语音识别性能大幅提升了49%，现在所有智能手机用户都可以通过谷歌语音获得这一服务。 </p><p>声学模型使用的深度前馈(非递归)网络是由Geoffrey Hinton和他在多伦多大学的学生Li Deng以及微软研究院的同事，在2009年下半年提出的<sup><a href=\"#quote_36\" class=\"kx_ref\">[36]</a></sup>，最初是在微软和多伦多大学的合作中，后来扩展到包括IBM和谷歌(因为在他们2012年的综述论文中有“四个研究小组的共同观点”副标题)。<sup><a href=\"#quote_37\" class=\"kx_ref\">[37]</a></sup><sup><a href=\"#quote_38\" class=\"kx_ref\">[38]</a></sup>微软的一名研究主管称这一创新是“自1979年以来准确性方面最戏剧性的变化”。<sup><a href=\"#quote_39\" class=\"kx_ref\">[39]</a></sup>与过去几十年稳步递增的改进相比，深度学习的应用将单词错误率降低了30%。<sup><a href=\"#quote_39\" class=\"kx_ref\">[39]</a></sup>这项创新很快被整个领域所采用。研究人员也开始将深度学习技术用于语言建模。</p><p>在语音识别的漫长历史中，人工神经网络的浅层和深层(例如递归网络)。<sup><a href=\"#quote_40\" class=\"kx_ref\">[40]</a></sup><sup><a href=\"#quote_41\" class=\"kx_ref\">[41]</a></sup><sup><a href=\"#quote_42\" class=\"kx_ref\">[42]</a></sup>但是这些方法从来没有赢过基于区分性训练的语音生成的高斯混合模型/隐马尔可夫模型(GMM-HMM)技术。<sup><a href=\"#quote_43\" class=\"kx_ref\">[43]</a></sup>1990年，对一些关键的困难进行了方法分析，包括梯度递减<sup><a href=\"#quote_44\" class=\"kx_ref\">[44]</a></sup>和弱时间相关结构。<sup><a href=\"#quote_45\" class=\"kx_ref\">[45]</a></sup><sup><a href=\"#quote_46\" class=\"kx_ref\">[46]</a></sup>所有这些困难都是对早期缺乏大训练数据和大计算能力的补充。大多数理解这些障碍的语音识别研究人员因此离开神经网络，转而寻求生成模型方法，直到从2009-2010年左右，由于深度学习克服了所有这些困难，因此神经网络开始复苏。Hinton和Deng等人回顾了这段历史的一部分，讲述了他们如何相互协作，然后与四个群体(多伦多大学、微软、谷歌和IBM)的同事协作，引发了深度前馈神经网络在语音识别中应用的复兴。<sup><a href=\"#quote_38\" class=\"kx_ref\">[38]</a></sup><sup><a href=\"#quote_47\" class=\"kx_ref\">[47]</a></sup><sup><a href=\"#quote_48\" class=\"kx_ref\">[48]</a></sup> </p><p><strong>2010年代</strong></p><p>到2010年代初语言识别，也称为语音识别<sup><a href=\"#quote_49\" class=\"kx_ref\">[49]</a></sup><sup><a href=\"#quote_50\" class=\"kx_ref\">[50]</a></sup><sup><a href=\"#quote_51\" class=\"kx_ref\">[51]</a></sup>，明显区别于说话人识别，而说话者独立被认为是一个重大突破。在那之前，系统需要一个“训练”期。一个1987年的洋娃娃广告上写着这样一句话:“终于，那个能理解你的洋娃娃。”它被描述为“哪些孩子可以接受训练以响应他们的声音”。<sup><a href=\"#quote_11\" class=\"kx_ref\">[11]</a></sup> </p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14996178911363598","title":"模型、方法和算法","versionId":"54416935697310217","lemmaId":10951,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82812762,"name":"雷克斯班纳","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599731021,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>声学模型和语言模型都是当今基于统计的语音识别算法的重要组成部分。隐马尔可夫模型(HMM)在许多系统中被广泛使用。语言建模也用于许多其他自然语言处理应用，如文档分类或统计机器翻译。</p><p><h3>隐马尔可夫模型</h3></p><p>现代通用语音识别系统普遍基于隐马尔可夫模型，是输出一系列符号或数量序列的统计模型。隐马尔科夫模型被用于语音识别，因为语音信号可以被视为分段平稳信号或短时平稳信号。在短时间尺度(例如10毫秒)内，语音可以近似为平稳过程。语音可以被认为是一个随机的马尔可夫模型。</p><p>隐马尔科夫模型流行的另一个原因是，它们可以自动训练，使用起来简单且计算可行。在语音识别中，隐马尔可夫模型将输出一系列n维实值向量(<i>n是</i>整数，例如10)，每10毫秒输出一个。向量由倒谱系数组成，这些系数是通过对短时间语音窗口进行傅立叶变换并使用余弦变换对频谱进行去相关，然后取其中第一个(最重要的)系数而获得的。隐马尔可夫模型在每个状态下均倾向于具有对角协方差高斯混合的统计分布，这将给出每个观测向量的可能性。每个单词，或者(对于更通用的语音识别系统)每个音素，将具有不同的输出分布；一个单词或音素序列的隐马尔可夫模型是通过将单独训练的单词和音素的隐马尔可夫模型串联而成的。</p><p>上述是最常见的基于隐马尔可夫模型的语音识别方法的核心要素。现代语音识别系统使用多种标准技术的各种组合，以便与上述基本方法进行对比改善。典型的大词汇系统需要音素的上下文依赖（因此具有不同左右上下文的音素具有与HMM状态不同的实现）；它会使用倒谱归一化来规范不同的扬声器和录音条件；为了进一步使说话人归一化，可以使用声道长度归一化(VTLN)进行男女归一化，此外，最大似然线性回归(MLLR)用于更一般的扬声器改编。这些特征会有所谓的δ和δ-δ系数以捕获语音动态，此外还可以使用异方差线性判别分析(HLDA)；或者可以跳过δ和δ-δ系数并使用插接和基于LDA的投影，然后通过异方差线性判别分析或全局半连接协方差进行变换（也称为最大似然线性变换，或MLLT）(也称为最大似然线性变换，MLLT)。许多系统使用所谓的判别训练技术，这种技术省去了HMM参数估计的纯统计方法，而是优化了训练数据的一些分类相关的测量。例子有：最大互信息（MMI），最小分类错误（MCE）和最小电话错误（MPE）。</p><p>语音的解码(当系统呈现新的话语并且必须计算最可能的源句子时的术语)可能使用维特比算法寻找最佳路径，这里有两种选择，一种是动态创建包含声学和语言模型信息的组合隐马尔可夫模型，另一种是预先静态组合的隐马尔可夫模型(有限状态传感器或FST方法)。</p><p>解码的一个可能的改进是保留一组好的候选项，而不仅仅是保留最好的候选项，并且使用更好的评分函数(重新评分)对这些优秀的候选语句进行评分，这样我们就可以根据这个精确的分数选出最好的候选语句。候选集可以保存为列表(即最佳列表方法)或作为模型的子集(格子)。重新评分通常是通过尝试最小化贝叶斯风险<sup><a href=\"#quote_52\" class=\"kx_ref\">[52]</a></sup>(或其近似)实现的：我们不是以最大概率选取源句子，而是试图选取相对于所有可能的转录，选择最小化给定损失函数的句子(即，我们选取以其估计概率加权的，并与其他句子平均距离最小化的候选者)。损失函数通常是Levenshtein距离，对于特定的任务它的数值是不同的；当然，可以删减一组可能的转录语句以保持易处理性。已经设计了有效的算法来重新划分表示为加权有限状态换能器的格子，其中编辑距离表示为验证某些假设的有限状态换能器。<sup><a href=\"#quote_53\" class=\"kx_ref\">[53]</a></sup> </p><p><h3>基于动态时间规整(DTW)的语音识别</h3></p><p>动态时间规整是一种历史上用于语音识别的方法，但现在已经在很大程度上被更成功的基于隐马尔可夫模型的方法所取代。</p><p>动态时间规整是一种用于测量可能随时间或速度变化的两个序列之间相似性的算法。例如，即使在一个视频中的人走得慢，而在另一个视频中走得快，或者即使在一次观察过程中有加速和减速，也可以检测到行走模式的相似性。DTW已经应用于视频、音频和图形中–事实上，任何可以转化为线性表示的数据都可以用DTW进行分析。</p><p>一个众所周知的应用是自动语音识别，以应对不同的说话速度。一般来说，这是一种允许计算机在具有特定限制的两个给定序列(例如时间序列)之间找到最佳匹配的方法。也就是说，序列被非线性地“规整”以相互匹配。这种序列比对方法经常在隐马尔可夫模型中使用。</p><p><h3>神经网络</h3></p><p>神经网络是20世纪80年代后期在ASR中出现的一种有吸引力的声学建模方法。从那时起，神经网络已经逐渐用于语音识别的许多方面，例如音素分类，<sup><a href=\"#quote_54\" class=\"kx_ref\">[54]</a></sup>孤立单词识别，<sup><a href=\"#quote_55\" class=\"kx_ref\">[55]</a></sup>视听语音识别、视听说话者识别和说话者适应。</p><p>与隐马尔科夫模型相比，神经网络对特征统计特性的明确假设较少，并且具有多种特性，使得它们成为语音识别领域中具有吸引力的识别模型。当用于估计语音特征片段的概率时，神经网络允许以自然和有效的方式进行判别训练。然而，尽管它们在分类短时间单位（如个体音素和孤立单词）方面有效，<sup><a href=\"#quote_56\" class=\"kx_ref\">[56]</a></sup> 但早期的神经网络很少成功地完成连续识别任务，这是由于它们对时间相关性建模的能力有限。</p><p>这种限制的一种解决方法是在基于HMM的识别之前使用神经网络对语音数据进行预处理，特征变换或降维<sup><a href=\"#quote_57\" class=\"kx_ref\">[57]</a></sup>。近年来，LSTM和相关的递归神经网络<sup><a href=\"#quote_58\" class=\"kx_ref\">[58]</a></sup><sup><a href=\"#quote_58\" class=\"kx_ref\">[58]</a></sup><sup><a href=\"#quote_58\" class=\"kx_ref\">[58]</a></sup><sup><a href=\"#quote_59\" class=\"kx_ref\">[59]</a></sup>和时延神经网络(TDNN)<sup><a href=\"#quote_60\" class=\"kx_ref\">[60]</a></sup>在这一领域的表现已经被证明是有效的。</p><p><strong>深度前馈和递归神经网络</strong></p><p>深度神经网络与自动去噪编码器<sup><a href=\"#quote_61\" class=\"kx_ref\">[61]</a></sup> 也正处于研究中。深度前馈神经网络(DNN)是一种在输入和输出层之间具有多个隐藏单元层的人工神经网络。<sup><a href=\"#quote_62\" class=\"kx_ref\">[62]</a></sup>与浅层神经网络相似，DNN可以模拟复杂的非线性关系。DNN架构生成合成模型，其中额外的层允许从较低层合成特征，提供了巨大的学习能力，因此具有建模复杂语音数据模式的潜力。<sup><a href=\"#quote_62\" class=\"kx_ref\">[62]</a></sup> </p><p>2010年，工业研究人员与学术研究人员合作，在大词汇量语音识别中成功实现了DNN，其中采用了基于决策树构造的上下文相关HMM状态的DNN大输出层。<sup><a href=\"#quote_63\" class=\"kx_ref\">[63]</a></sup><sup><a href=\"#quote_64\" class=\"kx_ref\">[64]</a></sup> <sup><a href=\"#quote_65\" class=\"kx_ref\">[65]</a></sup>请参见微软研究院最近出版的《Springer》著作中对截至2014年10月的发展以及最新技术的讨论。另请参见自动语音识别的相关背景和各种机器学习范例的影响，特别是包括深度学习的技术。<sup><a href=\"#quote_66\" class=\"kx_ref\">[66]</a></sup><sup><a href=\"#quote_67\" class=\"kx_ref\">[67]</a></sup> </p><p>深度学习的一个基本原则是摒弃手工制作的特征工程而使用原始特征进行学习。这一原理首先在深度自动编码器的架构中成功地探索到“原始”频谱图或线性滤波器特征，<sup><a href=\"#quote_68\" class=\"kx_ref\">[68]</a></sup>显示出优于Mel-倒谱图的特征，后者包含从光谱图固定转换的几个阶段。语音波形的真正“原始”特征最近被证明可以生成出色的大规模语音识别结果。<sup><a href=\"#quote_69\" class=\"kx_ref\">[69]</a></sup> </p><p><h3>端到端自动语音识别</h3></p><p>自2014年以来，人们对“端到端”ASR有了很大的研究兴趣。传统的基于语音的方法(即基于 HMM 的模型)需要单独的组件和对语音、声学和语言模型的训练。端到端模型共同学习语音识别器的所有组件。这一点很重要，因为它简化了培训过程和部署过程。例如，所有基于隐马尔可夫模型的系统都需要一个 n-gram语言模型，典型的n-gram语言模型通常需要几千兆字节的内存，使得它们无法部署在移动设备上。<sup><a href=\"#quote_70\" class=\"kx_ref\">[70]</a></sup>因此，谷歌和苹果的现代商用ASR系统(截至2017年)部署在云系统中，需要网络连接才可以正常使用。</p><p>端到端ASR的第一次尝试是使用基于Connectionist Temporal class ification(CTC)的系统，该系统由Google DeepMind的Alex Graves和多伦多大学的Navdeep Jaitly于2014年引入。<sup><a href=\"#quote_71\" class=\"kx_ref\">[71]</a></sup>该模型由递归神经网络和CTC层组成。RNN-CTC模型共同学习发音和声学模型，但是由于类似于HMM的条件独立假设，它无法学习语言。因此，CTC模型具有直接学习将语音声学映射到英语字符的能力，但这些模型会犯许多常见的拼写错误，必须依赖单独的语言模型来清理转录本。后来，百度利用非常大的数据集扩展了这项工作，并用中文和英文展示了一些商业上成功的案例。<sup><a href=\"#quote_72\" class=\"kx_ref\">[72]</a></sup>2016年，牛津大学推出了LipNet，<sup><a href=\"#quote_73\" class=\"kx_ref\">[73]</a></sup>第一个端到端语句级唇读模型，使用时空卷积结合RNN-CTC架构，在受限语法数据集中超越了人类的水平。<sup><a href=\"#quote_74\" class=\"kx_ref\">[74]</a></sup>谷歌DeepMind 在2018年推出了一个大型CNN-RNN-CTC架构，其性能是人类专家的6倍。<sup><a href=\"#quote_75\" class=\"kx_ref\">[75]</a></sup> </p><p>基于CTC模型的另一种方法是基于attention-based的模型。卡内基梅隆大学和Google Brain的Chan等人和蒙特利尔大学的Bahdanau等人于2016年同时引入了基于attention-based 的ASR模型。<sup><a href=\"#quote_76\" class=\"kx_ref\">[76]</a></sup><sup><a href=\"#quote_77\" class=\"kx_ref\">[77]</a></sup>名为“听、听、拼”(Listen, Attend and Spell，LAS)，字面上是“听”声音信号，注意信号的不同部分，一次“拼”出一个字符的抄本。与基于CTC的模型不同，基于attention-based的模型没有条件独立假设，可以直接学习语音识别器的所有组件，包括语音、声学和语言模型。这意味着，在部署过程中，不需要携带语言模型，这使得它在部署到仅具有特定内存存储器的应用程序上非常实用。截至2016年底，基于attention-based的模型取得了相当大的成功，包括超越CTC模型(有或没有外部语言模型)。<sup><a href=\"#quote_78\" class=\"kx_ref\">[78]</a></sup>自最初的LAS模型以来，已经提出了各种扩展。潜在序列分解（LSD）是由卡耐基梅隆大学、麻省理工学院和谷歌大脑提出的，可以直接发出比英文字符更自然的子词单元；<sup><a href=\"#quote_79\" class=\"kx_ref\">[79]</a></sup> 牛津大学和谷歌DeepMind 将LAS扩展为“观看，收听，参加和拼写“（Watch, Listen, Attend and Spell，WLAS）处理唇读超过人类表现。。<sup><a href=\"#quote_80\" class=\"kx_ref\">[80]</a></sup> </p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14996178928140812","title":"应用程序","versionId":"54416935697310218","lemmaId":10951,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82812762,"name":"雷克斯班纳","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599731021,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p><h3>车载系统</h3></p><p>通常，手动控制输入，例如通过方向盘上的手指控制，启动语音识别系统，并通过音频提示向驾驶员发出信号。在音频提示后，系统有一个“监听窗口”，在此期间它可以接受语音输入以进行识别。</p><p>简单的语音命令可用于发起电话呼叫、选择无线电台或从兼容的智能手机、MP3播放器或音乐加载闪存驱动器中播放音乐。声音识别能力因车型而异。一些最新的汽车模型提供自然语言语音识别代替固定的一组命令，允许驾驶员使用完整的句子和常用短语。因此，使用这样的系统，用户不需要记住一组固定的命令字。</p><p><h3>卫生保健</h3></p><p><strong>医疗文件</strong></p><p>在医疗保健领域，语音识别可以在医疗记录过程的前端或后端实现。前端语音识别是指命令者向语音识别引擎发出指令，识别出的单词在说话时显示出来，命令者负责编辑和签署文档。后端或延迟语音识别是指命令者将语音输入到数字听写系统中，语音通过语音识别机器传送，识别出的草稿文档与原始语音文件一起传送到编辑器，在编辑器中编辑草稿并最终完成报告。延迟语音识别目前在行业中广泛使用。</p><p>在医疗保健中使用语音识别的一个主要问题是，2009年美国复苏与再投资法案 ( ARRA )根据“有价值的使用”标准为使用EMR的医生提供了大量的经济利益。这些标准要求EMR必须维护大量数据(现在更普遍地称为电子健康记录或EHR)。语音识别的使用更自然地适合于叙事文本的生成，作为放射学/病理学解释、治疗进度注释或出院总结的一部分：对于视力正常并能操作键盘和鼠标的人来说，使用语音识别输入结构化离散数据(例如，列表或词汇表中的数值或代码)的人机工程收益相对较小。</p><p>一个更重要的问题是，大多数EHR没有被明确规定必须利用语音识别能力。临床医生与EHR的大部分互动涉及使用菜单和选项卡/按钮点击导航用户界面，并且严重依赖于键盘和鼠标：基于语音的导航仅提供适度的人机工程益处。相比之下，许多高度定制的放射学或病理学听写系统实现了语音“宏”，其中某些短语(例如，“正常”)的使用将自动填写大量默认值或生成样板文件，样板文件将随检查类型而变化，例如，例如胸部X射线与放射系统的胃肠对比系列。</p><p>作为手动导航的替代方案，语音识别和信息提取的级联使用已经被研究<sup><a href=\"#quote_81\" class=\"kx_ref\">[81]</a></sup>作为填写临床验证和签核交接表的一种方式。结果令人鼓舞，论文还将数据、相关性能基准和一些处理软件向研究和社区开放，用于研究临床文档和语言处理。</p><p><strong>治疗用途</strong></p><p>长期使用语音识别软件与文字处理器相结合，已经展现出接受切除术治疗的脑血管瘤患者的短期记忆再强化的好处。需要进行进一步的研究来确定使用放射技术治疗脑血管瘤(AVM)的益处。</p><p><h3>军队</h3></p><p><strong>高性能战斗机</strong></p><p>在过去的十年中，人们在测试和评估战斗机中的语音识别方面做了大量的工作。值得注意的是美国的高级战斗机技术集成(AFTI) / F-16 飞机(F-16 VISTA )语音识别计划，法国的幻影飞机计划，以及英国各种飞机平台计划。在这些程序中，语音识别器已经在战斗机上成功运行，应用包括:设置无线电频率、命令自动驾驶系统、设置转向点坐标和武器释放参数以及控制飞行显示。</p><p>与在JAS-39 Gripen驾驶舱内飞行的瑞典飞行员一起工作的Englund（2004）发现随着g负荷的增加，识别能力也在下降。该报告还得出结论，适应性极大地改善了所有病例的结果，并且显示呼吸模型的引入显著提高了识别分数。与预期相反，并没有发现说话者的蹩脚英语的影响。正如人们所期望的那样，自发语音会给识别器带来问题。因此，有限的词汇，最重要的是正确的语法，会大大提高识别精度。<sup><a href=\"#quote_82\" class=\"kx_ref\">[82]</a></sup> </p><p>目前在英国皇家空军服役的台风战斗机采用了一个与扬声器相关的系统，要求每个飞行员创建一个模板。该系统不用于任何安全或武器任务，如武器释放或起落架放下，但用于此外的其他驾驶舱功能。语音命令由视觉或听觉反馈确认。该系统被视为减少飞行员工作量的主要设计,<sup><a href=\"#quote_83\" class=\"kx_ref\">[83]</a></sup>甚至允许飞行员用两个简单的语音指令给飞机指定目标，或者只用五个指令给任何一个僚机指定目标。<sup><a href=\"#quote_84\" class=\"kx_ref\">[84]</a></sup> </p><p>独立于扬声器的系统也正在开发中，并正在 F35闪电二号 (JSF)和M-346教练机中进行测试。这些系统已经产生了超过98%的单词准确性率。<sup><a href=\"#quote_85\" class=\"kx_ref\">[85]</a></sup> </p><p><strong>直升机</strong></p><p>在压力和噪声下获得高识别精度的问题与直升机和喷气式战斗机飞行环境密切相关。声学噪声问题在直升机环境中实际上更为严重，不仅是因为高噪声水平，而且因为直升机飞行员通常不佩戴能够降低麦克风中声学噪声的面罩。在过去的十年中，直升机语音识别系统的应用已经进行了大量的测试和评估，尤其是由美国陆军航空电子研究发展中心(AVRADA)和英国皇家航空航天机构( RAE )的成就比较明显。法国的Puma直升机的语音识别、加拿大的也进行了不少工作。成果令人鼓舞，语音应用包括:通信无线电控制、导航系统设置和自动目标切换控制。</p><p>与战斗机应用一样，直升机中语音识别的首要问题是对飞行员效率的影响。据报告，AVRADA试验取得了令人鼓舞的结果，尽管这些结果只是试验环境中的可行性演示。因此，在语音识别和整体语音技术方面还有许多工作要做，以便在操作环境中持续实现性能改进。</p><p><strong>训练空中交通管制员</strong></p><p>空中交通管制员的培训是语音识别系统的一个很好的应用。许多空中交通管制训练系统目前需要一个人充当“伪飞行员”，与训练员控制器进行语音对话，模拟控制器在真实空中交通管制情况下与飞行员进行的对话。语音识别和合成技术有可能消除人们充当伪飞行员的需要，从而减少培训和地面支持人员。理论上，空中控制器语音任务具有高度结构化的语音输出特征，因此一定程度上降低了语音识别任务的难度。实际上，情况很少如此。美国联邦航空局文件7110.65详细说明了空中交通管制员应该使用的短语。虽然本文给出的这种短语的例子不到150个，但是由模拟供应商的语音识别系统支持的短语数量超过了500000个。</p><p>美国空军、美国海军陆战队、美国陆军、美国海军和联邦航空局以及许多国际空中交通管制培训组织，如澳大利亚皇家空军和意大利、巴西和加拿大的民航局，目前正在使用带有来自许多不同供应商的具有语音识别功能的空中交通管制模拟器。</p><p><h3>电话和其他领域</h3></p><p>ASR如今在电话领域很常见，并且在计算机游戏和模拟领域越来越广泛。在电话系统中，ASR现在主要通过与交互式语音应答IVR 系统集成来使用。尽管广泛应用于一般个人计算与字处理的高度集成，但在文档制作领域，ASR的使用没有达到预期的效果。</p><p>移动处理器速度的提高使语音识别在智能手机中变得切实可行。语音主要用作用户界面的一部分，用于创建预定义或自定义的语音命令。</p><p><h3>在教育和日常生活中的使用</h3></p><p>对于语言学习，语音识别对于学习第二语言很有用。它能教人正确的发音，还能帮助一个人训练流利的口语。<sup><a href=\"#quote_86\" class=\"kx_ref\">[86]</a></sup> </p><p>失明或视力低下的学生可以受益于使用这种技术来传达单词，然后听到计算机对它们进行朗读，还可以通过用声音发出指令来使用计算机，而不必看屏幕和键盘。<sup><a href=\"#quote_87\" class=\"kx_ref\">[87]</a></sup> </p><p>身体残疾或患有重复性劳损或上肢损伤的学生可以通过使用语音到文本的程序，而不必担心手写、打字或与抄写员一起完成学校作业。他们还可以利用语音识别技术来自由享受互联网或单独使用计算机的乐趣，而不必亲自操作鼠标和键盘。<sup><a href=\"#quote_87\" class=\"kx_ref\">[87]</a></sup> </p><p>语音识别可以让有学习障碍的学生成为更好的作家。通过大声说出单词，他们可以增加写作的流动性，减轻对拼写、标点符号和其它写作技巧的担忧。<sup><a href=\"#quote_88\" class=\"kx_ref\">[88]</a></sup> </p><p>使用语音识别软件，配合数字录音机和运行文字处理软件的个人计算机，已被证明对恢复中风和开颅患者受损的短期记忆能力是有益的。</p><p><h3>残疾人</h3></p><p>残疾人可以从语音识别项目中受益。对于聋人或听力困难的人，语音识别软件用于自动生成对话的隐藏字幕，例如会议室讨论、课堂讲座或宗教服务。<sup><a href=\"#quote_89\" class=\"kx_ref\">[89]</a></sup> </p><p>语音识别对使用手有困难的人也非常有用，从轻微的重复性应激损伤到涉及使用传统计算机输入设备的残疾。事实上，经常使用键盘并发展出 RSI 的人成为了语音识别的一个迫切的早期市场。<sup><a href=\"#quote_90\" class=\"kx_ref\">[90]</a></sup><sup><a href=\"#quote_91\" class=\"kx_ref\">[91]</a></sup>语音识别用于聋人电话，例如语音邮件到文本、中继服务和字幕电话。有学习障碍的人在思想到纸张的交流中有问题(本质上他们想到了一个想法，但处理不准确会导致表达错误)可能会从软件中受益，但该技术不是防错的。<sup><a href=\"#quote_92\" class=\"kx_ref\">[92]</a></sup>此外，对智力残疾人来说，与文本对话的整个想法可能很难实现，因为很少有人试图学习这种技术来教育残疾人。<sup><a href=\"#quote_93\" class=\"kx_ref\">[93]</a></sup> </p><p>这种技术可以帮助有阅读障碍的人，但其他残疾仍然存在问题。产品的有效性是阻碍其有效性的关键。虽然一个孩子可能会说一个词，这取决于他们说得有多清楚，但系统可能会认为他们说的是另一个词而输入错误的信息。给他们更多的工作去修正，导致他们不得不花更多的时间去修正错误的单词。<sup><a href=\"#quote_94\" class=\"kx_ref\">[94]</a></sup> </p><p><h3>进一步的应用</h3></p><p>\n <ul>\n  <li>航空航天(如空间探索、航天器等)，美国宇航局的火星极地着陆者号在着陆器上的麦克风中使用了传感公司的语音识别技术<sup><a href=\"#quote_95\" class=\"kx_ref\">[95]</a></sup></li>\n  <li>带语音识别的自动字幕</li>\n  <li>自动情感识别<sup><a href=\"#quote_96\" class=\"kx_ref\">[96]</a></sup></li>\n  <li>自动翻译</li>\n  <li>法庭报告(实时演讲写作)</li>\n  <li>电子发现(法律发现)</li>\n  <li>免提计算：语音识别计算机用户界面</li>\n  <li>智能家居</li>\n  <li>交互式语音响应</li>\n  <li>移动电话，包括移动电子邮件</li>\n  <li>多模式交互</li>\n  <li>计算机辅助语言学习应用中的发音评估</li>\n  <li>实时字幕</li>\n  <li>机器人</li>\n  <li>语音到文本(将语音转录为文本、实时视频字幕、法庭报告)</li>\n  <li>远程信息处理系统(例如车辆导航系统)</li>\n  <li>转录(数字语音到文本)</li>\n  <li>电子游戏（汤姆克兰西的EndWar和Lifeline）</li>\n  <li>虚拟助手(例如苹果Siri )</li>\n </ul></p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14996178928140813","title":"表演","versionId":"54416935697310219","lemmaId":10951,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82812762,"name":"雷克斯班纳","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599731021,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p>语音识别系统的性能通常根据准确性和速度来评估。<sup><a href=\"#quote_97\" class=\"kx_ref\">[97]</a></sup><sup><a href=\"#quote_98\" class=\"kx_ref\">[98]</a></sup>准确度通常用单词错误率来评定(WER)，而速度用实时性来测量。其它衡量准确性的指标包括单字错误率 (SWER)和命令成功率 (CSR)。</p><p>然而，机器语音识别是一个非常复杂的问题。发声在口音、发音、清晰度、粗糙度、鼻音、音高、音量和速度方面各不相同。语音被背景噪声和回声、电特性所扭曲。语音识别的准确性可能因以下因素而异：<sup><a href=\"#quote_99\" class=\"kx_ref\">[99]</a></sup> </p><p>\n <ul>\n  <li>词汇量和易混淆性</li>\n  <li>词汇量大小和混淆</li>\n  <li>说话者依赖与独立</li>\n  <li>孤立、不连续或连续的语音</li>\n  <li>任务和语言限制</li>\n  <li>阅读与自发言论</li>\n  <li>不利条件</li>\n </ul></p><p><h3>准确</h3></p><p>如本文前面所述，语音识别的准确性可能因以下因素而异:</p><p>\n <ul>\n  <li>错误率随着词汇量的增加而增加:</li>\n </ul></p><p>\n <dl>\n  <dd>\n   <dl>\n    <dd>\n     例如，从“0”到“9”的10位数字基本上可以被完美地识别，但是200、5000或100000的词汇量可能分别具有3%、7%或45%的错误率。\n    </dd>\n   </dl>\n  </dd>\n </dl></p><p>\n <ul>\n  <li>词汇如果包含容易混淆的单词，就很难识别:</li>\n </ul></p><p>\n <dl>\n  <dd>\n   <dl>\n    <dd>\n     例如，英语字母表中的26个字母很难区分，因为它们是容易混淆的单词(最明显的是E-set:“B，C，D，E，G，P，T，V，Z”)；8%的错误率被认为是这几个字母造成的。\n    </dd>\n   </dl>\n  </dd>\n </dl></p><p>\n <ul>\n  <li>说话者依赖性与独立性:</li>\n </ul></p><p>\n <dl>\n  <dd>\n   <dl>\n    <dd>\n     与扬声器相关的系统旨在由单个扬声器使用。\n    </dd>\n    <dd>\n     独立于扬声器的系统适用于任何扬声器(难度更大)。\n    </dd>\n   </dl>\n  </dd>\n </dl></p><p>\n <ul>\n  <li>孤立、不连续或连续的语音</li>\n </ul></p><p>\n <dl>\n  <dd>\n   <dl>\n    <dd>\n     对于孤立的语音，使用单个单词，因此更容易识别。\n    </dd>\n   </dl>\n  </dd>\n </dl></p><p>对于不连续的语音，使用由静音分隔的完整句子，因此与孤立的语音一样，识别语音变得更容易。<br>连续语音使用连续的自然口语句子，因此与孤立和不连续语音不同，语音识别变得更加困难。</p><p>\n <ul>\n  <li>任务和语言限制\n   <ul>\n    <li>例如，查询应用程序可能会驳回“苹果是红色的”假设。</li>\n    <li>例如，约束可以是语义化的；拒绝“苹果生气了”</li>\n    <li>例如，句法；拒绝“红色就是苹果”</li>\n   </ul></li>\n </ul></p><p>约束通常由语法表示。</p><p>\n <ul>\n  <li>阅读与自发言语——当一个人阅读时，通常是在事先准备好的背景下进行的，但是当一个人使用自发言语时，由于不流畅(如“呃”和“嗯”、错误的开头、不完整的句子、口吃、咳嗽和笑声)以及词汇有限，就很难识别出该言语。</li>\n  <li>不利条件——环境噪声(例如汽车或工厂中的噪声)。声学失真(例如回声、室内声学)</li>\n </ul></p><p>语音识别是一项多层次的模式识别任务。</p><p>\n <ul>\n  <li>声学信号被构造成单元的层次结构，例如音素、单词、短语和句子；</li>\n  <li>每个级别都提供了额外的约束；</li>\n </ul></p><p>例如已知单词发音或法律单词序列，其可以补偿较低水平的错误或不确定性；</p><p>\n <ul>\n  <li>利用这种约束层次。机器语音识别是一个分成几个阶段的过程，通过在所有较低层次上的组合概率决策，而只在最高层次上做出更确定性的决策。在计算上，这是一个声音模式必须被识别或分类到对人类来说有意义问题类别。每个声音信号都可以分解成更小、更基本的子信号。当更复杂的声音信号被分解成更小的子声音时，会产生不同的声音级别，在最高级别，我们会产生复杂的声音，这些声音由较低级别、更简单的声音组成，而在更低的级别，我们会产生更基本、更短、更简单的声音。最低级别，声音是最基本的，机器将检查声音应代表的简单和更概率的规则。一旦这些声音在上层组合成更复杂的声音，一组新的更确定的规则会预测新的复杂声音所代表的内容。确定性规则的最高层应该理解复杂表达式的含义。为了扩展我们关于语音识别的知识，就需要考虑神经网络。神经网络方法有四个步骤:</li>\n  <li>数字化我们想要识别的语音</li>\n </ul></p><p>对于电话语音，采样率为每秒8000个样本；</p><p>\n <ul>\n  <li>计算语音的谱域特征(傅里叶变换)；</li>\n </ul></p><p>每10毫秒计算一次，每个10毫秒的部分称为一帧；</p><p>对四步神经网络方法的分析可以通过进一步的信息来解释。声音是由空气(或其他介质)振动产生的，我们用耳朵记录，但机器用接收器记录。基本声音产生的波有两种描述：振幅(大小)和频率(每秒振动的次数)。</p><p><h3>安全问题</h3></p><p>语音识别可以成为攻击、盗窃或意外操作的手段。例如，音频或视频广播中所说的“Alexa”之类的激活词可能会导致家庭和办公室中的设备开始不恰当地收听外部音频输入，或者可能采取不必要的行动。<sup><a href=\"#quote_100\" class=\"kx_ref\">[100]</a></sup>如果可以在室内听到，语音控制设备也可以被大楼的访客使用，甚至是大楼外的访客。攻击者可能会访问个人信息，如日历、地址簿内容、私人消息和文档。他们也可以模拟用户发送消息或在线购物。</p><p>已经证明存在两种使用人工声音的攻击。人们发射超声波，试图在附近的人没有注意的情况下发送命令。<sup><a href=\"#quote_101\" class=\"kx_ref\">[101]</a></sup>另一种是为其他语音或音乐添加小的不可见的失真，这些语音或音乐被定做以混淆特定的语音识别系统将音乐识别为语音，或者使听起来像人类对系统发出的一个命令的声音。<sup><a href=\"#quote_102\" class=\"kx_ref\">[102]</a></sup> </p>","pics":null,"card":null,"references":[],"versionCount":0},{"paragraphId":"14996178928140814","title":"更多信息","versionId":"54416935697310220","lemmaId":10951,"createType":0,"creator":{"uid":0,"name":null,"pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"createTime":0,"versionEditor":{"uid":82812762,"name":"雷克斯班纳","pic":null,"introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":null,"jobBrief":null,"role":0,"roleName":null,"title":null,"professionalTitle":null,"phoneNo":null,"editable":false,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false},"editTime":1599731021,"comment":null,"dependVersionId":0,"contentType":1,"content":"<p><h3>会议和期刊</h3></p><p>每年或每两年举行的流行语音识别会议包括SpeechTEK和SpeechTEK Europe，ICASSP，Interspeech / Eurospeech和IEEE ASRU。自然语言处理领域的会议，如ACL，NAACL，EMNLP和HLT，包括有关语音处理的论文。重要期刊包括 IEEE Transactions on Speech and Audio Processing(后来改名IEEE Transactions on Audio, Speech and Language Processing，自2014年9月起更名为IEEE/ACM Transactions on Audio, Speech and Language Processing—随后，与ACM出版物合并)、计算机语音和语言（Computer Speech and Language）以及语音通信（Speech Communication）。</p><p><h3>书籍</h3></p><p>劳伦斯·拉比纳的《语音识别基础》有助于学习一些语音识别的基础知识，但可能不是完全最新的(1993)。此外还有Frederick Jelinek 的《语音识别统计方法》和黄东学的《口语处理(2001)》等。最新的是Manfred R. Schroeder 的《计算机语音》（2004年，第二版），李登和Doug O'Shaughnessey于2003年出版的《语音处理：动态和优化导向的方法》。最近更新的《<i>语音和语言处理</i>(2008)》由Jurafsky和Martin编写，主要介绍了ASR的基础知识和最新技术。说话人识别也使用与语音识别相同的功能、大多数相同的前端处理和分类技术。一本最新的综合性教科书《说话人识别基础》是关于理论和实践最新细节的讨论。<sup><a href=\"#quote_103\" class=\"kx_ref\">[103]</a></sup>通过关注政府资助的评估，例如由DARPA组织的评估，可以获得对最佳现代系统中使用的语音识别技术进行深入的了解（截至2007年，最大的语音识别相关项目是GALE项目，其涉及语音识别和翻译组件）。</p><p>罗伯托·皮拉西奇（Roberto Pieraccini，2012）的《机器中的声音》提供了语音识别技术及一个很好且容易理解的介绍。</p><p>最近一本关于语音识别的书是《<i>自动语音识别:深度学习方法》</i>(Publisher: Springer)由D. Yu和L. Deng撰写，于2014年底出版，在基于dnn和相关深度学习方法的现代语音识别系统中推导和实现上具有高度数学化的技术细节，可以深入了解学习方法的深度。<sup><a href=\"#quote_104\" class=\"kx_ref\">[104]</a></sup>D. Yu和L. Deng在2014年初出版的一本相关书籍《深度学习:方法和应用》，提供了2009-2014年期间基于DNN的语音识别的技术含量较低但更注重方法的概述，该书被放在深度学习应用的一般框架中，不仅包括语音识别，还包括图像识别、自然语言处理、信息检索、多模式处理和多任务学习。<sup><a href=\"#quote_104\" class=\"kx_ref\">[104]</a></sup> </p><p><h3>软件</h3></p><p>就免费资源而言，卡内基梅隆大学的Sphinx工具包是开始学习语音识别和开展实验的一个工具。另一个资源(免费但受版权保护)是 HTK 书籍(以及附带的HTK工具包)。如果需要更新、最先进的技术，可以使用 Kaldi 工具包。</p><p>Cobalt的网页上有在线语音识别器的演示。<sup><a href=\"#quote_104\" class=\"kx_ref\">[104]</a></sup> </p>","pics":null,"card":null,"references":[],"versionCount":0}],"references":[{"id":1,"type":"book","title":"\"Speaker Independent Connected Speech Recognition- Fifth Generation Computer Corporation\". Fifthgen.com. Archived from the original on 11 November 2013. Retrieved 15 June 2013.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":2,"type":"book","title":"P. Nguyen (2010). \"Automatic classification of speaker characteristics\".","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":3,"type":"book","title":"\"British English definition of voice recognition\". Macmillan Publishers Limited. Archived from the original on 16 September 2011. Retrieved 21 February 2012.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":4,"type":"book","title":"\"voice recognition, definition of\". WebFinance, Inc. Archived from the original on 3 December 2011. Retrieved 21 February 2012.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":5,"type":"book","title":"\"The Mailbag LG #114\". Linuxgazette.net. Archived from the original on 19 February 2013. Retrieved 15 June 2013.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":6,"type":"book","title":"Reynolds, Douglas; Rose, Richard (January 1995). \"Robust text-independent speaker identification using Gaussian mixture speaker models\" (PDF). IEEE Transactions on Speech and Audio Processing. 3 (1): 72–83. doi:10.1109/89.365379. ISSN 1063-6676. OCLC 26108901. Archived (PDF) from the original on 8 March 2014. Retrieved 21 February 2014.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":7,"type":"book","title":"\"Speaker Identification (WhisperID)\". Microsoft Research. Microsoft. Archived from the original on 25 February 2014. Retrieved 21 February 2014. When you speak to someone, they don't just recognize what you say: they recognize who you are. WhisperID will let computers do that, too, figuring out who you are by the way you sound.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":8,"type":"book","title":"\"Obituaries: Stephen Balashek\". The Star-Ledger. 22 July 2012.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":9,"type":"book","title":"\"IBM-Shoebox-front.jpg\". androidauthority.net. Retrieved 4 April 2019.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":10,"type":"book","title":"Juang, B. H.; Rabiner, Lawrence R. \"Automatic speech recognition–a brief history of the technology development\" (PDF): 6. Archived (PDF) from the original on 17 August 2004. Retrieved 17 January 2015.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":11,"type":"book","title":"Melanie Pinola (2 November 2011). \"Speech Recognition Through the Decades: How We Ended Up With Siri\". PC World. Retrieved 22 October 2018.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":12,"type":"book","title":"John R. Pierce (1969). \"Whither speech recognition?\". Journal of the Acoustical Society of America. 46 (48): 1049–1051. Bibcode:1969ASAJ...46.1049P. doi:10.1121/1.1911801.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":13,"type":"book","title":"Benesty, Jacob; Sondhi, M. M.; Huang, Yiteng (2008). Springer Handbook of Speech Processing. Springer Science & Business Media. ISBN 978-3540491255.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":14,"type":"book","title":"John Makhoul. \"ISCA Medalist: For leadership and extensive contributions to speech and language processing\". Archived from the original on 24 January 2018. Retrieved 23 January 2018.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":15,"type":"book","title":"Blechman, R. O.; Blechman, Nicholas (23 June 2008). \"Hello, Hal\". The New Yorker. Archived from the original on 20 January 2015. Retrieved 17 January 2015.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":16,"type":"book","title":"Klatt, Dennis H. (1977). \"Review of the ARPA speech understanding project\". The Journal of the Acoustical Society of America. 62 (6): 1345–1366. Bibcode:1977ASAJ...62.1345K. doi:10.1121/1.381666.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":17,"type":"book","title":"Rabiner (1984). \"The Acoustics, Speech, and Signal Processing Society. A Historical Perspective\" (PDF). Archived (PDF) from the original on 9 August 2017. Retrieved 23 January 2018.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":18,"type":"book","title":"\"First-Hand:The Hidden Markov Model – Engineering and Technology History Wiki\". ethw.org. Archived from the original on 3 April 2018. Retrieved 1 May 2018.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":19,"type":"book","title":"\"James Baker interview\". Archived from the original on 28 August 2017. Retrieved 9 February 2017.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":20,"type":"book","title":"\"Pioneering Speech Recognition\". 7 March 2012. Archived from the original on 19 February 2015. Retrieved 18 January 2015.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":21,"type":"book","title":"Xuedong Huang; James Baker; Raj Reddy. \"A Historical Perspective of Speech Recognition\". Communications of the ACM. Archived from the original on 20 January 2015. Retrieved 20 January 2015.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":22,"type":"book","title":"Juang, B. H.; Rabiner, Lawrence R. \"Automatic speech recognition–a brief history of the technology development\" (PDF): 10. Archived (PDF) from the original on 17 August 2014. Retrieved 17 January 2015.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":23,"type":"book","title":"\"History of Speech Recognition\". Dragon Medical Transcription. Archived from the original on 13 August 2015. Retrieved 17 January 2015.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":24,"type":"book","title":"Kevin McKean (8 April 1980). \"When Cole talks, computers listen\". Sarasota Journal. AP. Retrieved 23 November 2015.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":25,"type":"book","title":"Melanie Pinola (2 November 2011). \"Speech Recognition Through the Decades: How We Ended Up With Siri\". PC World. Archived from the original on 13 January 2017. Retrieved 28 July 2017.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":26,"type":"book","title":"\"Ray Kurzweil biography\". KurzweilAINetwork. Archived from the original on 5 February 2014. Retrieved 25 September 2014.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":27,"type":"book","title":"Juang, B.H.; Rabiner, Lawrence. \"Automatic Speech Recognition – A Brief History of the Technology Development\" (PDF). Archived (PDF) from the original on 9 August 2017. Retrieved 28 July 2017.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":28,"type":"book","title":"\"Nuance Exec on iPhone 4S, Siri, and the Future of Speech\". Tech.pinions. 10 October 2011. Archived from the original on 19 November 2011. Retrieved 23 November 2011.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":29,"type":"book","title":"\"Switchboard-1 Release 2\". Archived from the original on 11 July 2017. Retrieved 26 July 2017.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":30,"type":"book","title":"Jason Kincaid. \"The Power of Voice: A Conversation With The Head Of Google's Speech Technology\". Tech Crunch. Archived from the original on 21 July 2015. Retrieved 21 July 2015.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":31,"type":"book","title":"Froomkin, Dan (5 May 2015). \"THE COMPUTERS ARE LISTENING\". The Intercept. Archived from the original on 27 June 2015. Retrieved 20 June 2015.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":32,"type":"book","title":"赫尔夫·布尔拉德和尼尔森·摩根，Connectionist语音识别:混合方法，Kluwer国际工程和计算机科学系列；247，波士顿:克鲁瓦学术出版社，1994年。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":33,"type":"book","title":"Schmidhuber, Jürgen (2015). \"Deep learning in neural networks: An overview\". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":34,"type":"book","title":"Alex Graves，Santiago Fernandez，Faustino Gomez和 jürgen schmid Huber (2006)。联结主义时间分类:用递归神经网络标记未分段的序列数据。ICML会议录' 06，第369-376页。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":35,"type":"book","title":"圣地亚哥·费尔南德斯、亚历克斯·格雷夫斯和尤尔根·施密休伯(2007年)。递归神经网络在判别关键词识别中的应用。ICANN会议录(2)，第220-229页。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":36,"type":"book","title":"\"Li Deng\". Li Deng Site.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":37,"type":"book","title":"NIPS研讨会:语音识别和相关应用的深度学习，加拿大不列颠哥伦比亚省惠斯勒，2009年12月(组织者:邓梨、杰夫·辛顿、俞敏洪)。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":38,"type":"book","title":"Deng, L.; Hinton, G.; Kingsbury, B. (2013). \"New types of deep neural network learning for speech recognition and related applications: An overview\". 2013 IEEE International Conference on Acoustics, Speech and Signal Processing: New types of deep neural network learning for speech recognition and related applications: An overview. p. 8599. doi:10.1109/ICASSP.2013.6639344. ISBN 978-1-4799-0356-6.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":39,"type":"book","title":"Markoff, John (23 November 2012). \"Scientists See Promise in Deep-Learning Programs\". New York Times. Archived from the original on 30 November 2012. Retrieved 20 January 2015.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":40,"type":"book","title":"Morgan，Bourlard，Renals，Cohen，Franco (1993)\"用于连续语音识别的混合神经网络/隐马尔可夫模型系统。ICASSP/IJPRAI \"","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":41,"type":"book","title":"罗宾逊。(1992) 一种实时循环错误传播网络单词识别系统 Archived 3 9月 2017 at the Wayback Machine，ICASSP。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":42,"type":"book","title":"外贝尔，哈那扎瓦，辛顿，志卡诺，朗。(1989)\"使用延时神经网络的音素识别。IEEE声学、语音和信号处理汇刊","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":43,"type":"book","title":"Baker, J.; Li Deng; Glass, J.; Khudanpur, S.; Chin-Hui Lee; Morgan, N.; O'Shaughnessy, D. (2009). \"Developments and Directions in Speech Recognition and Understanding, Part 1\". IEEE Signal Processing Magazine. 26 (3): 75–80. Bibcode:2009ISPM...26...75B. doi:10.1109/MSP.2009.932166.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":44,"type":"book","title":"Sepp Hochreiter(1991年)，动态神经网络 Archived 6 3月 2015 at the Wayback Machine，毕业论文。慕尼黑工业大学信息研究所。顾问:施密休伯。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":45,"type":"book","title":"Bengio, Y. (1991). Artificial Neural Networks and their Application to Speech/Sequence Recognition (Ph.D.). McGill University.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":46,"type":"book","title":"Deng, L.; Hassanein, K.; Elmasry, M. (1994). \"Analysis of the correlation structure for a neural predictive model with application to speech recognition\". Neural Networks. 7 (2): 331–339. doi:10.1016/0893-6080(94)90027-2.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":47,"type":"book","title":"主旨演讲:深度神经网络的最新发展。ICASSP，2013(作者:Geoff Hinton)。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":48,"type":"book","title":"主旨演讲:“深度学习的成就和挑战:从语音分析和识别到语言和多模式处理”，Interspeech，2014年9月(作者:邓梨)。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":49,"type":"book","title":"\"Improvements in voice recognition software increase\". TechRepublic.com. 27 August 2002. Maners said IBM has worked on advancing speech recognition ... or on the floor of a noisy trade show.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":50,"type":"book","title":"\"Voice Recognition To Ease Travel Bookings: Business Travel News\". BusinessTravelNews.com. 3 March 1997. The earliest applications of speech recognition software were dictation ... Four months ago, IBM introduced a 'continual dictation product' designed to ... debuted at the National Business Travel Association trade show in 1994.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":51,"type":"book","title":"Ellis Booker (14 March 1994). \"Voice recognition enters the mainstream\". Computerworld. p. 45. Just a few years ago, speech recognition was limited to ...","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":52,"type":"book","title":"Goel, Vaibhava; Byrne, William J. (2000). \"Minimum Bayes-risk automatic speech recognition\". Computer Speech & Language. 14 (2): 115–135. doi:10.1006/csla.2000.0138. Archived from the original on 25 July 2011. Retrieved 28 March 2011.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":53,"type":"book","title":"Mohri, M. (2002). \"Edit-Distance of Weighted Automata: General Definitions and Algorithms\" (PDF). International Journal of Foundations of Computer Science. 14 (6): 957–982. doi:10.1142/S0129054103002114. Archived (PDF) from the original on 18 March 2012. Retrieved 28 March 2011.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":54,"type":"book","title":"Waibel, A.; Hanazawa, T.; Hinton, G.; Shikano, K.; Lang, K. J. (1989). \"Phoneme recognition using time-delay neural networks\". IEEE Transactions on Acoustics, Speech, and Signal Processing. 37 (3): 328–339. doi:10.1109/29.21701.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":55,"type":"book","title":"Wu, J.; Chan, C. (1993). \"Isolated Word Recognition by Neural Network Models with Cross-Correlation Coefficients for Speech Dynamics\". IEEE Transactions on Pattern Analysis and Machine Intelligence. 15 (11): 1174–1185. doi:10.1109/34.244678.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":56,"type":"book","title":"S.A. Zahorian，A. M. Zimmer和F. Meng，(2002)“基于计算机视觉反馈的听力受损者语音训练元音分类”，ICSLP 2002","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":57,"type":"book","title":"Hu, Hongbing; Zahorian, Stephen A. (2010). \"Dimensionality Reduction Methods for HMM Phonetic Recognition\" (PDF). ICASSP 2010. Archived (PDF) from the original on 6 July 2012.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":58,"type":"book","title":"Sepp Hochreiter; J. Schmidhuber (1997). \"Long Short-Term Memory\". Neural Computation. 9 (8): 1735–1780. doi:10.1162/neco.1997.9.8.1735. PMID 9377276.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":59,"type":"book","title":"Graves, Alex; Mohamed, Abdel-rahman; Hinton, Geoffrey (2013). \"Speech recognition with deep recurrent neural networks\". arXiv:1303.5778 [cs.NE].ICASSP 2013。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":60,"type":"book","title":"Waibel, Alex (1989). \"Modular Construction of Time-Delay Neural Networks for Speech Recognition\" (PDF). Neural Computation. 1 (1): 39–46. doi:10.1162/neco.1989.1.1.39. Archived (PDF) from the original on 29 June 2016.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":61,"type":"book","title":"Maas, Andrew L.; Le, Quoc V.; O'Neil, Tyler M.; Vinyals, Oriol; Nguyen, Patrick; Ng, Andrew Y. (2012). \"Recurrent Neural Networks for Noise Reduction in Robust ASR\". Proceedings of Interspeech 2012.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":62,"type":"book","title":"Hinton, Geoffrey; Deng, Li; Yu, Dong; Dahl, George; Mohamed, Abdel-Rahman; Jaitly, Navdeep; Senior, Andrew; Vanhoucke, Vincent; Nguyen, Patrick; Sainath, Tara; Kingsbury, Brian (2012). \"Deep Neural Networks for Acoustic Modeling in Speech Recognition: The shared views of four research groups\". IEEE Signal Processing Magazine. 29 (6): 82–97. Bibcode:2012ISPM...29...82H. doi:10.1109/MSP.2012.2205597.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":63,"type":"book","title":"Yu, D.; Deng, L.; Dahl, G. (2010). \"Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition\". NIPS Workshop on Deep Learning and Unsupervised Feature Learning.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":64,"type":"book","title":"Dahl, George E.; Yu, Dong; Deng, Li; Acero, Alex (2012). \"Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition\". IEEE Transactions on Audio, Speech, and Signal Processing. 20 (1): 30–42. doi:10.1109/TASL.2011.2134090.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":65,"type":"book","title":"邓l，李军，黄军，姚军，k，俞，d，塞德，f .等。微软语音研究深度学习的最新进展。会计师协会，2013年。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":66,"type":"book","title":"Deng, L.; Li, Xiao (2013). \"Machine Learning Paradigms for Speech Recognition: An Overview\". IEEE Transactions on Audio, Speech, and Language Processing.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":67,"type":"book","title":"Schmidhuber, Jürgen (2015). \"Deep Learning\". Scholarpedia. 10 (11): 32832. Bibcode:2015SchpJ..1032832S. doi:10.4249/scholarpedia.32832.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":68,"type":"book","title":"长度Deng，M. Seltzer，D. Yu，A. Acero，A. Mohamed和G. Hinton (2010)使用深度自编码对语音频谱图进行二进制编码。散布技术。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":69,"type":"book","title":"Tüske, Zoltán; Golik, Pavel; Schlüter, Ralf; Ney, Hermann (2014). \"Acoustic Modeling with Deep Neural Networks Using Raw Time Signal for LVCSR\" (PDF). Interspeech 2014. Archived (PDF) from the original on 21 December 2016.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":70,"type":"book","title":"Jurafsky, Daniel (2016). Speech and Language Processing.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":71,"type":"book","title":"Graves, Alex (2014). \"Towards End-to-End Speech Recognition with Recurrent Neural Networks\". ICML.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":72,"type":"book","title":"Amodei, Dario (2016). \"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\". arXiv:1512.02595 [cs.CL].","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":73,"type":"book","title":"\"LipNet: How easy do you think lipreading is?\". YouTube. Archived from the original on 27 April 2017. Retrieved 5 May 2017.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":74,"type":"book","title":"Assael, Yannis; Shillingford, Brendan; Whiteson, Shimon; de Freitas, Nando (5 November 2016). \"LipNet: End-to-End Sentence-level Lipreading\". arXiv:1611.01599 [cs.CV].","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":75,"type":"book","title":"Shillingford, Brendan; Assael, Yannis; Hoffman, Matthew W.; Paine, Thomas; Hughes, Cían; Prabhu, Utsav; Liao, Hank; Sak, Hasim; Rao, Kanishka (2018-07-13). \"Large-Scale Visual Speech Recognition\". arXiv:1807.05162.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":76,"type":"book","title":"Chan, William; Jaitly, Navdeep; Le, Quoc; Vinyals, Oriol (2016). \"Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition\". ICASSP.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":77,"type":"book","title":"Bahdanau, Dzmitry (2016). \"End-to-End Attention-based Large Vocabulary Speech Recognition\". arXiv:1508.04395 [cs.CL].","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":78,"type":"book","title":"Chorowski, Jan; Jaitly, Navdeep (8 December 2016). \"Towards better decoding and language model integration in sequence to sequence models\". arXiv:1612.02695 [cs.NE].","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":79,"type":"book","title":"Chan, William; Zhang, Yu; Le, Quoc; Jaitly, Navdeep (10 October 2016). \"Latent Sequence Decompositions\". arXiv:1610.03035 [stat.ML].","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":80,"type":"book","title":"Chung, Joon Son; Senior, Andrew; Vinyals, Oriol; Zisserman, Andrew (16 November 2016). \"Lip Reading Sentences in the Wild\". arXiv:1611.05358 [cs.CV].","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":81,"type":"book","title":"Suominen, Hanna; Zhou, Liyuan; Hanlen, Leif; Ferraro, Gabriela (2015). \"Benchmarking Clinical Speech Recognition and Information Extraction: New Data, Methods, and Evaluations\". JMIR Medical Informatics. 3 (2): e19. doi:10.2196/medinform.4321. PMC 4427705. PMID 25917752.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":82,"type":"book","title":"Englund, Christine (2004). Speech recognition in the JAS 39 Gripen aircraft: Adaptation to speech at different G-loads (PDF) (Masters thesis). Stockholm Royal Institute of Technology. Archived (PDF) from the original on 2 October 2008.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":83,"type":"book","title":"\"The Cockpit\". Eurofighter Typhoon. Archived from the original on 1 March 2017.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":84,"type":"book","title":"\"Eurofighter Typhoon – The world's most advanced fighter aircraft\". www.eurofighter.com. Archived from the original on 11 May 2013. Retrieved 1 May 2018.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":85,"type":"book","title":"Schutte, John (15 October 2007). \"Researchers fine-tune F-35 pilot-aircraft speech system\". United States Air Force. Archived from the original on 20 October 2007.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":86,"type":"book","title":"Cerf, Vinton; Wrubel, Rob; Sherwood, Susan. \"Can speech-recognition software break down educational language barriers?\". Curiosity.com. Discovery Communications. Archived from the original on 7 April 2014. Retrieved 26 March 2014.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":87,"type":"book","title":"\"Speech Recognition for Learning\". National Center for Technology Innovation. 2010. Archived from the original on 13 April 2014. Retrieved 26 March 2014.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":88,"type":"book","title":"Follensbee, Bob; McCloskey-Dale, Susan (2000). \"Speech recognition in schools: An update from the field\". Technology And Persons With Disabilities Conference 2000. Archived from the original on 21 August 2006. Retrieved 26 March 2014.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":89,"type":"book","title":"\"Overcoming Communication Barriers in the Classroom\". MassMATCH. 18 March 2010. Archived from the original on 25 July 2013. Retrieved 15 June 2013.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":90,"type":"book","title":"\"Speech recognition for disabled people\". Archived from the original on 4 April 2008.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":91,"type":"book","title":"朋友国际支持小组","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":92,"type":"book","title":"Garrett, Jennifer Tumlin; et al. (2011). \"Using Speech Recognition Software to Increase Writing Fluency for Individuals with Physical Disabilities\". Journal of Special Education Technology. 26 (1): 25–41. doi:10.1177/016264341102600104.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":93,"type":"book","title":"辅助技术:增强残疾学生的能力信息交换所75.3(2002):122–6。网络。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":94,"type":"book","title":"Tang, K. W.; Kamoua, Ridha; Sutan, Victor (2004). \"Speech Recognition Technology for Disabilities Education\". Journal of Educational Technology Systems. 33 (2): 173–84. CiteSeerX 10.1.1.631.3736. doi:10.2190/K6K8-78K2-59Y7-R9R2.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":95,"type":"book","title":"\"Projects: Planetary Microphones\". The Planetary Society. Archived from the original on 27 January 2012.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":96,"type":"book","title":"Caridakis, George; Castellano, Ginevra; Kessous, Loic; Raouzaiou, Amaryllis; Malatesta, Lori; Asteriadis, Stelios; Karpouzis, Kostas (19 September 2007). Multimodal emotion recognition from expressive faces, body gestures and speech. IFIP the International Federation for Information Processing (in 英语). 247. Springer US. pp. 375–388. doi:10.1007/978-0-387-74161-1_41. ISBN 978-0-387-74160-4.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":97,"type":"book","title":"再见，阿尔伯托。\"性能评估报告的原型.\"日晷工作包8000 (1993)。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":98,"type":"book","title":"Gerbino，e .，Baggia，p .，Ciaramella，a . & Rullent，C. (1993年，4月)。口语对话系统的测试与评估。声学、语音和信号处理，1993年。ICASSP-93。，1993年IEEE国际会议(第2卷，第135-138页)。IEEE。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":99,"type":"book","title":"国家标准和技术研究所。“NIST自动语音识别历史评价 Archived 8 10月 2013 at the Wayback Machine“。","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":100,"type":"book","title":"\"Listen Up: Your AI Assistant Goes Crazy For NPR Too\". NPR. 6 March 2016. Archived from the original on 23 July 2017.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":101,"type":"book","title":"Claburn, Thomas (25 August 2017). \"Is it possible to control Amazon Alexa, Google Now using inaudible commands? Absolutely\". The Register. Archived from the original on 2 September 2017.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":102,"type":"book","title":"\"Attack Targets Automatic Speech Recognition Systems\". vice.com. 31 January 2018. Archived from the original on 3 March 2018. Retrieved 1 May 2018.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":103,"type":"book","title":"Beigi, Homayoon (2011). Fundamentals of Speaker Recognition. New York: Springer. ISBN 978-0-387-77591-3. Archived from the original on 31 January 2018.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false},{"id":104,"type":"book","title":"Deng, Li; Yu, Dong (2014). \"Deep Learning: Methods and Applications\" (PDF). Foundations and Trends in Signal Processing. 7 (3–4): 197–387. CiteSeerX 10.1.1.691.3679. doi:10.1561/2000000039. Archived (PDF) from the original on 22 October 2014.","site":"","url":"","journalName":null,"author":"","press":"","publishYear":"","publishTime":null,"publishPlace":"","page":"","volume":"","quoteTime":null,"quoted":false}],"recommendReferences":null,"auditState":2,"lemmaLevel":1,"origin":0,"originEnTitle":null,"originZhTitle":null,"pv":5866,"auditType":0,"synonyms":null,"showEditTime":"2019.12.13 18:53","auditors":[{"uid":0,"name":"Ki.κe","pic":"https://web.archive.org/web/20221028212226/https://wx.qlogo.cn/mmopen/vi_32/y67kfr32Doib4wg71Jiau7jVWvharic3nRKgdRRQSl6koeQJCo0GQs2Krw0vwdFRsOWnHIQOwAZsSg5lIkIrFCOcQ/132","introduction":null,"educations":null,"jobs":null,"works":null,"educationBrief":"","jobBrief":"","role":0,"roleName":null,"title":"","professionalTitle":null,"phoneNo":null,"editable":true,"partnerId":0,"partnerIdCreateTime":0,"partnerIdPoped":false}],"hasZhishiNav":false,"auditInfos":{},"isHistory":false};</script><script crossorigin="anonymous" src="./951.语音识别 - 搜狗科学百科_files/aegis.min.js.download"></script><script crossorigin="anonymous" src="./951.语音识别 - 搜狗科学百科_files/main_2020092401.js.download"></script><script crossorigin="anonymous" src="./951.语音识别 - 搜狗科学百科_files/react.production.min.js.download"></script><script crossorigin="anonymous" src="./951.语音识别 - 搜狗科学百科_files/react-dom.production.min.js.download"></script><script crossorigin="anonymous" src="./951.语音识别 - 搜狗科学百科_files/jquery-1.11.1.min.js.download"></script><script crossorigin="anonymous" src="./951.语音识别 - 搜狗科学百科_files/main_2022062701.js.download"></script><script crossorigin="anonymous" src="./951.语音识别 - 搜狗科学百科_files/main_66bbe21.js.download"></script><script crossorigin="anonymous" src="./951.语音识别 - 搜狗科学百科_files/react.production.min.js.download"></script><script crossorigin="anonymous" src="./951.语音识别 - 搜狗科学百科_files/react-dom.production.min.js.download"></script><script crossorigin="anonymous" src="./951.语音识别 - 搜狗科学百科_files/main_edf0f08.js.download"></script>
</body></html>